In 1950, Alan Turing's famous article "Computing Machinery and Intelligence" was published, which proposed what is now called the Turing test as a criterion of intelligence.
This criterion depends on the ability of a computer program to impersonate a human in a real-time written conversation with a human judge, sufficiently well that the judge is unable to distinguish reliably—on the basis of the conversational content alone—between the program and a real human.
The notoriety of Turing's proposed test stimulated great interest in Joseph Weizenbaum's program ELIZA, published in 1966, which seemed to be able to fool users into believing that they were conversing with a real human.
However Weizenbaum himself did not claim that ELIZA was genuinely intelligent, and the introduction to his paper presented it more as a debunking exercise:
[In] artificial intelligence ... machines are made to behave in wondrous ways, often sufficient to dazzle even the most experienced observer.
But once a particular program is unmasked, once its inner workings are explained ... its magic crumbles away; it stands revealed as a mere collection of procedures ...
The observer says to himself "I could have written that".
With that thought he moves the program in question from the shelf marked "intelligent", to that reserved for curios ...
The object of this paper is to cause just such a re-evaluation of the program about to be "explained".
Few programs ever needed it more.
ELIZA's key method of operation (copied by chatbot designers ever since) involves the recognition of clue words or phrases in the input, and the output of corresponding pre-prepared or pre-programmed responses that can move the conversation forward in an apparently meaningful way (e.g. by responding to any input that contains the word 'MOTHER' with 'TELL ME MORE ABOUT YOUR FAMILY').
Thus an illusion of understanding is generated, even though the processing involved has been merely superficial.
ELIZA showed that such an illusion is surprisingly easy to generate, because human judges are so ready to give the benefit of the doubt when conversational responses are capable of being interpreted as "intelligent".
Interface designers have come to appreciate that humans' readiness to interpret computer output as genuinely conversational—even when it is actually based on rather simple pattern-matching—can be exploited for useful purposes.
Most people prefer to engage with programs that are human-like, and this gives chatbot-style techniques a potentially useful role in interactive systems that need to elicit information from users, as long as that information is relatively straightforward and falls into predictable categories.
Thus, for example, online help systems can usefully employ chatbot techniques to identify the area of help that users require, potentially providing a "friendlier" interface than a more formal search or menu system.
This sort of usage holds the prospect of moving chatbot technology from Weizenbaum's "shelf ... reserved for curios" to that marked "genuinely useful computational methods".
The classic historic early chatbots are ELIZA (1966) and PARRY (1972).
More recent notable programs include A.L.I.C.E., Jabberwacky and D.U.D.E
(Agence Nationale de la Recherche and CNRS 2006).
While ELIZA and PARRY were used exclusively to simulate typed conversation, many chatbots now include functional features such as games and web searching abilities.
In 1984, a book called The Policeman's Beard is Half Constructed was published, allegedly written by the chatbot Racter (though the program as released would not have been capable of doing so).One pertinent field of AI research is natural language processing.
Usually, weak AI fields employ specialized software or programming languages created specifically for the narrow function required.
For example, A.L.I.C.E. uses a markup language called AIML, which is specific to its function as a conversational agent, and has since been adopted by various other developers of, so called, Alicebots.
Nevertheless, A.L.I.C.E. is still purely based on pattern matching techniques without any reasoning capabilities, the same technique ELIZA was using back in 1966.
This is not strong AI, which would require sapience and logical reasoning abilities.
Jabberwacky learns new responses and context based on real-time user interactions, rather than being driven from a static database.
Some more recent chatbots also combine real-time learning with evolutionary algorithms that optimise their ability to communicate based on each conversation held.
Still, there is currently no general purpose conversational artificial intelligence, and some software developers focus on the practical aspect, information retrieval.
Chatbot competitions focus on the Turing test or more specific goals.
Two such annual contests are the Loebner Prize and The Chatterbox Challenge (the latter has been offline since 2015, however materials can still be found from web archives).DBpedia created a chatbot during the GSoC of 2017. and can communicate through Facebook Messenger.
DBpedia started in 2007 and allows to extract structured content from the Wikipedia dataset, along with many other datasets.
DBpedia is currently one of the biggest representatives of Linked Open Data (LOD).
Many companies' chatbots run on messaging apps or simply via SMS.
They are used for B2C customer service, sales and marketing.
In 2016, Facebook Messenger allowed developers to place chatbots on their platform.
There were 30,000 bots created for Messenger in the first six months, rising to 100,000 by September 2017.
Since September 2017, this has also been as part of a pilot program on WhatsApp.
Airlines KLM and Aeroméxico both announced their participation in the testing; both airlines had previously launched customer services on the Facebook Messenger platform.
The bots usually appear as one of the user's contacts, but can sometimes act as participants in a group chat.
Many banks, insurers, media companies, e-commerce companies, airlines, hotel chains, retailers, health care providers, government entities and restaurant chains have used chatbots to answer simple questions, increase customer engagement, for promotion, and to offer additional ways to order from them.
A 2017 study showed 4% of companies used chatbots.
According to a 2016 study, 80% of businesses said they intended to have one by 2020.
Other companies explore ways they can use chatbots internally, for example for Customer Support, Human Resources, or even in Internet-of-Things (IoT) projects.
Overstock.com, for one, has reportedly launched a chatbot named Mila to automate certain simple yet time-consuming processes when requesting for a sick leave.
Other large companies such as Lloyds Banking Group, Royal Bank of Scotland, Renault and Citroën are now using automated online assistants instead of call centres with humans to provide a first point of contact.
A SaaS chatbot business ecosystem has been steadily growing since the F8 Conference when Facebook's Mark Zuckerberg unveiled that Messenger would allow chatbots into the app.
In large companies, like in hospitals and aviation organizations, IT architects are designing reference architectures for Intelligent Chatbots that are used to unlock and share knowledge and experience in the organization more efficiently, and reduce the errors in answers from expert service desks significantly.
These Intelligent Chatbots make use of all kinds of artificial intelligence like image moderation and natural language understanding (NLU), natural language generation (NLG), machine learning and deep learning.
Nowadays a high majority of high-tech banking organizations are looking for integration of automated AI-based solutions such as chatbots in their customer service in order to provide faster and cheaper assistance to their clients becoming increasingly technodexterous.
In particularly, chatbots can efficiently conduct a dialogue, usually substituting other communication tools such as email, phone, or SMS.
In banking area their major application is related to quick customer service answering common requests, and transactional support.
Several studies accomplished by analytics agencies such as Juniper or Gartner  report significant reduction of cost of customer services, leading to billions of dollars of economy in the next 10 years.
Gartner predicts an integration by 2020 of chatbots in at least 85% of all client's applications to customer service.
Juniper's study announces an impressive amount of $8 billion retained annually by 2022 due to the use of chatbots.
Since 2016 when Facebook allows businesses to deliver automated customer support, e-commerce guidance, content and interactive experiences through chatbots, a large variety of chatbots for Facebook Messenger platform were developed.
In 2016, Russia-based Tochka Bank launched the world's first Facebook bot for a range of financial services, in particularly including a possibility of making payments.
In July 2016, Barclays Africa also launched a Facebook chatbot, making it the first bank to do so in Africa.
The France's third largest bank by total assets
Société Générale launched their chatbot called SoBot in March 2018.
While the 80% of users of the SoBot express their satisfaction after having tested it, il will never replace the expertise provided by a human advisor according to SG deputy director Bertrand Cozzarolo.
There are several potential advantages of using chatbots in interaction with customer in banking: Cost reduction.
Chatbots eliminate the obligation of any human presence during online interaction and are so long seen as a huge advantage by companies operating with multiple repetitive queries at once as long as they don't require any decision-making procedure.
Human touch.
Chatbots, providing an interface similar to human-to-human interaction, are more intuitive and so less difficult to use than a standard banking mobile application.
They do not require any additional software installation and are more adaptive as able to be personalized during the exploitation by the means of machine learning.
Chatbots are instant and so much faster that phone calls, shown to be considered as tedious in some studies.
Then they satisfy both speed and personalization requirement while interacting with a bank.
Financial advice.
State-of-the-art banking assistants can, by accessing personal customer's data such as consumer behavior or history of purchases, provide useful information on client's account and use it to develop AI-Based recommendations for better adapted money management.
Attracting new clients.
A chatbot greeting a bank's website guest can provide some useful and relevant information, helping to exploit the offered products and services in a personalized manner helps to attract the potential client and the he is more likely to end by creation of a new account.
24/7 digital support.
An instant and always accessible assistant is assumed by the more and more digital consumer of the new era.
Unlike humans, chatbots once developed and installed don't have a limited workdays, holidays or weekends and are ready to attend queries at any hour of the day.
It helps to the customer to avoid waiting of a company's agent to be available.
Thus, the customer doesn't have to wait for the company executive to help them.
This also lets companies keep an eye on the traffic during the non-working hours and reach out to them later.
Chatbots are also appearing in the healthcare industry.
A study suggested that physicians in the United States believed that chatbots would be most beneficial for scheduling doctor appointments, locating health clinics, or providing medication information.
Certain patient groups are still reluctant to use chatbots.
A mixed-methods study showed that people are still hesitant to use chatbots for their healthcare due to poor understanding of the technological complexity, the lack of empathy and concerns about cyber-security.
The analysis showed that while 6% had heard of a health chatbot and 3% had experience of using it, 67% perceived themselves as likely to use one within 12 months.
The majority of participants would use a health chatbot for seeking general health information (78%), booking a medical appointment (78%) and looking for local health services (80%).
However, a health chatbot was perceived as less suitable for seeking results of medical tests and seeking specialist advice such as sexual health.
The analysis of attitudinal variables showed that most participants reported their preference for discussing their health with doctors (73%) and having access to reliable and accurate health information (93%).
While 80% were curious about new technologies that could improve their health, 66% reported only seeking a doctor when experiencing a health problem and 65% thought that a chatbot was a good idea.
Interestingly, 30% reported dislike about talking to computers, 41% felt it would be strange to discuss health matters with a chatbot and about half were unsure if they could trust the advice given by a chatbot.
Therefore, perceived trustworthiness, individual attitudes towards bots and dislike for talking to computers are the main barriers to health chatbots.
In New Zealand, the chatbot SAM -short for Semantic Analysis Machine-
(made by Nick Gerritsen of Touchtech) has been developed.
It is designed to share its political thoughts, for example on topics such as climate change, healthcare and education, etc.
It talks to people through Facebook Messenger.
Chatbots have also been incorporated into devices not primarily meant for computing such as toys.
Hello Barbie is an Internet-connected version of the doll that uses a chatbot provided by the company ToyTalk, which previously used the chatbot for a range of smartphone-based characters for children.
These characters' behaviors are constrained by a set of rules that in effect emulate a particular character and produce a storyline.
IBM's Watson computer has been used as the basis for chatbot-based educational toys for companies such as CogniToys intended to interact with children for educational purposes.
The process of building, testing and deploying chatbots can be done on cloud-based chatbot development platforms offered by cloud Platform as a Service (PaaS) providers such as Oracle Cloud Platform  SnatchBot and IBM Watson.
These cloud platforms provide Natural Language Processing, Artificial Intelligence and Mobile Backend as a Service for chatbot development.
API
Some Companies like Microsoft Azure and AARC are currently providing their Bot Engines through which chatbot Platforms or Software can be developed.
Malicious chatbots are frequently used to fill chat rooms with spam and advertisements, by mimicking human behavior and conversations or to entice people into revealing personal information, such as bank account numbers.
They are commonly found on Yahoo!
Messenger, Windows Live Messenger, AOL Instant Messenger and other instant messaging protocols.
There has also been a published report of a chatbot used in a fake personal ad on a dating service's website.
Tay, an AI chatbot that learns from previous interaction, caused major controversy due to it being targeted by internet trolls on Twitter.
The bot was exploited, and after 16 hours began to send extremely offensive Tweets to users.
This suggests that although the bot learnt effectively from experience, adequate protection was not put in place to prevent misuse.
If a text-sending algorithm can pass itself off as a human instead of a chatbot, its message would be more credible.
Therefore, human-seeming chatbots with well-crafted online identities could start scattering fake news that seem plausible, for instance making false claims during a presidential election.
With enough chatbots, it might be even possible to achieve artificial social proof.
The creation and implementation of chatbots is still a developing area, heavily related to artificial intelligence and machine learning, so the provided solutions, while possessing obvious advantages, have some important limitations in terms of functionalities and use cases.
However this is changing over time.
The most common ones are listed below: As the database, used for output generation, is fixed and limited, chatbots can fail while dealing with an unsaved query.
A chatbot's efficiency highly depends on language processing and is limited because of irregularities, such as accents and mistakes that can create an important barrier for international and multi-cultural organisations  Chatbots are unable to deal with multiple questions at the same time and so conversation opportunities are limited.
As it happens usually with technology-led changes in existing services, some consumers, more often than not from the old generation, are uncomfortable with chatbots due to their limited understanding, making it obvious that their requests are being dealt machines.
Chatbots are increasingly present in businesses and often are used to automate tasks that do not require skill-based talents.
With customer service taking place via messaging apps as well as phone calls, there are growing numbers of use-cases where chatbot deployment gives organisations a clear return on investment.
Call centre workers may be particularly at risk from AI-driven chatbots.
A study by Forrester (June 2017) predicts that 25% of today's jobs will most likely be impacted by AI technologies by 2019.
Computer History Museum (2006), "Internet History—1970's", Exhibits, Computer History Museum, archived from the original on 2008-02-21, retrieved 2008-03-05 Güzeldere, Güven; Franchi, Stefano (1995-07-24), "Constructions of the Mind", Stanford Humanities Review, SEHR, Stanford University, 4 (2), retrieved 2008-03-05 Mauldin, Michael (1994), "ChatterBots, TinyMuds, and the Turing Test: Entering the Loebner Prize Competition", Proceedings of the Eleventh National Conference on Artificial Intelligence, AAAI Press, retrieved 2008-03-05 (abstract)
Network Working Group (1973), "RFC 439, PARRY Encounters the DOCTOR", Internet Engineering Task Force, Internet Society, retrieved 2008-03-05 Sondheim, Alan J (1997), <nettime> Important Documents from the Early Internet (1972), nettime.org, archived from the original on 2008-06-13, retrieved 2008-03-05 Searle, John (1980), "Minds, Brains and Programs", Behavioral and Brain Sciences, 3 (3): 417–457, doi:10.1017/S0140525X00005756 Shevat, Amir (2017).
Designing bots: Creating conversational experiences (First ed.).
Sebastopol, CA:
O'Reilly Media.
ISBN 9781491974827.
OCLC 962125282.
Turing, Alan (1950), "Computing Machinery and Intelligence", Mind, 59 (236):
433–60, doi:10.1093/mind/lix.236.433 Weizenbaum, Joseph (January 1966), "ELIZA—A Computer Program For the Study of Natural Language Communication Between Man And Machine", Communications of the ACM, 9 (1): 36–45, doi:10.1145/365153.365168
Dr. Sbaitso ELIZA PARRY Racter (or Claude Chatterbot)
Mark V Shaney
Albert One - 1998 and 1999 Loebner winner, by Robby Garner. A.L.I.C.E. - 2001, 2002, and 2004 Loebner Prize winner developed by Richard Wallace.
Charlix Cleverbot (winner of the 2010 Mechanical Intelligence Competition) Elbot - 2008 Loebner Prize winner, by Fred Roberts.
Eugene Goostman - 2012 Turing 100 winner, by Vladimir Veselov.
Fred - an early chatterbot by Robby Garner.
Jabberwacky Jeeney AI MegaHAL
SimSimi
- A popular artificial intelligence conversation program that was created in 2002 by ISMaker.
Spookitalk - A chatterbot used for NPCs in Douglas Adams' Starship Titanic video game.
Ultra Hal - 2007 Loebner Prize winner, by Robert Medeksza.
Verbot
GooglyMinotaur, specializing in Radiohead, the first bot released by ActiveBuddy (June 2001-March 2002) SmarterChild, developed by ActiveBuddy and released in June 2001 Infobot, an assistant on IRC channels such as #perl, primarily to help out with answering Frequently Asked Questions (June 1995-today) Lara.ng, is a personal directions assistant developed by Road Preppers Technologies, for navigating public transportation in Africa.
(released in March 2017)
It is sometimes desirable to identify when a Twitter account is controlled by a bot.
In a 2012 paper, Chu et al. propose the following criteria that indicate that an account may be a bot (they were designing an automated system):  "Periodic and regular timing" of tweets; Whether the tweet content contains known spam; and The ratio of tweets from mobile versus desktop, as compared to an average human Twitter user.
Research shows that humans can view Twitter bots as a credible source of information.
There are many different types of Twitter bots and their purposes vary from one to another.
Some bots may tweet helpful material such as @EarthquakesSF (description below).
In 2009, Twitter bots were estimated to create approximately 24% of tweets that on Twitter.
Here are examples of some of the Twitter bots and how they interact with users on Twitter.
@Betelgeuse_3 sends at-replies in response to tweets that include the phrase, "Beetlejuice, beetlejuice, beetlejuice".
The tweets are sent in the voice of the lead character from the Beetlejuice film.
@CongressEdits and @parliamentedits posts whenever someone makes edits to Wikipedia from the US Congress and UK Parliament IP addresses, respectively.
@DBZNappa replied with "WHAT!?
NINE THOUSAND?
" to anyone on Twitter that used the internet meme phrase "over 9000".
The account began in 2011, and was eventually suspended in 2015, most likely a victim of its own success.
@DearAssistant sends auto-reply tweets responding to complex queries in simple English by utilizing Wolfram Alpha.
@DeepDrumpf is a recurrent neural network, created at MIT, that releases tweets imitating Donald Trump's speech patterns.
It received its namesake from the term 'Donald Drumpf', popularized in the segment 'Donald Trump' from the show Last Week Tonight with John Oliver.
@DroptheIBot tweets the message, "People aren't illegal.
Try saying 'undocumented immigrant' or 'unauthorized immigrant' instead" to Twitter users who have sent a tweet containing the phrase "illegal immigrant".
It was created by American Fusion.net journalists Jorge Rivas and Patrick Hogan.
@everyword has tweeted every word of the English language.
It started in 2007 and tweeted every thirty minutes until 2014.
@factbot1 was created by Eric Drass to illustrate what he believed to be a prevalent problem: that of people on the internet believing unsupported facts which accompany pictures.
@fuckeveryword
@Horse_ebooks was a bot that gained a following among people who found its tweets poetic.
It has inspired various _ebooks-suffixed Twitter bots which use Markov text generators (or similar techniques) to create new tweets by mashing up the tweets of their owner.
It went inactive following a brief promotion for Bear Stearns Bravo.
@infinite_scream tweets and auto-replies a 2-39 character scream.
At least partially inspired by Edvard Munch's The Scream, it attracted attention from those distressed by the Presidency of Donald Trump and bad news.
@MetaphorMagnet is an AI bot that generates metaphorical insights using its knowledge-base of stereotypical properties and norms.
A companion bot @MetaphorMirror pairs these metaphors to news tweets.
Another companion bot @BestOfBotWorlds uses metaphor to generate faux-religious insights.
@Pentametron finds tweets incidentally written in iambic pentameter using the CMU Pronouncing Dictionary, pairs them into couplets using a rhyming dictionary, and retweets them as couplets into followers' feeds.
@RedScareBot tweets in the persona of Joseph McCarthy in response to Twitter posts mentioning "socialist", "communist", or "communism".
@tinycarebot promotes simple self care actions to its followers, such as remembering to look up from your screens, taking a break to go outside, and drink more water.
It will also send a self care suggestion if you tweet directly at it.
There are also families of related Twitter bots.
For example, @LessicoFeed, @SpracheFeed, @SwedishFeed, @TraductionFeed, @VocabularioFeed, @WelshFeed each tweet an English word along with a translation every hour into Italian, German, Swedish, French, Spanish, and Welsh, respectively.
The translations are crowdsourced by volunteers and subscribers.
Detecting non-human Twitter users has been of interest to academics.
Indiana University has developed a free service called Botometer (formerly BotOrNot), which scores Twitter handles based on their likelihood of being a Twitterbot.
One significant academic study estimated that up to 15% of Twitter users were automated bot accounts.
The prevalence of Twitter bots coupled with the ability of some bots to give seemingly human responses has enabled these non-human accounts to garner widespread influence.
A subset of Twitter bots programmed to complete social tasks played an important role in the United States 2016 Presidential Election.
Researchers estimated that pro-Trump bots generated four tweets for every pro-Clinton automated account and out-tweeted pro-Clinton bots 7:1 on relevant hashtags during the final debate.
Deceiving Twitter bots fooled candidates and campaign staffers into retweeting misappropriated quotes and accounts affiliated with incendiary ideals.
Concerns about political Twitter bots include the promulgation of malicious content, increased polarization, and the spreading of fake news.
Many non-malicious bots are popular for their entertainment value.
However, as technology and the creativity of bot-makers improves, so does the potential for Twitter bots that fill social needs.
@tinycarebot is a Twitterbot that encourages followers to practice self care, and brands are increasingly using automated Twitter bots to engage with customers in interactive ways.
One anti-bullying organization has created @TheNiceBot, which attempts to combat the prevalence of mean tweets by automatically tweeting kind messages.
The majority of Twitter accounts following public figures and brands are often fake or inactive, making the number of Twitter followers a celebrity a difficult metric for gauging popularity.
While this cannot always be helped, some public figures who have gained or lost huge quantities of followers in short periods of time have been accused of discreetly paying for Twitter followers.
For example, the Twitter accounts of Sean Combs, Rep Jared Polis (D-Colo), PepsiCo, Mercedes-Benz, and 50 Cent have come under scrutiny for possibly engaging in the buying and selling of Twitter followers, which is estimated to be between a $40 million and $360 million business annually.
Account sellers may charge a premium for more realistic accounts that have Twitter profile pictures and bios and retweet the accounts they follow.
In addition to an ego boost, public figures may gain  more lucrative endorsement contracts from inflated Twitter metrics.
For brands, however, the translation of online buzz and social media followers into sales has recently come under question after The Coca-Cola Company disclosed that a corporate study revealed that social media buzz does not create a spike in short term sales.
Lutz Finger identifies 5 immediate uses for social bots: foster fame: having an arbitrary number of (unrevealed) bots as (fake) followers can help simulate real success spamming: having advertising bots in online chats is similar to email spam, but a lot more direct mischief:
e.g. signing up an opponent with a lot of fake identities and spam the account or help others discover it to discreditize the opponent bias public opinion: influence trends by countless messages of similar content with different phrasings limit free speech: important messages can be pushed out of sight by a deluge of automated bot messagesThe effects of all points can be likened to and support methods of traditional psychological warfare.
The first generation of bots could sometimes be distinguished from real users by their often superhuman capacities to post messages around the clock (and at massive rates).
Later developments have succeeded in imprinting more "human" activity and behavioral patterns in the agent.
To unambiguously detect social bots as what they are, a variety of criteria must be applied together using pattern detection techniques, some of which are: cartoon figures as user pictures sometimes also random real user pictures are captured (identity fraud) reposting rate temporal patterns sentiment expression followers-to-friends ratio length of user names variability in (re)posted messagesBotometer (formerly BotOrNot) is a public Web service that checks the activity of a Twitter account and gives it a score based on how likely the account is to be a bot.
The system leverages over a thousand features.
An active method that worked well in detecting early spam bots was to set up honeypot accounts where obvious nonsensical content was posted and then dumbly reposted (retweeted) by bots.
However, recent studies show that bots evolve quickly and detection methods have to be updated constantly, because otherwise they may get useless after few years.
Some bots communicate with other users of Internet-based services, via instant messaging (IM), Internet Relay Chat (IRC), or another web interface such as Facebook Bots and Twitterbots.
These chatterbots may allow people to ask questions in plain English and then formulate a proper response.
These bots can often handle many tasks, including reporting weather, zip-code information, sports scores, converting currency or other units, etc.
Others are used for entertainment, such as SmarterChild on AOL Instant Messenger and MSN Messenger.
An additional role of IRC bots may be to lurk in the background of a conversation channel, commenting on certain phrases uttered by the participants (based on pattern matching).
This is sometimes used as a help service for new users, or for censorship of profanity.
Social networking bots are sets of algorithms that take on the duties of repetitive sets of instructions in order to establish a service or connection among social networking users.
Various designs of networking bots vary from chat bots, algorithms designed to converse with a human user, to social bots, algorithms designed to mimic human behaviors to converse with behavioral patterns similar to that of a human user.
The history of social botting can be traced back to Alan Turing in the 1950s and his vision of designing sets of instructional code that passes the Turing test.
From 1964 to 1966, ELIZA, a natural language processing computer program created by Joseph Weizenbaum, is an early indicator of artificial intelligence algorithms that inspired computer programmers to design tasked programs that can match behavior patterns to their sets of instruction.
As a result, natural language processing has become an influencing factor to the development of artificial intelligence and social bots as innovative technological advancements are made alongside the progression of the mass spreading of information and thought on social media websites.
Reports of political interferences in recent elections, including the 2016 US and 2017 UK general elections, have set the notion of botting being more prevalent because of the ethics that is challenged between the bot's design and the bot's designer.
According to Emilio Ferrara, a computer scientist from the University of Southern California reporting on Communications of the ACM, the lack of resources available to implement fact-checking and information verification results in the large volumes of false reports and claims made on these bots in social media platforms.
In the case of Twitter, most of these bots are programmed with searching filter capabilities that target key words and phrases that reflect in favor and against political agendas and retweet them.
While the attention of bots is programmed to spread unverified information throughout the social media platform, it is a challenge that programmers face in the wake of a hostile political climate.
Binary functions are designated to the programs and using an Application Program interface embedded in the social media website executes the functions tasked.
The Bot Effect is what Ferrera reports as when the socialization of bots and human users creates a vulnerability to the leaking of personal information and polarizing influences outside the ethics of the bot's code.
According to Guillory Kramer in his study, he observes the behavior of emotionally volatile users and the impact the bots have on the users, altering the perception of reality.
There has been a great deal of controversy about the use of bots in an automated trading function.
Auction website eBay has been to court in an attempt to suppress a third-party company from using bots to traverse their site looking for bargains; this approach backfired on eBay and attracted the attention of further bots.
The United Kingdom-based bet exchange Betfair saw such a large amount of traffic coming from bots that it launched a WebService API aimed at bot programmers, through which it can actively manage bot interactions.
Bot farms are known to be used in online app stores, like the Apple App Store and Google Play, to manipulate positions or to increase positive ratings/reviews.
A rapidly growing, benign, form of internet bot is the chatbot.
From 2016, when Facebook Messenger allowed developers to place chatbots on their platform, there has been an exponential growth of their use on that forum alone.
30,000 bots were created for Messenger in the first six months, rising to 100,000 by September 2017.
Avi Ben Ezra, CTO of SnatchBot, told Forbes that evidence from the use of their chatbot building platform pointed to a near future saving of millions of hours of human labour as 'live chat' on websites was replaced with bots.
Companies use internet bots to increase online engagement and streamline communication.
Companies often use bots to cut down on cost, instead of employing people to communicate with consumers, companies have developed new ways to be efficient.
These chatbots are used to answer customers' questions.
For example, Domino's has developed a chatbot that can take orders via Facebook Messenger.
Chatbots allow companies to allocate their employees' time to more important things.
A malicious use of bots is the coordination and operation of an automated attack on networked computers, such as a denial-of-service attack by a botnet.
Internet bots can also be used to commit click fraud and more recently have seen usage around MMORPG games as computer game bots.
A spambot is an internet bot that attempts to spam large amounts of content on the Internet, usually adding advertising links.
More than 94.2% of websites have experienced a bot attack.
There are malicious bots (and botnets) of the following types:
Spambots that harvest email addresses from contact or guestbook pages Downloader programs that suck bandwidth by downloading entire websites Website scrapers that grab the content of websites and re-use it without permission on automatically generated doorway pages Registration bots which sign up a specific email address to a large number of services in order to have the confirmation messages flood the email inbox and distract from important messages indicating a security breach.
Viruses and worms DDoS attacks Botnets, zombie computers, etc.
Spambots that try to redirect people onto a malicious website, sometimes found in comment sections or forums of various websites.
Bots are also used to buy up good seats for concerts, particularly by ticket brokers who resell the tickets.
Bots are employed against entertainment event-ticketing sites.
The bots are used by ticket brokers to unfairly obtain the best seats for themselves while depriving the general public of also having a chance to obtain the good seats.
The bot runs through the purchase process and obtains better seats by pulling as many seats back as it can.
Bots are often used in Massively Multiplayer Online Roleplaying Games to farm for resources that would otherwise take significant time or effort to obtain; this is a concern for most online in-game economies.
Bots are also used to increase views for YouTube videos.
Bots are used to increase traffic counts on analytics reporting to extract money from advertisers.
A study by comScore found that 54 percent of display ads shown in thousands of campaigns between May 2012 and February 2013 never appeared in front of a human being. in 2012, reporter Percy von Lipinski reported that he discovered millions of bot or botted or pinged views at CNN iReport.
CNN iReport quietly removed millions of views from the account of so-called superstar iReporter Chris Morrow.
It is not known if the ad revenue received by CNN from the fake views was ever returned to the advertisers.
Bots may be used on internet forums to automatically post inflammatory or nonsensical posts to disrupt the forum and anger users.
The most widely used anti-bot technique is the use of CAPTCHA, which is a form of Turing test used to distinguish between a human user and a less-sophisticated AI-powered bot, by the use of graphically-encoded human-readable text.
Examples of providers include Recaptcha, and commercial companies such as Minteye, Solve Media, and NuCaptcha.
Captchas, however, are not foolproof in preventing bots as they can often be circumvented by computer character recognition, security holes, and even by outsourcing captcha solving to cheap laborers.
Companies and customers can benefit from internet bots.
Internet bots are allowing customers to communicate with companies without having to communicate with a person.
KLM Royal Dutch Airlines has produced a chatbot that allows customers to receive boarding passes, check in reminders, and other information that is needed for a flight.
Companies have made chatbots that can benefit customers.
Customer engagement has grown since these chatbots have been developed.
Chat bots are used on a daily basis.
Google Assistant and Siri are considered forms of chat bots.
Google Assistant and Siri allow people to ask questions and get a response using an AI system.
These technological advances are positively benefiting people's daily lives.
State sponsored online sockpuppetry and manipulation of online views is practiced by several countries, in particular by Russia, China, United States, United Kingdom, Israel, Turkey, Iran, Vietnam, India and Ukraine.
The earliest documented allegations of the existence of "web brigades" appear to be in the April 2003 Vestnik Online article "The Virtual Eye of Big Brother" by French journalist Anna Polyanskaya (a former assistant to assassinated Russian politician Galina Starovoitova) and two other authors, Andrey Krivov and Ivan Lomako.
The authors claim that up to 1998, contributions to forums on Russian Internet sites (Runet) predominantly reflected liberal and democratic values, but after 2000, the vast majority of contributions reflected totalitarian values.
This sudden change was attributed to the appearance of teams of pro-Russian commenters who appeared to be organized by the Russian state security service.
According to the authors, about 70% of Russian Internet posters were of generally liberal views prior to 1998–1999, while a surge of "antidemocratic" posts (about 60–80%) suddenly occurred at many Russian forums in 2000.
This could also be a reflection to the fact that access to Internet among the general Russian population soared during this time, which was until then accessible only to some sections of the society.
In January 2012, a hacktivist group calling itself the Russian arm of Anonymous published a massive collection of email allegedly belonging to former and present leaders of the pro-Kremlin youth organization Nashi (including a number of government officials).
Journalists who investigated the leaked information found that the pro-Kremlin movement had engaged in a range of activities including paying commentators to post content and hijacking blog ratings in the fall of 2011.
The e-mails indicated that members of the "brigades" were paid 85 rubles (about US$3) or more per comment, depending on whether the comment received replies.
Some were paid as much as 600,000 roubles (about US$21,000) for leaving hundreds of comments on negative press articles on the internet, and were presented with iPads.
A number of high-profile bloggers were also mentioned as being paid for promoting Nashi and government activities.
The Federal Youth Agency, whose head (and the former leader of Nashi)
Vasily Yakemenko was the highest-ranking individual targeted by the leaks, refused to comment on the authenticity of the e-mails.
In 2013, a Freedom House report stated that 22 of 60 countries examined have been using paid pro-government commentators to manipulate online discussions, and that Russia has been at the forefront of this practice for several years, along with China and Bahrain.
In the same year, Russian reporters investigated the St. Petersburg Internet Research Agency, which employs at least 400 people.
They found that the agency covertly hired young people as "Internet operators" paid to write pro-Kremlin postings and comments, smearing opposition leader Alexei Navalny and U.S. politics and culture.
Some Russian opposition journalists state that such practices create a chilling effect on the few independent media outlets remaining in the country.
Further investigations were performed by Russian opposition newspaper Novaya Gazeta and Institute of Modern Russia in 2014–15, inspired by the peak of activity of the pro-Russian brigades during the Ukrainian conflict and assassination of Boris Nemtsov.
The effort of using "troll armies" to promote Putin's policies is reported to be a multimillion-dollar operation.
According to an investigation by the British Guardian newspaper, the flood of pro-Russian comments is part of a coordinated "informational-psychological war operation".
One Twitter bot network was documented to use more than 20,500 fake Twitter accounts to spam negative comments after the death of Boris Nemtsov and events related to the Ukrainian conflict.
An article based on the original Polyanskaya article, authored by the Independent Customers' Association, was published in May 2008 at Expertiza.
Ru.
In this article the term web brigades is replaced by the term Team "G".During his presidency, Donald Trump retweeted a tweet from an account operated by the Russians.
Web brigades commentators sometimes leave hundreds of postings a day that criticize the country's opposition and promote Kremlin-backed policymakers.
Commentators simultaneously react to discussions of "taboo" topics, including the historical role of Soviet leader Joseph Stalin, political opposition, dissidents such as Mikhail Khodorkovsky, murdered journalists, and cases of international conflict or rivalry (with countries such as Estonia, Georgia, and Ukraine, but also with the foreign policies of the United States and the European Union).
Prominent journalist and Russia expert Peter Pomerantsev believes Russia's efforts are aimed at confusing the audience, rather than convincing it.
He states that they cannot censor information but can "trash it with conspiracy theories and rumours".To avert suspicions, the users sandwich political remarks between neutral articles on travelling, cooking and pets.
They overwhelm comment sections of media to render meaningful dialogue impossible.
A collection of leaked documents, published by Moy Rayon, suggests that work at the "troll den" is strictly regulated by a set of guidelines.
Any blog post written by an agency employee, according to the leaked files, must contain "no fewer than 700 characters" during day shifts and "no fewer than 1,000 characters" on night shifts.
Use of graphics and keywords in the post's body and headline is also mandatory.
In addition to general guidelines, bloggers are also provided with "technical tasks" – keywords and talking points on specific issues, such as Ukraine, Russia's internal opposition and relations with the West.
On an average working day, the workers are to post on news articles 50 times.
Each blogger is to maintain six Facebook accounts publishing at least three posts a day and discussing the news in groups at least twice a day.
By the end of the first month, they are expected to have won 500 subscribers and get at least five posts on each item a day.
On Twitter, the bloggers are expected to manage 10 accounts with up to 2,000 followers and tweet 50 times a day.
In 2015, Lawrence Alexander disclosed a network of propaganda websites sharing the same Google Analytics identifier and domain registration details, allegedly run by Nikita Podgorny from Internet Research Agency.
The websites were mostly meme repositories focused on attacking Ukraine, Euromaidan, Russian opposition and Western policies.
Other websites from this cluster promoted president Putin and Russian nationalism, and spread alleged news from Syria presenting anti-Western and pro-Bashar al-Assad viewpoints.
In August 2015, Russian researchers correlated Google search statistics of specific phrases with their geographic origin, observing increases in specific politically loaded phrases (such as "Poroshenko", "Maidan", "sanctions") starting from 2013 and originating from very small, peripheral locations in Russia, such as Olgino, which also happens to be the headquarters of the Internet Research Agency company.
The  Internet Research Agency also appears to be the primary sponsor of an anti-Western exhibition Material Evidence.
Since 2015, Finnish reporter Jessikka Aro has inquired into web brigades and Russian trolls.
In addition, Western journalists have referred to the phenomenon and have supported traditional media.
In May 2019, it was reported that a study from the George Washington University found that Russian Twitter bots had tried to inflame the United States' anti-vaccination debate by posting opinions on both sides in 2018.In June 2019 a group of 12 editors introducing coordinated pro-government and anti-opposition bias was blocked on the Russian-language Wikipedia.
In July 2019 two operatives of the Internet Research Agency were detained in Libya and charged with attempting to  influence local elections.
They were reportedly employees of Alexander Malkevich, manager of USA Really, a propaganda website.
The WalkAway campaign is a hashtag introduced by American political activist Brandon Straka in a viral video promoting the idea that members of the Democratic Party should "walk away" from the Party due to its political stances.
This online campaign was swiftly supported by Russian bots.
Jolanta Darczewska: The Anatomy of Russian Information Warfare: The Crimean Operation, a Case Study.
Centre for Eastern Studies, Warsaw 2014, ISBN 978-83-62936-45-8 (PDF) Peter Pomerantsev & Michael Weiss:
The Menace of Unreality: How the Kremlin Weaponizes Information, Culture and Money.
The Institute of Modern Russia, New York 2014 (PDF)
The WWW is built on HTTP protocol to transfer information.
To imitate legitimate user behavior, such as voting in an online poll, the attacker sends a HTTP request to particular server hosting the poll.
Analyzing the target, or the voting project, should be done before actually building the votebot.
When handling a voting website for example, one needs to do some webpage analysis on the target, extracting the request URL of the voting action as well as some HTTP header settings to cheat the website.
There are lots of tools which help people to analyze the web, such as Firebug and httpanalyzer.
One can trace the voting process of HTTP packages by these tools and find the right voting target and some simple protecting tricks used by websites, such as referrer verification.
Before sending requests, the attacker must carefully analyze the target and identify potential attack vectors.
During analysis, the attacker must determine if HTTP sessions (maintained via cookies) are necessary to consider or not.
For example, an online poll could require a session so that only authorized users can vote.
Crafting an HTTP request defines how an actual user would behave based on parameters defined in the request.
Two HTTP request methods are useful in voting, POST and PUT.
Request methods are simply different ways to send data to a certain endpoint (i.e., a poll about "How Many Users Like The Votebots Article?").
The below is a simple Python example using httplib2 to send messages (cited from httplib2 wiki):  >>
> from httplib2 import
Http >
>> from urllib import urlencode
h = Http() >
>> data =
dict(name="Joe", comment="A test comment")
>> resp, content = h.request("http://bitworking.org/news/223/Meet-Ares", "POST", urlencode(data))
>> resp
{'status': '200', 'transfer-encoding'
: 'chunked', 'vary': 'Accept-Encoding,User-Agent',  'server': 'Apache', 'connection': 'close', 'date': 'Tue, 31 Jul 2007 15:29:52 GMT',   'content-type': 'text/html'}
In many voting projects, developers try to distinguish the bots from legal users.
They may use the strategy talked about below, and the votebots try to bypass their barriers or detecting methods to successfully vote at the website.
For example, some websites restrict the number of votes one IP address can make in a time period.
Votebots can bypass this rule by proxy its IP address frequently to cheat the website.
Another frequently used strategy is to analyze the account created by a votebot to tell any difference from the normal accounts created by human beings, or to analyze the action history of accounts in the system to find out potential votebots creating ones.
Votebots, on the other hand, try to simulate human action such as logging in and out as well as sharing some articles in some social network service before voting.
YouTube is reported to be a big victim of votebots.
Many small, temporary voting projects are also usual target of votebots.
Many people try to program or buy malicious scripts to vote for themselves in some processes, and it is hard to count the number of attacks that happen every day.
As talked above, web developers want to distinguish votebot from legal voting users in voting projects.
Normal ways includes IP checking, account-handling, Turing test (e.g. CAPTCHA) and account action analysis.
E-mail spambots harvest
e-mail addresses from material found on the Internet in order to build mailing lists for sending unsolicited e-mail, also known as spam.
Such spambots are web crawlers that can gather e-mail addresses from websites, newsgroups, special-interest group (SIG) postings, and chat-room conversations.
Because e-mail addresses have a distinctive format, such spambots are easy to code.
A number of programs and approaches have been devised to foil spambots.
One such technique is address munging, in which an e-mail address is deliberately modified so that a human reader (and/or human-controlled web browser) can interpret it
but spambots cannot.
This has led to the evolution of more sophisticated spambots that are able to recover e-mail addresses from character strings that appear to be munged, or instead can render the text into a web browser and then scrape it for e-mail addresses.
Alternative transparent techniques include displaying all or part of the e-mail address on a web page as an image, a text logo shrunken to normal size using inline CSS, or as text with the order of characters jumbled, placed into readable order at display time using CSS.
Forum spambots browse the internet, looking for guestbooks, wikis, blogs, forums, and other types of web forms that they can then use to submit bogus content.
These often use OCR technology to bypass CAPTCHAs.
Some spam messages are targeted towards readers and can involve techniques of target marketing or even phishing, making it hard to tell real posts from the bot generated ones.
Other spam messages are not meant to be read by humans, but are instead posted to increase the number of links to a particular website, to boost its search engine ranking.
One way to prevent spambots from creating automated posts is to require the poster to confirm their intention to post via e-mail.
Since most spambot scripts use a fake e-mail address when posting, any email confirmation request is unlikely to be successfully routed to them.
Some spambots will pass this step by providing a valid email address and use it for validation, mostly via webmail services.
Using methods such as security questions are also proven to be effective in curbing posts generated by spambots, as they are usually unable to answer it upon registering.
A Twitterbot is a program used to produce automated posts on the Twitter microblogging service, or to automatically follow Twitter users.
Twitterbots come in various forms.
For example, many serve as spam, enticing clicks on promotional links.
Others post @replies or automatically "retweet" in response to tweets that include a certain word or phrase.
These automatic tweets are often seen as fun or silly.
Some Twitter users even program Twitterbots to assist themselves with scheduling or reminders.
The basic attributes of an autonomous software agent are that agents  are not strictly invoked for a task, but activate themselves, may reside in wait status on a host, perceiving context, may get to run status on a host upon starting conditions, do not require interaction of user, may invoke other tasks including communication.
The term "agent" describes a software abstraction, an idea, or a concept, similar to OOP terms such as methods, functions, and objects.
The concept of an agent provides a convenient and powerful way to describe a complex software entity that is capable of acting with a certain degree of autonomy in order to accomplish  tasks on behalf of its host.
But unlike objects, which are defined in terms of methods and attributes, an agent is defined in terms of its behavior.
Various authors have proposed different definitions of agents, these commonly include concepts such as  persistence (code is not executed on demand but runs continuously and decides for itself when it should perform some activity) autonomy (agents have capabilities of task selection, prioritization, goal-directed behavior, decision-making without human intervention) social ability (agents are able to engage other components through some sort of communication and coordination, they may collaborate on a task) reactivity (agents perceive the context in which they operate and react to it appropriately).
All agents are programs, but not all programs are agents.
Contrasting the term with related concepts may help clarify its meaning.
Franklin & Graesser (1997) discuss four key notions that distinguish agents from arbitrary programs: reaction to the environment, autonomy, goal-orientation and persistence.
Agents are more autonomous than objects.
Agents have flexible behaviour: reactive, proactive, social.
Agents have at least one thread of control but may have more.
Expert systems are not coupled to their environment.
Expert systems are not designed for reactive, proactive behavior.
Expert systems do not consider social ability.
Intelligent agents (also known as rational agents) are not just computer programs: they may also be machines, human beings, communities of human beings (such as firms) or anything that is capable of goal-directed behavior.(Russell & Norvig 2003)
Software agents may offer various benefits to their end users by automating complex or repetitive tasks.
However, there are organizational and cultural impacts of this technology that need to be considered prior to implementing software agents.
People like to perform easy tasks providing the sensation of success unless the repetition of the simple tasking is affecting the overall output.
In general implementing software agents to perform administrative requirements provides a substantial increase in work contentment, as administering their own work does never please the worker.
The effort freed up serves for a higher degree of engagement in the substantial tasks of individual work.
Hence, software agents may provide the basics to implement self-controlled work, relieved from hierarchical controls and interference.
Such conditions may be secured by application of software agents for required formal support.
The cultural effects of the implementation of software agents include trust affliction, skills erosion, privacy attrition and social detachment.
Some users may not feel entirely comfortable fully delegating important tasks to software applications.
Those who start relying solely on intelligent agents may lose important skills, for example, relating to information literacy.
In order to act on a user's behalf, a software agent needs to have a complete understanding of a user's profile, including his/her personal preferences.
This, in turn, may lead to unpredictable privacy issues.
When users start relying on their software agents more, especially for communication activities, they may lose contact with other human users and look at the world with the eyes of their agents.
These consequences are what agent researchers and users must consider when dealing with intelligent agent technologies.
The concept of an agent can be traced back to Hewitt's Actor Model (Hewitt, 1977) - "A self-contained, interactive and concurrently-executing object, possessing internal state and communication capability.
To be more academic, software agent systems are a direct evolution of Multi-Agent Systems (MAS).
MAS evolved from Distributed Artificial Intelligence (DAI), Distributed Problem Solving (DPS) and Parallel AI (PAI), thus inheriting all characteristics (good and bad) from DAI and AI.
John Sculley's 1987 “
Knowledge Navigator” video portrayed an image of a relationship between end-users and agents.
Being an ideal first, this field experienced a series of unsuccessful top-down implementations, instead of a piece-by-piece, bottom-up approach.
The range of agent types is now (from 1990) broad: WWW, search engines, etc.
Buyer agents travel around a network (e.g. the internet) retrieving information about goods and services.
These agents, also known as 'shopping bots', work very efficiently for commodity products such as CDs, books, electronic components, and other one-size-fits-all products.
Buyer agents are typically optimized to allow for digital payment services used in e-commerce and traditional businesses.
User agents, or personal agents, are intelligent agents that take action on your behalf.
In this category belong those intelligent agents that already perform, or will shortly perform, the following tasks:  Check your e-mail, sort it according to the user's order of preference, and alert you when important emails arrive.
Play computer games as your opponent or patrol game areas for you.
Assemble customized news reports for you.
There are several versions of these, including CNN.
Find information for you on the subject of your choice.
Fill out forms on the Web automatically for you, storing your information for future reference Scan Web pages looking for and highlighting text that constitutes the "important" part of the information there Discuss topics with you ranging from your deepest fears to sports Facilitate with online job search duties by scanning known job boards and sending the resume to opportunities who meet the desired criteria Profile synchronization across heterogeneous social networks
Monitoring and Surveillance Agents are used to observe and report on equipment, usually computer systems.
The agents may keep track of company inventory levels, observe competitors' prices and relay them back to the company, watch stock manipulation by insider trading and rumors, etc.
For example, NASA's Jet Propulsion Laboratory has an agent that monitors inventory, planning, schedules equipment orders to keep costs down, and manages food storage facilities.
These agents usually monitor complex computer networks that can keep track of the configuration of each computer connected to the network.
A special case of Monitoring-and-Surveillance agents are organizations of agents used to emulate the Human Decision-Making process during tactical operations.
The agents monitor the status of assets (ammunition, weapons available, platforms for transport, etc.) and receive Goals (Missions) from higher level agents.
The Agents then pursue the Goals with the Assets at hand, minimizing expenditure of the Assets while maximizing Goal Attainment.
(See Popplewell, "Agents and Applicability")
This agent uses information technology to find trends and patterns in an abundance of information from many different sources.
The user can sort through this information in order to find whatever information they are seeking.
A data mining agent operates in a data warehouse discovering information.
A 'data warehouse' brings together information from lots of different sources.
Data mining" is the process of looking through the data warehouse to find information that you can use to take action, such as ways to increase sales or keep customers who are considering defecting.
'Classification' is one of the most common types of data mining, which finds patterns in information and categorizes them into different classes.
Data mining agents can also detect major shifts in trends or a key indicator and can detect the presence of new information and alert you to it.
For example, the agent may detect a decline in the construction industry for an economy; based on this relayed information construction companies will be able to make intelligent decisions regarding the hiring/firing of employees or the purchase/lease of equipment in order to best suit their firm.
Some other examples of current intelligent agents include some spam filters, game bots, and server monitoring tools.
Search engine indexing bots also qualify as intelligent agents.
User agent - for browsing the World Wide Web Mail transfer agent -
For serving E-mail, such as Microsoft Outlook.
Why?
It communicates with the POP3 mail server, without users having to understand POP3 command protocols.
It even has rule sets that filter mail for the user, thus sparing them the trouble of having to do it themselves.
SNMP agent
In Unix-style networking servers, httpd is an HTTP daemon that implements the Hypertext Transfer Protocol at the root of the World Wide Web Management agents used to manage telecom devices Crowd simulation for safety planning or 3D computer graphics, Wireless beaconing agent is a simple process hosted single tasking entity for implementing wireless lock or electronic leash in conjunction with more complex software agents hosted e.g. on wireless receivers.
Use of autonomous agents (deliberately equipped with noise) to optimize coordination in groups online.
Software bots are becoming important in software engineering.
An example of a software bot is a bot that automatically repairs continuous integration build failures.
Issues to consider in the development of agent-based systems include   how tasks are scheduled and how synchronization of tasks is achieved how tasks are prioritized by agents how agents can collaborate, or recruit resources, how agents can be re-instantiated in different environments, and how their internal state can be stored, how the environment will be probed and how a change of environment leads to behavioral changes of the agents how messaging and communication can be achieved, what hierarchies of agents are useful (e.g. task execution agents, scheduling agents, resource providers ...)
.For
software agents to work together efficiently
they must share semantics of their data elements.
This can be done by having computer systems publish their metadata.
The definition of agent processing can be approached from two interrelated directions:  internal state processing and ontologies for representing knowledge interaction protocols – standards for specifying communication of tasksAgent systems are used to model real-world systems with concurrency or parallel processing.
Agent Machinery – Engines of various kinds, which support the varying degrees of intelligence Agent Content – Data employed by the machinery in Reasoning and Learning Agent Access – Methods to enable the machinery to perceive content and perform actions as outcomes of Reasoning Agent Security – Concerns related to distributed computing, augmented by a few special concerns related to agentsThe agent uses its access methods to go out into local and remote databases to forage for content.
These access methods may include setting up news stream delivery to the agent, or retrieval from bulletin boards, or using a spider to walk the Web.
The content that is retrieved in this way is probably already partially filtered – by the selection of the newsfeed or the databases that are searched.
The agent next may use its detailed searching or language-processing machinery to extract keywords or signatures from the body of the content that has been received or retrieved.
This abstracted content (or event) is then passed to the agent's Reasoning or inferencing machinery in order to decide what to do with the new content.
This process combines the event content with the rule-based or knowledge content provided by the user.
If this process finds a good hit or match in the new content, the agent may use another piece of its machinery to do a more detailed search on the content.
Finally, the agent may decide to take an action based on the new content; for example, to notify the user that an important event has occurred.
This action is verified by a security function and then given the authority of the user.
The agent makes use of a user-access method to deliver that message to the user.
If the user confirms that the event is important by acting quickly on the notification, the agent may also employ its learning machinery to increase its weighting for this kind of event.
Bots can act on behalf of their creators to do good as well as bad.
There are a few ways which bots can be created to demonstrate that they are designed with the best intention and are not built to do harm.
This is first done by having a bot identify itself in the user-agent HTTP header when communicating with a site.
The source IP address must also be validated to establish itself as legitimate.
Next, the bot must also always respect a site's robots.txt file since it has become the standard across most of the web.
And like respecting the robots.txt file, bots should shy away from being too aggressive and respect any crawl delay instructions.
DAML (DARPA Agent Markup Language)
Jason (multi-agent systems development platform) 3APL (Artificial Autonomous Agents Programming Language) GOAL agent programming language Web Ontology Language (OWL) daemons in Unix-like systems.
Java Agent Template (JAT) Java Agent Development Framework (JADE) SARL agent programming language (arguably an Actor and not Agent oriented paradigm)
Often, an IRC bot is deployed as a detached program running from a stable host.
It sits on an IRC channel to keep it open and prevents malicious users from taking over the channel.
It can be configured to give channel operator status to privileged users when they join the channel, and can provide a unified channel operator list.
Many of these features require that the bot be a channel operator.
Thus, most IRC bots are run from computers which have long uptimes (generally running a BSD derivative or Linux) and a fast, stable Internet connection.
As IRC has become popular with many dial-up users as well, shell accounts at shell providers have become popular as a stable Linux server with a decent connection to run a bot from.
Aside from managing channel permissions, a bot can also perform functions such as logging what is posted to an IRC channel, giving out information on demand (very popular in IRC channels dealing with user support), creating statistics tracking the channel's top posters and longest-lived lurkers, or hosting trivia, Uno and other games.
These functions are usually provided by scripts, often written in a scripting programming language such as Tcl or Perl by the bot's users.
Channels dedicated to file sharing often use XDCC bots to distribute their files.
IRC bots are particularly useful on IRC networks such as EFnet and IRCnet without channel registration services, and on networks like Undernet or QuakeNet that require conditions to be met (minimum user count, etc.)
before a channel may be registered.
Where bots are used for administrative functions such as this, they may need more access than a normal client connection allows.
Some versions of IRC have a "Service" protocol that allows clients with these extra powers.
Such server-sanctioned bots are called IRC services.
Bots are not always welcome.
Some IRC networks forbid the usage of bots.
One of the reasons for doing so is that each nickname connected to the network increases the size of the network database which is being kept in sync across all servers.
Allowing for bots in large networks can cause a relevant amount of network traffic overhead which needs to be financed and may even lead to netsplits.
Basshunter's 2006 song, Boten Anna, is about a female IRC user mistaken for an IRC bot
Following tests of a new instant messaging platform on Facebook in March 2008, the feature, then-titled "Facebook Chat", was gradually released to users in April 2008.
Facebook revamped its messaging platform in November 2010, and subsequently acquired group messaging service Beluga in March 2011, which the company used to launch its standalone iOS and Android mobile apps on August 9, 2011.
Facebook later launched a BlackBerry version in October 2011.
An app for Windows Phone, though lacking features including voice messaging and chat heads, was released in March 2014.
In April 2014, Facebook announced that the messaging feature would be removed from the main Facebook app and users will be required to download the separate Messenger app.
An iPad-optimized version of the iOS app was released in July 2014.
In April 2015, Facebook launched a website interface for Messenger.
A Tizen app was released on July 13, 2015.
Facebook launched Messenger for Windows 10 in April 2016.
In October 2016, Facebook released Facebook Messenger Lite, a stripped-down version of Messenger with a reduced feature set.
The app is aimed primarily at old Android phones and regions where high-speed Internet is not widely available.
In April 2017, Facebook Messenger Lite was expanded to 132 more countries.
In May 2017, Facebook revamped the design for Messenger on Android and iOS, bringing a new home screen with tabs and categorization of content and interactive media, red dots indicating new activity, and relocated sections.
Facebook announced a Messenger program for Windows 7 in a limited beta test in November 2011.
The following month, Israeli blog TechIT leaked a download link for the program, with Facebook subsequently confirming and officially releasing the program.
The program was eventually discontinued in March 2014.
A Firefox web browser add-on was released in December 2012, but was also discontinued in March
2014.In December 2017, Facebook announced Messenger Kids, a new app aimed for persons under 13 years of age.
The app comes with some differences compared to the standard version.
The following is a table of features available in Facebook Messenger, as well as their geographical coverage and what devices they are available on:
In January 2017, Facebook announced that it was testing showing advertisements in Facebook Messenger's home feed.
At the time, the testing was limited to a "small number of users in Australia and Thailand", with the ad format being swipe-based carousel ads.
In July, the company announced that they were expanding the testing to a global audience.
Stan Chudnovsky, head of Messenger, told VentureBeat that "We'll start slow ...
When the average user can be sure to see them we truly don't know because we're just going to be very data-driven and user feedback-driven on making that decision".
Facebook told TechCrunch that the advertisements' placement in the inbox depends on factors such as thread count, phone screen size, and pixel density.
In a TechCrunch editorial by Devin Coldewey, he described the ads as "huge" in the space they occupy, "intolerable" in the way they appear in the user interface, and "irrelevant" due to the lack of context.
Coldewey finished by writing "Advertising is how things get paid for on the internet, including TechCrunch, so I'm not an advocate of eliminating it or blocking it altogether.
But bad advertising experiences can spoil a perfectly good app like (for the purposes of argument) Messenger.
Messaging is a personal, purposeful use case and these ads are a bad way to monetize it.
In November 2014, the Electronic Frontier Foundation (EFF) listed Facebook Messenger (Facebook chat) on its Secure Messaging Scorecard.
It received a score of 2 out of 7 points on the scorecard.
It received points for having communications encrypted in transit and for having recently completed an independent security audit.
It missed points because the communications were not encrypted with keys the provider didn't have access to, users could not verify contacts' identities, past messages were not secure if the encryption keys were stolen, the source code was not open to independent review, and the security design was not properly documented.
As stated by Facebook in its Help Center, there is no way to log out of the Facebook Messenger application.
Instead, users can choose between different availability statuses, including "Appear as inactive", "Switch accounts", and "Turn off notifications".
Media outlets have reported on a workaround, by pressing a "Clear data" option in the application's menu in Settings on Android devices, which returns the user to the log-in screen.
After being separated from the main Facebook app, Facebook Messenger had 600 million users in April 2015.
This grew to 900 million in June 2016, 1 billion in July 2016, and 1.2 billion in April 2017.
In early 2018, the US Department of Justice went to court to attempt to force Facebook to modify its Messenger app to enable surveillance by third parties so that agents could listen in on encrypted voice conversations over Messenger.
:1
The court decided against the Justice Department, but sealed the case.
:1
In November 2018, the ACLU and EFF filed suit to have the case unsealed so that the public can be informed about the encryption/surveillance debate.
:1
A botnet is a logical collection of Internet-connected devices such as computers, smartphones or IoT devices whose security have been breached and control ceded to a third party.
Each compromised device, known as a "bot", is created when a device is penetrated by software from a malware (malicious software) distribution.
The controller of a botnet is able to direct the activities of these compromised computers through communication channels formed by standards-based network protocols, such as IRC and Hypertext Transfer Protocol (HTTP).Botnets are increasingly rented out by cyber criminals as commodities for a variety of purposes.
Botnet architecture has evolved over time in an effort to evade detection and disruption.
Traditionally, bot programs are constructed as clients which communicate via existing servers.
This allows the bot herder (the person controlling the botnet) to perform all control from a remote location, which obfuscates their traffic.
Many recent botnets now rely on existing peer-to-peer networks to communicate.
These P2P bot programs perform the same actions as the client-server model, but they do not require a central server to communicate.
The first botnets on the internet used a client-server model to accomplish their tasks.
Typically, these botnets operate through Internet Relay Chat networks, domains, or websites.
Infected clients access a predetermined location and await incoming commands from the server.
The bot herder sends commands to the server, which relays them to the clients.
Clients execute the commands and report their results back to the bot herder.
In the case of IRC botnets, infected clients connect to an infected IRC server and join a channel pre-designated for C&C by the bot herder.
The bot herder sends commands to the channel via the IRC server.
Each client retrieves the commands and executes them.
Clients send messages back to the IRC channel with the results of their actions.
In response to efforts to detect and decapitate IRC botnets, bot herders have begun deploying malware on peer-to-peer networks.
These bots may use digital signatures so that only someone with access to the private key can control the botnet.
See e.g. Gameover ZeuS and ZeroAccess botnet.
Newer botnets fully operate over P2P networks.
Rather than communicate with a centralized server, P2P bots perform as both a command distribution server and a client which receives commands.
This avoids having any single point of failure, which is an issue for centralized botnets.
In order to find other infected machines, the bot discreetly probes random IP addresses until it contacts another infected machine.
The contacted bot replies with information such as its software version and list of known bots.
If one of the bots' version is lower than the other, they will initiate a file transfer to update.
This way, each bot grows its list of infected machines and updates itself by periodically communicating to all known bots.
A botnet's originator (known as a "bot herder" or "bot master") controls the botnet remotely.
This is known as the command-and-control (C&C).
The program for the operation which must communicate via a covert channel to the client on the victim's machine (zombie computer).
IRC is a historically favored means of C&C because of its communication protocol.
A bot herder creates an IRC channel for infected clients to join.
Messages sent to the channel are broadcast to all channel members.
The bot herder may set the channel's topic to command the botnet.
E.g. the message :herder!herder@example.com TOPIC #channel DDoS www.victim.com from the bot herder alerts all infected clients belonging to #channel to begin a DDoS attack on the website www.victim.com.
An example response :bot1!bot1@compromised.net PRIVMSG #channel
I am DDoSing www.victim.com by a bot client alerts the bot herder that it has begun the attack.
Some botnets implement custom versions of well-known protocols.
The implementation differences can be used for detection of botnets.
For example, Mega-D features a slightly modified SMTP implementation for testing spam capability.
Bringing down the Mega-D's SMTP server disables the entire pool of bots that rely upon the same SMTP server.
In computer science, a zombie computer is a computer connected to the Internet that has been compromised by a hacker, computer virus or trojan horse and can be used to perform malicious tasks of one sort or another under remote direction.
Botnets of zombie computers are often used to spread e-mail spam and launch denial-of-service attacks.
Most owners of zombie computers are unaware that their system is being used in this way.
Because the owner tends to be unaware, these computers are metaphorically compared to zombies.
A coordinated DDoS attack by multiple botnet machines also resembles a zombie horde attack.
Many computer users are unaware that their computer is infected with bots.
The process of stealing computing resources as a result of a system being joined to a "botnet" is sometimes referred to as "scrumping".
Botnet Command and control (C&C) protocols have been implemented in a number of ways, from traditional IRC approaches to more sophisticated versions.
Telnet botnets use a simple C&C botnet Protocol in which bots connect to the main command server to host the botnet.
Bots are added to the botnet by using a scanning script, the scanning script is run on an external server and scans IP ranges for telnet and SSH server default logins.
Once a login is found it is added to an infection list and infected with a malicious infection line via SSH on from the scanner server.
When the SSH command is run it infects the server and commands the server to ping to the control server and becomes its slave from the malicious code infecting it.
Once servers are infected to the server the bot controller can launch DDoS attacks of high volume using the C&C panel on the host server.
IRC networks use simple, low bandwidth communication methods, making them widely used to host botnets.
They tend to be relatively simple in construction and have been used with moderate success for coordinating DDoS attacks and spam campaigns while being able to continually switch channels to avoid being taken down.
However, in some cases, merely blocking of certain keywords has proven effective in stopping IRC-based botnets.
The RFC 1459 (IRC) standard is popular with botnets.
The first known popular botnet controller script, "MaXiTE Bot" was using IRC XDCC protocol for private control commands.
One problem with using IRC is that each bot client must know the IRC server, port, and channel to be of any use to the botnet.
Anti-malware organizations can detect and shut down these servers and channels, effectively halting the botnet attack.
If this happens, clients are still infected, but they typically lie dormant since they have no way of receiving instructions.
To mitigate this problem, a botnet can consist of several servers or channels.
If one of the servers or channels becomes disabled, the botnet simply switches to another.
It is still possible to detect and disrupt additional botnet servers or channels by sniffing IRC traffic.
A botnet adversary can even potentially gain knowledge of the control scheme and imitate the bot herder by issuing commands correctly.
Since most botnets using IRC networks and domains can be taken down with time, hackers have moved to P2P botnets with C&C as a way to make it harder to be taken down.
Some have also used encryption as a way to secure or lock down the botnet from others, most of the time when they use encryption it is public-key cryptography and has presented challenges in both implementing it and breaking it.
Many large botnets tend to use domains rather than IRC in their construction (see Rustock botnet and Srizbi botnet).
They are usually hosted with bulletproof hosting services.
This is one of the earliest types of
C&C. A zombie computer accesses a specially-designed webpage or domain(s) which serves the list of controlling commands.
The advantages of using web pages or domains as C&C is that a large botnet can be effectively controlled and maintained with very simple code that can be readily updated.
Disadvantages of using this method are that it uses a considerable amount of bandwidth at large scale, and domains can be quickly seized by government agencies without much trouble or effort.
If the domains controlling the botnets are not seized, they are also easy targets to compromise with denial-of-service attacks.
Fast-flux DNS can be used as a way to make it difficult to track down the control servers, which may change from day to day.
Control servers may also hop from DNS domain to DNS domain, with domain generation algorithms being used to create new DNS names for controller servers.
Some botnets use free DNS hosting services such as DynDns.org, No-IP.com, and Afraid.org to point a subdomain towards an IRC server that harbors the bots.
While these free DNS services do not themselves host attacks, they provide reference points (often hard-coded into the botnet executable).
Removing such services can cripple an entire botnet.
Calling back to large social media sites such as GitHub, Twitter, Reddit, Instagram, the XMPP open source instant message protocol and Tor hidden services are popular ways of avoiding egress filtering to communicate with a C&C server.
This example illustrates how a botnet is created and used for malicious gain.
A hacker purchases or builds a Trojan and/or exploit kit and uses it to start infecting users' computers, whose payload is a malicious application
—the bot.
The bot instructs the infected PC to connect to a particular command-and-control (C&C) server.
(This allows the botmaster to keep logs of how many bots are active and online.)
The botmaster may then use the bots to gather keystrokes or use form grabbing to steal online credentials and may rent out the botnet as DDoS and/or spam as a service or sell the credentials online for a profit.
Depending on the quality and capability of the bots, the value is increased or decreased.
Newer bots can automatically scan their environment and propagate themselves using vulnerabilities and weak passwords.
Generally, the more vulnerabilities a bot can scan and propagate through, the more valuable it becomes to a botnet controller community.
Computers can be co-opted into a botnet when they execute malicious software.
This can be accomplished by luring users into making a drive-by download, exploiting web browser vulnerabilities, or by tricking the user into running a Trojan horse program, which may come from an email attachment.
This malware will typically install modules that allow the computer to be commanded and controlled by the botnet's operator.
After the software is downloaded, it will call home (send a reconnection packet) to the host computer.
When the re-connection is made, depending on how it is written, a Trojan may then delete itself or may remain present to update and maintain the modules.
Calling back to large social media sites such as GitHub, Twitter, Reddit, Instagram, the XMPP open source instant message protocol and Tor hidden services are popular ways of avoiding egress filtering to communicate with a C&C server.
Most botnets currently feature distributed denial-of-service attacks in which multiple systems submit as many requests as possible to a single Internet computer or service, overloading it and preventing it from servicing legitimate requests.
An example is an attack on a victim's server.
The victim's server is bombarded with requests by the bots, attempting to connect to the server, therefore, overloading it.
Spyware is software which sends information to its creators about a user's activities – typically passwords, credit card numbers and other information that can be sold on the black market.
Compromised machines that are located within a corporate network can be worth more to the bot herder, as they can often gain access to confidential corporate information.
Several targeted attacks on large corporations aimed to steal sensitive information, such as the Aurora botnet.
E-mail spam are e-mail messages disguised as messages from people, but are either advertising, annoying, or malicious.
Click fraud occurs when the user's computer visits websites without the user's awareness to create false web traffic for personal or commercial gain.
Bitcoin mining was used in some of the more recent botnets have which include bitcoin mining as a feature in order to generate profits for the operator of the botnet.
Self-spreading functionality, to seek for pre-configured command-and-control (CNC) pushed instruction contains targeted devices or network, to aim for more infection, is also spotted in several botnets.
Some of the botnets are utilizing this function to automate their infections.
The botnet controller community features a constant and continuous struggle over who has the most bots, the highest overall bandwidth, and the most "high-quality" infected machines, like university, corporate, and even government machines.
While botnets are often named after the malware that created them, multiple botnets typically use the same malware but are operated by different entities.
Botnets can be used for many electronic scams.
These botnets can be used to distribute malware such as viruses to take control of a regular users computer/software
By taking control of someone's personal computer they have unlimited access to their personal information, including passwords and login information to accounts.
This is called phishing.
Phishing is the acquiring of login information to the "victim's" accounts with a link the "victim" clicks on that is sent through an email or text.
A survey by Verizon found that around two-thirds of electronic "espionage" cases come from phishing.
The geographic dispersal of botnets means that each recruit must be individually identified/corralled/repaired and limits the benefits of filtering.
Computer security experts have succeeded in destroying or subverting malware command and control networks, by, among other means, seizing servers or getting them cut off from the Internet, denying access to domains that were due to be used by malware to contact its C&C infrastructure, and, in some cases, breaking into the C&C network itself.
In response to this, C&C operators have resorted to using techniques such as overlaying their C&C networks on other existing benign infrastructure such as IRC or Tor, using peer-to-peer networking systems that are not dependent on any fixed servers, and using public key encryption to defeat attempts to break into or spoof the network.
Norton AntiBot was aimed at consumers, but most target enterprises and/or ISPs.
Host-based techniques use heuristics to identify bot behavior that has bypassed conventional anti-virus software.
Network-based approaches tend to use the techniques described above; shutting down C&C servers, null-routing DNS entries, or completely shutting down IRC servers.
BotHunter is software, developed with support from the U.S. Army Research Office, that detects botnet activity within a network by analyzing network traffic and comparing it to patterns characteristic of malicious processes.
Researchers at Sandia National Laboratories are analyzing botnets' behavior by simultaneously running
one million Linux kernels—a similar scale to a botnet—as virtual machines on a 4,480-node high-performance computer cluster to emulate a very large network, allowing them to watch how botnets work and experiment with ways to stop them.
Detecting automated bot attacks is becoming more difficult each day as newer and more sophisticated generations of bots are getting launched by attackers.
For example, an automated attack can deploy a large bot army and apply brute-force methods with highly accurate username and password lists to hack into accounts.
The idea is to overwhelm sites with tens of thousands of requests from different IPs all over the world, but with each bot only submitting a single request every 10 minutes or so, which can result in more than 5 million attempts per day.
In these cases, many tools try to leverage volumetric detection, but automated bot attacks now have ways of circumventing triggers of volumetric detection.
One of the techniques for detecting these bot attacks is what's known as "signature-based systems" in which the software will attempt to detect patterns in the request packet.
But attacks are constantly evolving, so this may not be a viable option when patterns can't be discerned from thousands of requests.
There's also the behavioral approach to thwarting bots, which ultimately is trying distinguish bots from humans.
By identifying non-human behavior and recognizing known bot behavior, this process can be applied at the user, browser, and network levels.
The most capable method of using software to combat against a virus has been to utilize honeypot software in order to convince the malware that a system is vulnerable.
The malicious files are then analyzed using forensic software.
On July 15, 2014, the Subcommittee on Crime and Terrorism of the Committee on the Judiciary, United States Senate, held a hearing on the threats posed by botnets and the public and private efforts to disrupt and dismantle them.
The first botnet was first acknowledged and exposed by Earthlink during a lawsuit with notorious spammer Khan C. Smith in 2001 for the purpose of bulk spam accounting for nearly 25% of all spam at the time.
Around 2006, to thwart detection, some botnets were scaling back in size.
Researchers at the University of California, Santa Barbara took control of a botnet that was six times smaller than expected.
In some countries, it is common that users change their IP address a few times in one day.
Estimating the size of the botnet by the number of IP addresses is often used by researchers, possibly leading to inaccurate assessments.
Radio Rex was the first voice activated toy released in 1911.Another early tool which was enabled to perform digital speech recognition was the IBM Shoebox, presented to the general public during the 1962 Seattle World's Fair after its initial market launch in 1961.
This early computer, developed almost 20 years before the introduction of the first IBM Personal Computer in 1981, was able to recognize 16 spoken words and the digits 0 to 9.
The next milestone in the development of voice recognition technology was achieved in the 1970s at the Carnegie Mellon University in Pittsburgh, Pennsylvania with substantial support of the United States Department of Defense and its DARPA agency.
Their tool "Harpy" mastered about 1000 words, the vocabulary of a three-year-old.
About ten years later the same group of scientists developed a system that could analyze not only individual words but entire word sequences enabled by a Hidden Markov Model.
Thus, the earliest virtual assistants, which applied speech recognition software were automated attendant and medical digital dictation software.
In the 1990s digital speech recognition technology became a feature of the personal computer with Microsoft, IBM, Philips and Lernout & Hauspie fighting for customers.
Much later the market launch of the first smartphone IBM Simon in 1994 laid the foundation for smart virtual assistants as we know them today.
The first modern digital virtual assistant installed on a smartphone was Siri, which was introduced as a feature of the iPhone 4S on October 4, 2011.
Apple Inc. developed Siri following the 2010 acquisition of Siri Inc., a spin-off of SRI International, which is a research institute financed by DARPA and the United States Department of Defense.
Virtual assistants make work via:  Text, including: online chat (especially in an instant messaging app or other app), SMS Text, e-mail or other text-based communication channel, for example Conversica's Intelligent Virtual Assistants for business.
Voice, for example with Amazon Alexa on the Amazon Echo device, Siri on an iPhone, or Google Assistant on Google-enabled/Android mobile devices By taking and/or uploading images, as in the case of Samsung Bixby on the Samsung Galaxy S8Some virtual assistants are accessible via multiple methods, such as Google Assistant via chat on the Google Allo and Google Messages app and via voice on Google Home smart speakers.
Virtual assistants use natural language processing (NLP) to match user text or voice input to executable commands.
Many continually learn using artificial intelligence techniques including machine learning.
To activate a virtual assistant using the voice, a wake word might be used.
This is a word or groups of words such as "Hey Siri", "OK Google" or "Hey Google", "Alexa", and "Hey Microsoft".
Virtual assistants may be integrated into many types of platforms or, like Amazon Alexa, across several of them:  Into devices like smart speakers such as Amazon Echo, Google Home and Apple HomePod
In instant messaging apps on both smartphones and via the Web, e.g. Facebook's M (virtual assistant) on both Facebook and Facebook Messenger apps or via the Web Built into a mobile operating system (OS), as are Apple's Siri on iOS devices and BlackBerry Assistant on BlackBerry 10 devices, or into a desktop OS such as Cortana on Microsoft Windows OS Built into a smartphone independent of the OS, as is Bixby on the Samsung Galaxy S8 and Note 8.
Within instant messaging platforms, assistants from specific organizations, such as Aeromexico's Aerobot on Facebook Messenger or Wechat Secretary on WeChat Within mobile apps from specific companies and other organizations, such as Dom from Domino's Pizza In appliances, cars, and wearable technology.
Previous generations of virtual assistants often worked on websites, such as Alaska Airlines' Ask Jenn, or on interactive voice response (IVR) systems such as American Airlines' IVR by Nuance.
Virtual assistants can provide a wide variety of services.
These include: Provide information such as weather, facts from e.g. Wikipedia or IMDb, set an alarm, make to-do lists and shopping lists Play music from streaming services such as Spotify and Pandora; play radio stations; read audiobooks Play videos, TV shows or movies on televisions, streaming from e.g. Netflix Conversational commerce (see below)
Assist public interactions with government (see Artificial intelligence in government)
Complement and/or replace customer service by humans.
One report estimated that an automated online assistant produced a 30% decrease in the work-load for a human-provided call centre.
Conversational commerce is e-commerce via various means of messaging, including via voice assistants but also live chat on e-commerce Web sites, live chat on messaging apps such as WeChat, Facebook Messenger and WhatsApp and chatbots on messaging apps or Web sites.
Amazon enables Alexa "Skills" and Google "Actions", essentially apps that run on the assistant platforms.
Virtual assistants have a variety of privacy concerns associated with them.
Features such as activation by voice pose a threat, as such features requires the device to always be listening.
Modes of privacy such as the virtual security button have been proposed to create a multilayer authentication for virtual assistants.
Notable developer platforms for virtual assistants include:
Amazon Lex was opened to developers in April 2017.
It involves natural language understanding technology combined with automatic speech recognition and had been introduced in November 2016.
Google provides the Actions on Google and Dialogflow platforms for developers to create "Actions" for Google Assistant Apple provides SiriKit for developers to create extensions for Siri IBM's Watson, while sometimes spoken of as a virtual assistant is in fact an entire artificial intelligence platform and community powering some virtual assistants, chatbots. and many other types of solutions.
In previous generations of text chat-based virtual assistants, the assistant was often represented by an avatar of (a.k.a. 'interactive online character or automated character)
— this was known as an embodied agent.
Digital experiences enabled by virtual assistants are considered to be among the major recent technological advances and most promising consumer trends.
Experts claim that digital experiences will achieve a status-weight comparable to ‘real' experiences, if not become more sought-after and prized.
The trend is verified by a high number of frequent users and the substantial growth of worldwide user numbers of virtual digital assistants.
In mid-2017, the number of frequent users of digital virtual assistants is estimated to be around 1bn worldwide.
In addition, it can be observed that virtual digital assistant technology is no longer restricted to smartphone applications, but present across many industry sectors (incl.
automotive, telecommunications, retail, healthcare and education).
In response to the significant R&D expenses of firms across all sectors and an increasing implementation of mobile devices, the market for speech recognition technology is predicted to grow at a CAGR of 34.9% globally over the period of 2016 to 2024 and thereby surpass a global market size of USD 7.5 billion by 2024.
According to an Ovum study, the "native digital assistant installed base" is projected to exceed the world's population by 2021, with 7.5 billion active voice AI–capable devices.
According to Ovum, by that time "Google Assistant will dominate the voice AI–capable device market with 23.3% market share, followed by Samsung's Bixby (14.5%),
Apple's Siri (13.1%), Amazon's Alexa (3.9%), and Microsoft's Cortana (2.3%)."Taking into consideration the regional distribution of market leaders, North American companies (e.g. Nuance Communications, IBM, eGain) are expected to dominate the industry over the next years, due to the significant impact of BYOD (Bring Your Own Device) and enterprise mobility business models.
Furthermore, the increasing demand for smartphone-assisted platforms are expected to further boost the North American Intelligent Virtual Assistant (IVA) industry growth.
Despite its smaller size in comparison to the North American market, the intelligent virtual assistant industry from the Asia-Pacific region, with its main players located in India and China is predicted to grow at an annual growth rate of 40% (above global average) over the 2016-2024 period.
In May 2018, researchers from the University of California, Berkeley, published a paper that showed audio commands undetectable for the human ear could be directly embedded into music or spoken text, thereby manipulating virtual assistants into performing certain actions without the user taking note of it.
The researchers made small changes to audio files, which cancelled out the sound patterns that speech recognition systems are meant to detect.
These were replaced with sounds that would be interpreted differently by the system and command it to dial phone numbers, open websites or even transfer money.
The possibility of this has been known since 2016, and affects devices from Apple, Amazon and Google.
In addition to unintentional actions and voice recording, another security and privacy risk associated with intelligent virtual assistants is malicious voice commands: An attacker who impersonates a user and issues malicious voice commands to, for example, unlock a smart door to gain unauthorized entry to a home or garage or order items online without the user's knowledge.
Although some IVAs provide a voice-training feature to prevent such impersonation, it can be difficult for the system to distinguish between similar voices.
Thus, a malicious person who is able to access an IVA-enabled device might be able to fool the system into thinking that he or she is the real owner and carry out criminal or mischievous acts.
Before the 1990s, "wizard" was a common term for a technical expert, somewhat akin to "hacker."When developing the first version of its desktop publishing software, Microsoft Publisher, around 1991, Microsoft wanted to let users with no graphic design skill make documents that still looked good.
Publisher was targeted at non-professionals, and Microsoft figured that, no matter what tools the program had, users wouldn't know what to do with them.
Publisher's "Page Wizards" instead provided a set of forms to produce a complete document layout, based on a professionally-designed template, which could then be manipulated with the standard tools.
Wizards had been in development at Microsoft for several years before Publisher, notably for Microsoft Access, which wouldn't ship until November 1992.
Wizards were intended to learn from how someone used a program and anticipate what they may want to do next, guiding them through more complex sets of tasks by structuring and sequencing them.
They also served to teach the product by example.
As early as 1989, Microsoft discussed using voice and talking heads as guides, but multimedia-capable hardware was not yet widespread.
The feature spread quickly to other applications.
In 1992, Excel 4.0 for Mac introduced wizards for tasks like building crosstab tables, and Windows later used wizards for tasks like printer or Internet configuration.
By 2001, wizards had become commonplace in most consumer-oriented operating systems, although not always under the name "wizard.
On the Mac OS, starting with tools like the Setup Assistant introduced in Mac OS 8.0, similar tools were (and still are) referred to as "assistants" (this is not to be confused with the 'Assist' feature which was on the Apple Newton).
The "Setup Assistant" is run when the Macintosh starts up out of the box or after a fresh installation, and a similar process also takes place on Apple iOS.
Aside from first time setup, other assistants like the "Network Setup Assistant" are similar to the Windows "New Connection Wizard.
GNOME also refers to its wizards as "assistants".
Today, a wizard-like experience is often used to "onboard" users the first time they open an app.
Many web applications, for instance online booking sites, make use of the wizard paradigm to complete lengthy interactive processes.
Oracle Designer also uses wizards extensively.
The Microsoft Manual of Style for Technical Publications (Version 3.0) urges technical writers to refer to these assistants as "wizards" and to use lowercase letters.
The following screenshots show the installation wizard for Kubuntu 12.04, a free and open-source operating system.
The wizard consists of seven steps.
By the end of the step seven, the operation will be completed.
A version of the simulation hypothesis was first theorised as a part of a philosophical argument on the part of René Descartes, and later by Hans Moravec.
The philosopher Nick Bostrom developed an expanded argument examining the probability of our reality being a simulation.
His argument states that at least one of the following statements is very likely to be true:  1.
Human civilization or a comparable civilization is unlikely to reach a level of technological maturity capable of producing simulated realities or such simulations are physically impossible to construct.
2.
A comparable civilization reaching aforementioned technological status will likely not produce a significant number of simulated realities (one that might push the probable existence of digital entities beyond the probable number of "real" entities in a Universe) for any of a number of reasons, such as diversion of computational processing power for other tasks, ethical considerations of holding entities captive in simulated realities, etc.
3.
Any entities with our general set of experiences are almost certainly living in a simulation.
4.
We are living in a reality in which posthumans have not developed yet
and we are actually living in reality.
Bostrom's argument rests on the premise that given sufficiently advanced technology, it is possible to represent the populated surface of the Earth without recourse to digital physics; that the qualia experienced by a simulated consciousness are comparable or equivalent to those of a naturally occurring human consciousness, and that one or more levels of simulation within simulations would be feasible given only a modest expenditure of computational resources in the real world.
If one assumes first that humans will not be destroyed nor destroy themselves before developing such a technology, and that human descendants will have no overriding legal restrictions or moral compunctions against simulating biospheres or their own historical biosphere, then, Bostrom argues, it would be unreasonable to count ourselves among the small minority of genuine organisms who, sooner or later, will be vastly outnumbered by artificial simulations.
Epistemologically, it is not impossible to tell whether we are living in a simulation.
For example, Bostrom suggests that a window could pop up saying: "You are living in a simulation.
Click here for more information.
However, imperfections in a simulated environment might be difficult for the native inhabitants to identify and for purposes of authenticity, even the simulated memory of a blatant revelation might be purged programmatically.
Nonetheless, should any evidence come to light, either for or against the skeptical hypothesis, it would radically alter the aforementioned probability.
Computationalism is a philosophy of mind theory stating that cognition is a form of computation.
It is relevant to the Simulation hypothesis in that it illustrates how a simulation could contain conscious subjects, as required by a "virtual people" simulation.
For example, it is well known that physical systems can be simulated to some degree of accuracy.
If computationalism is correct and if there is no problem in generating artificial consciousness or cognition, it would establish the theoretical possibility of a simulated reality.
Nevertheless, the relationship between cognition and phenomenal qualia of consciousness is disputed.
It is possible that consciousness requires a vital substrate that a computer cannot provide and that simulated people, while behaving appropriately, would be philosophical zombies.
This would undermine Nick Bostrom's simulation argument; we cannot be a simulated consciousness, if consciousness, as we know it, cannot be simulated.
The skeptical hypothesis remains intact, however, and we could still be envatted brains, existing as conscious beings within a simulated environment, even if consciousness cannot be simulated.
It has been suggested that whereas virtual reality would enable a participant to experience only three senses (sight, sound and optionally smell), simulated reality would enable all five (including taste and touch).Some theorists have argued that if the "consciousness-is-computation" version of computationalism and mathematical realism (or radical mathematical Platonism) are true then consciousnesses is computation, which in principle is platform independent and thus admits of simulation.
This argument states that a "Platonic realm" or ultimate ensemble would contain every algorithm, including those which implement consciousness.
Hans Moravec has explored the simulation hypothesis and has argued for a kind of mathematical Platonism according to which every object (including, for example, a stone) can be regarded as implementing every possible computation.
A dream could be considered a type of simulation capable of fooling someone who is asleep.
As a result, the "dream hypothesis" cannot be ruled out, although it has been argued that common sense and considerations of simplicity rule against it.
One of the first philosophers to question the distinction between reality and dreams was Zhuangzi, a Chinese philosopher from the 4th century BC.
He phrased the problem as the well-known "Butterfly Dream," which went as follows:  Once Zhuangzi dreamt he was a butterfly, a butterfly flitting and fluttering around, happy with himself and doing as he pleased.
He didn't know he was Zhuangzi.
Suddenly he woke up and there he was, solid and unmistakable Zhuangzi.
But he didn't know if he was Zhuangzi who had dreamt he was a butterfly or a butterfly
dreaming he was Zhuangzi.
Between Zhuangzi and a butterfly there must be some distinction!
This is called the Transformation of Things.
(2, tr.
Burton Watson 1968:49)
The philosophical underpinnings of this argument are also brought up by Descartes, who was one of the first Western philosophers to do so.
In Meditations on First Philosophy, he states "... there are no certain indications by which we may clearly distinguish wakefulness from sleep", and goes on to conclude that "It is possible that I am dreaming right now and that all of my perceptions are false".Chalmers (2003) discusses the dream hypothesis and notes that this comes in two distinct forms:  that he is currently dreaming, in which case many of his beliefs about the world are incorrect; that he has always been dreaming, in which case the objects he perceives actually exist, albeit in his imagination.
Both the dream argument and the simulation hypothesis can be regarded as skeptical hypotheses; however in raising these doubts, just as Descartes noted that his own thinking led him to be convinced of his own existence, the existence of the argument itself is testament to the possibility of its own truth.
Another state of mind in which some argue an individual's perceptions have no physical basis in the real world is called psychosis though psychosis may have a physical basis in the real world and explanations vary.
The dream hypothesis is also used to develop other philosophical concepts, such as Valberg's personal horizon: what this world would be internal to if this were all a dream.
Known as the idea of Nested Simulations: the existence of simulated reality is seen to be unprovable in any concrete sense as  there is an infinite regress problem with the argument: any evidence that is directly observed could be another simulation itself.
Even if we are a simulated reality, there is no way to be sure the beings running the simulation are not themselves a simulation and the operators of that simulation are not
a simulation."Recursive simulation involves a simulation or an entity in the simulation, creating another instance of the same simulation, running it and using its results" (Pooch and Sullivan 2000).In August 2019, philosopher Preston Greene suggested that it may be best not to find out if we're living in a computer simulation since, if it were found to be true, such knowing may end the simulation.
Some philosophers and authors (Nick Bostrom's “
Are You Living In a Computer Simulation?
”, Jean Baudrillard's “Simulacra and Simulation”, Iurii Vovchenko's “Answers in Simulation”) tried to address the implications of the simulated reality on mankind's way of life and future.
Simulated reality has significant implications to the philosophical questions such as the questions of existence of gods, meaning of life etc.
There are attempts to link religion to the simulated reality.
Simulated reality in fiction has been explored by many authors, game designers and film directors.
Nick Bostrom and his simulation argument René Descartes (1596–1650) and his Evil Demon, sometimes also called his 'Evil Genius' George Berkeley (1685–1753) and his "immaterialism" (later referred to as subjective idealism by others) Plato (424/423 BC – 348/347 BC) and his Allegory of the Cave Zhuangzi (around the 4th century BCE) and his Chinese Butterfly Dream
The history of natural language processing (NLP) generally started in the 1950s, although work can be found from earlier periods.
In 1950, Alan Turing published an article titled "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence.
The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English.
The authors claimed that within three or five years, machine translation would be a solved problem.
However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.
Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.
Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted "blocks worlds" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.
Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction.
When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?".
During the 1970s, many programmers began to write "conceptual ontologies", which structured real-world information into computer-understandable data.
Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).
During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.
Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules.
Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.
This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.
Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard
if-then rules similar to existing hand-written rules.
However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.
The cache language models upon which many speech recognition systems now rely are examples of such statistical models.
Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.
Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed.
These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.
However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems.
As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.
Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms.
Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data.
Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.
However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.
In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques can achieve state-of-the-art results in many natural language tasks, for example in language modeling, parsing, and many others.
Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing).
In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing.
For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that were used in statistical machine translation (SMT).
In the early days, many language-processing systems were designed by hand-coding a set of rules: such as by writing grammars or devising heuristic rules for stemming.
Since the so-called "statistical revolution" in the late 1980s and mid 1990s, much natural language processing research has relied heavily on machine learning.
The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.
Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks.
These algorithms take as input a large set of "features" that are generated from the input data.
Some of the earliest-used algorithms, such as decision trees, produced systems of hard
if-then rules similar to the systems of handwritten rules that were then common.
Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature.
Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.
Systems based on machine-learning algorithms have many advantages over handproduced rules:
The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed.
Automatic learning procedures can make use of statistical-inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted).
Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming.
Systems based on automatically learning the rules can be made more accurate simply by supplying more input data.
However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task.
In particular, there is a limit to the complexity of systems based on handcrafted rules, beyond which the systems become more and more unmanageable.
However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process.
The following is a list of some of the most commonly researched tasks in natural language processing.
Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
Though natural language processing tasks are closely intertwined, they are frequently subdivided into categories for convenience.
A coarse division is given below.
Grammar induction Generate a formal grammar that describes a language's syntax.
Lemmatization
The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma.
Morphological segmentation Separate words into individual morphemes and identify the class of the morphemes.
The difficulty of this task depends greatly on the complexity of the morphology (i.e. the structure of words) of the language being considered.
English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g. "open, opens, opened, opening") as separate words.
In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.
Part-of-speech tagging Given a sentence, determine the part of speech (POS) for each word.
Many words, especially common ones, can serve as multiple parts of speech.
For example, "book" can be a noun ("the book on the table") or verb ("to book a flight"); "set" can be a noun, verb or adjective; and "out" can be any of at least five different parts of speech.
Some languages have more such ambiguity than others.
Languages with little inflectional morphology, such as English, are particularly prone to such ambiguity.
Chinese is prone to such ambiguity because it is a tonal language during verbalization.
Such inflection is not readily conveyed via the entities employed within the orthography to convey intended meaning.
Parsing Determine the parse tree (grammatical analysis) of a given sentence.
The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses.
In fact, perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human).
There are two primary types of parsing, Dependency Parsing and Constituency Parsing.
Dependency Parsing focuses on the relationships between words in a sentence (marking things like Primary Objects and predicates), whereas Constituency Parsing focuses on building out the Parse Tree using a Probabilistic Context-Free Grammar (PCFG).
See also: Stochastic grammar.
Sentence breaking (also known as sentence boundary disambiguation)
Given a chunk of text, find the sentence boundaries.
Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g. marking abbreviations).
Stemming
The process of reducing inflected (or sometimes derived) words to their root form.
(e.g. "close" will be the root for "closed", "closing", "close", "closer" etc.).
Word segmentation Separate a chunk of continuous text into separate words.
For a language like English, this is fairly trivial, since words are usually separated by spaces.
However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language.
Sometimes this process is also used in cases like Bag of Words (BOW) creation in data mining.
Terminology extraction
The goal of terminology extraction is to automatically extract relevant terms from a given corpus.
Lexical semantics
What is the computational meaning of individual words in context?
Distributional semantics
How can we learn semantic representations from data?
Machine translation Automatically translate text from one human language to another.
This is one of the most difficult problems, and is a member of a class of problems colloquially termed "AI-complete", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.)
in order to solve properly.
Named entity recognition (NER)
Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization).
Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case is often inaccurate or insufficient.
For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized.
Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names.
For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives.
Natural language generation Convert information from computer databases or semantic intents into readable human language.
Natural language understanding Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate.
Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts.
Introduction and creation of language metamodel and ontology are efficient however empirical solutions.
An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective
Yes/
No vs. objective
True/False is expected for the construction of a basis of semantics formalization.
Optical character recognition (OCR)
Given an image representing printed text, determine the corresponding text.
Question answering Given a human-language question, determine its answer.
Typical questions have a specific right answer (such as "What is the capital of Canada?"), but sometimes open-ended questions are also considered (such as "What is the meaning of life?").
Recent works have looked at even more complex questions.
Recognizing Textual entailment
Given two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false.
Relationship extraction Given a chunk of text, identify the relationships among named entities (e.g. who is married to whom).
Sentiment analysis (see also multimodal sentiment analysis)
Extract subjective information usually from a set of documents, often using online reviews to determine "polarity" about specific objects.
It is especially useful for identifying trends of public opinion in the social media, for the purpose of marketing.
Topic segmentation and recognition
Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment.
Word sense disambiguation Many words have more than one meaning; we have to select the meaning which makes the most sense in context.
For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or from an online resource such as WordNet.
Automatic summarization Produce a readable summary of a chunk of text.
Often used to provide summaries of text of a known type, such as research papers, articles in the financial section of a newspaper.
Coreference resolution
Given a sentence or larger chunk of text, determine which words ("mentions") refer to the same objects ("entities").
Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer.
The more general task of coreference resolution also includes identifying so-called "bridging relationships" involving referring expressions.
For example, in a sentence such as "He entered John's house through the front door", "the front door" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).
Discourse analysis
This rubric includes a number of related tasks.
One task is identifying the discourse structure of connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast).
Another possible task is recognizing and classifying the speech acts in a chunk of text
(e.g. yes-no question, content question, statement, assertion, etc.).
Speech recognition
Given a sound clip of a person or people speaking, determine the textual representation of the speech.
This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed "AI-complete" (see above).
In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below).
In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process.
Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent.
Speech segmentation Given a sound clip of a person or people speaking, separate it into words.
A subtask of speech recognition and typically grouped with it.
Text-to-speech Given a text, transform those units and produce a spoken representation.
Text-to-speech can be used to aid the visually impaired.
The first published work by an artificial intelligence was published in 2018, 1 the Road, marketed as a novel, contains sixty million words.
According to Alan Cooper, the "Father of Visual Basic," the concept of Clippit was based on a "tragic misunderstanding" of research conducted at Stanford University, showing that the same part of the brain in use while using a mouse or keyboard was also responsible for emotional reactions while interacting with other human beings and thus is the reason people yell at their computer monitors.
Microsoft concluded that if humans reacted to computers the same way they react to other humans, it would be beneficial to include a human-like face in their software.
As people already related to computers directly as they do with humans, the added human-like face emerged as an annoying interloper distracting the user from the primary conversation.
First introduced in Microsoft Office 97, the Office Assistant was codenamed TFC during development.
It appeared when the program determined the user could be assisted with using Office wizards, searching help, or advising users on using Office features more effectively.
It also presented tips and keyboard shortcuts.
For example, typing an address followed by "Dear" would cause the Assistant to appear with the message, "It looks like you're writing a letter.
Would you like help?".
Apart from Clippit, other Office Assistants were also available:
The Dot (a shape-shifting smiley-faced red ball) Hoverbot (a robot)
The Genius (a caricature of Albert Einstein, removed in Office XP but available as a downloadable add-on) Office Logo (a jigsaw puzzle)
Mother Nature (a globe)
Scribble (an origami-esque cat)
Power Pup (a superhero dog)
Will (a caricature of William Shakespeare).In
many cases
the Office installation CD was necessary to activate a different Office assistant character, so the default character, Clippit, remains widely known compared to other Office Assistants.
In Office 2000, the Hoverbot, Scribble and Power Pup assistants were replaced by:  F1 (a robot)
Links (a cat)
Rocky (a dog)The Clippit and Office Logo assistants were also redesigned.
The removed assistants later resurfaced as downloadable add-ons.
The Microsoft Office XP Multilingual Pack had two more assistants, Saeko Sensei (冴子先生), an animated secretary, and a version of the Monkey King (Chinese: 孫悟空) for Asian language users in non-Asian Office versions.
Native language versions provided additional representations, such as Kairu the dolphin in Japanese.
A small image of Clippit can be found in Office 2013 or newer, which could be enabled by going to Options and changing the theme to "School Supplies".
Clippit would then appear on the ribbon.
The Office Assistant used technology initially from Microsoft Bob and later Microsoft Agent, offering advice based on Bayesian algorithms.
From Office 2000 onwards, Microsoft Agent (.acs) replaced the Microsoft Bob-descended Actor (.act) format as the technology supporting the feature.
Users can add other assistants to the folder where Office is installed for them to show up in the Office application, or install in the Microsoft Agent folder in System32 folder.
Microsoft Agent-based characters have richer forms and colors, and are not enclosed within a boxed window.
Furthermore, the Office Assistant could use the Lernout & Hauspie TruVoice
Text-to-Speech Engine to provide output speech capabilities to Microsoft Agent, but it required SAPI 4.0.
The Microsoft Speech Recognition Engine allowed the Office Assistant to accept speech input.
The Microsoft Agent components it required were not included in Windows 7 or later; however, they can be downloaded from the Microsoft website.
Installation of Microsoft Agent on Windows 8 and Windows 10 is also possible.
When desktop compositing with Aero glass is enabled on Windows Vista or 7, or when running on Windows 8 or newer, the normally transparent space around the Office Assistant becomes solid-colored pink, blue, or green.
In 2019, Clippy has been ported to macOS using the SpriteKit-Framework and written in Swift.
Since their introduction, more assistants have been released and have been exclusively available via download.
Bosgrove (a butler) Courtney (a flying car driver)
Earl (a surfboarding alien)
Genie (a genie) Kairu the Dolphin, otherwise known as Chacha (available for East Asian editions, downloadable for Office 97)
Max (a Macintosh Plus computer) (
Macintosh) Merlin (a wizard) Peedy (a green parrot, which was ultimately reused in the first iteration of the notorious BonziBuddy software)
Robby (a robot)
Rover (a dog, also featured as Windows XP Search companion.)
The Monkey King (available for East Asian editions, downloadable for Office
97)The
1997 assistants can be downloaded from the Microsoft website.
The program was widely reviled among users as intrusive and annoying, and was criticized even within Microsoft.
Microsoft's internal codename TFC had a derogatory origin: Steven Sinofsky states that "C" stood for "clown", while allowing his readers to guess what "TF" might stand for.
Smithsonian Magazine called Clippit "one of the worst software design blunders in the annals of computing".
Time magazine included Clippit in a 2010 article listing the fifty worst inventions.
In July 2000, the online comic strip User Friendly ran a series of panels featuring Clippit.
In 2001, a Microsoft advertising campaign for Office XP included the (now defunct) website officeclippy.com, which highlighted the disabling of Clippit in the software.
It featured the animated adventures of Clippit (voiced by comedian Gilbert Gottfried) as he learned to cope with unemployment (
"X… XP…
As in, ex-paperclip?!") and parodied behaviors of the Office assistant.
Curiously, one of these ("Clippy Faces Facts") uses the same punchline as one of the User Friendly comic strips.
These videos can be downloaded from Microsoft's website as self-contained Flash Player executables.
Clippit ends up in an office as a floppy disk ejecting pin.
There is a Clippit parody in the Plus!
Dancer application included in Microsoft Plus!
Digital Media Edition which is later included as Windows Dancer in Windows XP Media Center Edition 2005.
The dancing character
Boo Who? is wearing a ghost outfit, roughly having the shape of Clippit's body, with a piece of wire visible underneath.
Occasionally, the white sheet slips, and reveals the thin curve of steel.
The description mentions "working for a short while for a Redmond, WA based software company, where he continued to work until being retired in 2001".
Clippit is also included as a player character in Microsoft Bicycle Card Games and Microsoft Bicycle Board Games.
It was also used in the "Word Crimes" music video by "Weird Al" Yankovic.
Vigor is a Clippit-inspired parody software - a version of the vi text editor featuring a rough-sketched Clippit.
On April 1, 2014, Clippit appeared as an Office Assistant in Office Online as part of an April Fools' Day joke.
Several days later, an easter egg was found in the then-preview version of Windows Phone 8.1.
When asked if she likes Clippit, the personal assistant Cortana would answer "Definitely.
He taught me how important it is to listen.
or "What's not to like?
That guy took a heck of a beating
and he's still smiling.
Her avatar occasionally turned into a two-dimensional Metro-style Clippit for several seconds.
This easter egg is still available in the full release version of the Windows Phone operating system and Windows 10.On April 1, 2015, Tumblr created a parody of Clippit, Coppy, as an April Fools joke.
Coppy is an anthropomorphized photocopier that behaved in similar ways to Clippit, asking the user if they want help.
Coppy would engage the reader in a series of pointless questions, with a dialogue box written in Comic Sans MS, deliberately designed to be extremely annoying.
In the ninth episode of Season 3 of HBO's Silicon Valley, originally aired in June, 2016, a new animated character called "Pipey", clearly based on Microsoft's Clippit, provides help to users of the Pied Piper platform.
After featuring Clippit's tomb in a movie to promote Office 2010, the character was relaunched as the main character of the game Ribbon Hero 2, which is an interactive tutorial released by Microsoft in 2011.
In the game, Clippy needs a new job and accidentally goes inside a time machine, travelling to different ages solving problems with Word, Excel, PowerPoint and OneNote.
Other Office Assistant names are also featured during the "Future Age" as planets of the future solar system.
In "Search Committee," the seventh season finale of The Office aired in May 2011, Darryl calls Microsoft and asks whether they still have Clippy while trying to build a resume.
In 2015 a music video was released for the song "Ghost" (by Delta Heavy) in which the abandoned Clippit is stuck between the software of the mid-nineties but then travels to the contemporary web and regains his place by hacking himself into any digital system.
Clippit made a cameo appearance in the Drawn Together episode
"The One Wherein
There Is a Big Twist, Part II", where he offered to help Wooldoor Sockbat with his suicide note.
Clippit is portrayed as a romantic interest in "Conquered by Clippy", a comedic/erotic story by Leonard Delaney.
Apple produced several concept videos showcasing the idea.
All of them featured a tablet style computer with numerous advanced capabilities, including an excellent text-to-speech system with no hint of "computerese", a gesture based interface resembling the multi-touch interface later used on the iPhone and an equally powerful speech understanding system, allowing the user to converse with the system via an animated "butler" as the software agent.
In one vignette a university professor returns home and turns on his computer, in the form of a tablet the size of a large-format book.
The agent is a bow-tie wearing butler who appears on the screen and informs him that he has several calls waiting.
He ignores most of these, from his mother, and instead uses the system to compile data for a talk on deforestation in the Amazon Rainforest.
While he is doing this, the computer informs him that a colleague is calling, and they then exchange data through their machines while holding a video based conversation.
In another such video, a young student uses a smaller handheld version of the system to prompt him while he gives a class presentation on volcanoes, eventually sending a movie of an exploding volcano to the video "blackboard".
In a final installment a user scans in a newspaper by placing it on the screen of the full-sized version, and then has it help him learn to read by listening to him read the scanned results, and prompting when he pauses.
The videos were funded and sponsored by Bud Colligan, Director of Apple's higher education marketing group, written and creatively developed by Hugh Dubberly and Doris Mitsch of Apple Creative Services, with technical and conceptual input from Mike Liebhold of Apple's Advanced Technologies Group and advice from Alan Kay, then an Apple Fellow.
The videos were produced by The Kenwood Group in San Francisco and directed by Randy Field.
The director of photography was Bill Zarchy.
The post-production mix was done by Gary Clayton at Russian Hill Recording for The Kenwood Group.
The product industrial design was created by Gavin Ivester and Adam Grosser of Apple design.
Samir Arora, a software engineer at Apple was involved in R&D on application navigation and what was then called hypermedia.
He wrote an important white paper entitled “Information Navigation: The Future of Computing".
While working for Apple CEO John Sculley at the time, Arora built the technology to show fluid access to linked data displayed in a friendly manner an emerging area of research at Apple.
The Knowledge Navigator video premiered in 1987 at Educom, the leading higher education conference, in a keynote by John Sculley, with demos of multimedia, hypertext and interactive learning directed by Bud Colligan.
The music featured in this video is Georg Anton Benda 's Harpsichord Concerto in C.
The astute bow tie wearing software agent in the video has been the center of quite a few heated discussions in the domain of human–computer interaction.
It was criticized as being an unrealistic portrayal of the capacities of any software agent in the foreseeable future, or even in a distant future.
Some user interface professionals like Ben Shneiderman of the University of Maryland, College Park have also criticized its use of a human likeness for giving a misleading idea of the nature of any interaction with a computer, present or future.
Some visions put forth by proponents of the Semantic Web have been likened to that of the Knowledge Navigator by Marshall and Shipman, who argue that some of these visions "ignore the difficulty of scaling knowledge-based systems to reason across domains, like Apple's Knowledge Navigator," and conclude that, as of 2003, "scenarios of the complexity of [a previously quoted] Knowledge Navigator-like approach to interacting with people and things in the world seem unlikely.
The notion of Siri was firmly planted at Apple 25 years ago though “Knowledge Navigator” with the voice of the assistant was only a concept prototype.
In October 2011, Apple relaunched Siri, a voice activated personal assistant software vaguely similar to that aspect of the Knowledge Navigator.
Intelligent agents have been defined in many different ways.
According to Nikola Kasabov IA systems should exhibit the following characteristics:  Accommodate new problem solving rules incrementally Adapt online and in real time Are able to analyze themselves in terms of behavior, error and success.
Learn and improve through interaction with the environment (embodiment) Learn quickly from large amounts of data Have
memory-based exemplar storage and retrieval capacities Have parameters to represent short and long term memory, age, forgetting, etc.
A simple agent program can be defined mathematically as a function f (called the "agent function") which maps every possible percepts sequence to a possible action the agent can perform or to a coefficient, feedback element, function or constant that affects eventual actions:                         f         :
→         A                 {\displaystyle
f:P^{\ast
}\rightarrow
A}   Agent function is an abstract concept as it could incorporate various principles of decision making like calculation of utility of individual options, deduction over logic rules, fuzzy logic, etc.
The program agent, instead, maps every possible percept to an action.
We use the term percept to refer to the agent's perceptional inputs at any given instant.
In the following figures an agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.
Weiss (2013) defines four classes of agents:  Logic-based agents – in which the decision about what action to perform is made via logical deduction; Reactive agents – in which decision making is implemented in some form of direct mapping from situation to action; Belief-desire-intention agents – in which decision making depends upon the manipulation of data structures representing the beliefs, desires, and intentions of the agent; and finally, Layered architectures
– in which decision making is realized via various software layers, each of which is more or less explicitly reasoning about the environment at different levels of abstraction.
Generally, an agent can be constructed by separating the body into the sensors and actuators, and so that it operates with a complex perception system that takes the description of the world as input for a controller and outputs commands to the actuator.
However, a hierarchy of controller layers is often necessary to balance the immediate reaction desired for low-level tasks and the slow reasoning about complex, high-level goals.
Russell & Norvig (2003) group agents into five classes based on their degree of perceived intelligence and capability: simple reflex agents model-based reflex agents goal-based agents utility-based agents learning agents
Simple reflex agents act only on the basis of the current percept, ignoring the rest of the percept history.
The agent function is based on the condition-action rule: "if condition, then action".
This agent function only succeeds when the environment is fully observable.
Some reflex agents can also contain information on their current state which allows them to disregard conditions whose actuators are already triggered.
Infinite loops are often unavoidable for simple reflex agents operating in partially observable environments.
Note: If the agent can randomize its actions, it may be possible to escape from infinite loops.
A model-based agent can handle partially observable environments.
Its current state is stored inside the agent maintaining some kind of structure which describes the part of the world which cannot be seen.
This knowledge about "how the world works" is called a model of the world, hence the name "model-based agent".
A model-based reflex agent should maintain some sort of internal model that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state.
Percept history and impact of action on the environment can be determined by using internal model.
It then chooses an action in the same way as reflex agent.
An agent may also use models to describe and predict the behaviors of other agents in the environment.
Goal-based agents further expand on the capabilities of the model-based agents, by using "goal" information.
Goal information describes situations that are desirable.
This allows the agent a way to choose among multiple possibilities, selecting the one which reaches a goal state.
Search and planning are the subfields of artificial intelligence devoted to finding action sequences that achieve the agent's goals.
Goal-based agents only distinguish between goal states and non-goal states.
It is possible to define a measure of how desirable a particular state is.
This measure can be obtained through the use of a utility function which maps a state to a measure of the utility of the state.
A more general performance measure should allow a comparison of different world states according to exactly how happy they would make the agent.
The term utility can be used to describe how "happy" the agent is.
A rational utility-based agent chooses the action that maximizes the expected utility of the action outcomes - that is, what the agent expects to derive, on average, given the probabilities and utilities of each outcome.
A utility-based agent has to model and keep track of its environment, tasks that have involved a great deal of research on perception, representation, reasoning, and learning.
Learning has the advantage that it allows the agents to initially operate in unknown environments and to become more competent than its initial knowledge alone might allow.
The most important distinction is between the "learning element", which is responsible for making improvements, and the "performance element", which is responsible for selecting external actions.
The learning element uses feedback from the "critic" on how the agent is doing and determines how the performance element should be modified to do better in the future.
The performance element is what we have previously considered to be the entire agent: it takes in percepts and decides on actions.
The last component of the learning agent is the "problem generator".
It is responsible for suggesting actions that will lead to new and informative experiences.
To actively perform their functions, Intelligent Agents today are normally gathered in a hierarchical structure containing many “sub-agents”.
Intelligent sub-agents process and perform lower level functions.
Taken together, the intelligent agent and sub-agents create a complete system that can accomplish difficult tasks or goals with behaviors and responses that display a form of intelligence.
Intelligent agents are applied as automated online assistants, where they function to perceive the needs of customers in order to perform individualized customer service.
Such an agent may basically consist of a dialog system, an avatar, as well an expert system to provide specific expertise to the user.
They can also be used to optimize coordination of human groups online.
Establishing this kind of network is often necessary for sharing residential Internet access to all networked devices.
Based on techniques to mitigate IPv4 address exhaustion, most Internet service providers provide only a single wide area network-facing IP address for each residential customer.
Therefore, such networks require network address translation in the network router.
DHCP is used in a typical personal home local area network (LAN) to assign IP addresses within the home subnet.
The DHCP server is a router while the clients are hosts (eg. personal computers, smart phones, printers, etc.).
The router receives the configuration information through a modem from an internet service provider, which also operates DHCP servers with this router as one of the clients.
The clients request configuration settings using the DHCP protocol such as an IP address, a default route and one or more DNS server addresses.
Once the client implements these settings, the host is able to communicate on that internet.
A home network usually relies on one or more of the following equipment to establish physical layer, data link layer, and network layer connectivity among internal devices, also known as the LAN, and external devices outside the LAN networks or the WAN.
The following are examples of typical LAN devices:
A modem exposes an Ethernet interface to a service provider's native telecommunications infrastructure.
In homes these usually come in the form of a DSL modem or cable modem.
A router manages network layer connectivity between a WAN and the HAN.
It performs the key function of network address translation enabling multiple devices to share the home's single WAN address.
Most home networks feature a particular class of small, passively cooled, table-top device with an integrated wireless access point and 4 port Ethernet switch.
These devices aim to make the installation, configuration, and management of a home network as automated, user friendly, and "plug-and-play" as possible.
A network switch is used to allow devices on the home network to talk to one another via Ethernet.
While the needs of most home networks are satisfied with the built-in wireless and/or switching capabilities of their router, some situations require the addition of a separate switch with advanced capabilities.
For example: A typical home router has 4 to 6 Ethernet LAN ports, so a router's switching capacity could be exceeded.
A network device might require a non-standard port feature such as power over Ethernet (PoE).
(IP cameras and IP phones)
A wireless access point is required for connecting wireless devices to a network.
Most home networks rely on a wireless router, which has a built in wireless access point, to fill this role.
A home automation controller enables low-power wireless communications with simple, non-data-intensive devices such as smart light bulbs (Philips Hue) and smart locks (August Home).
A network bridge connects two networks, often in order to grant a wired-only device, e.g. Xbox, access to a wireless network medium.
A service provider's triple play solution features a rented modem/wireless router combination device, such as an Arris SURFboard SBG6580, that only requires the setting of a password to complete the installation and configuration.
In most situations, there is no longer a need to acquire additional infrastructure devices or even for the user to possess advanced technical knowledge to successfully distribute internet access throughout the home.
Home networks can use either wired or wireless technologies to connect endpoints.
Wireless is the predominant option in homes due to the ease of installation, lack of unsightly cables, and network performance characteristics sufficient for residential activities.
Most wired network infrastructures found in homes utilize Category 5 or Category 6 twisted pair cabling with RJ45 compatible terminations.
This medium provides physical connectivity between the Ethernet interfaces present on a large number of residential IP-aware devices.
Depending on the grade of cable and quality of installation, speeds of up to 10 Mbit/s, 100 Mbit/s, 1 Gbit/s, or 10Gbit/s are supported.
Newer upscale neighborhoods can feature fiber optic cables running directly into the homes.
This enables service providers to offer internet services with much higher bandwidth and/or lower latency characteristics associated with end-to-end optical signaling.
VDSL and VDSL2
HomePNA support up to 160 Mbit/s
The following standards allow devices to communicate over coaxial cables, which are frequently installed to support multiple television sets throughout homes.
DOCSIS
The Multimedia over Coax Alliance (MoCA) standard can achieve up to 270 Mbit/s CWave
HomePNA support up to 320 Mbit/s
The ITU-T G.hn and IEEE Powerline standard, which provide high-speed (up to 1 Gbit/s) local area networking over existing home wiring, are examples of home networking technology designed specifically for IPTV delivery.
Recently, the IEEE passed proposal P1901 which grounded a standard within the Market for wireline products produced and sold by companies that are part of the HomePlug Alliance.
The IEEE is continuously working to push for P1901 to be completely recognized worldwide as the sole standard for all future products that are produced for Home Networking.
HomePlug and HomePNA are associated standards Universal Powerline Association
Traditionally, data-centric equipment such as computers and media players have been the primary tenants of a home network.
However, due to the lowering cost of computing and the ubiquity of smartphone usage, many traditionally non-networked home equipment categories now include new variants capable of control or remote monitoring through an app on a smartphone.
Newer startups and established home equipment manufacturers alike have begun to offer these products as part of a "Smart" or "Intelligent" or "Connected Home" portfolio.
The control and/or monitoring interfaces for these products can be accessed through proprietary smartphone applications specific to that product line.
Personal computers such as desktops, laptops, netbooks, and tablets
A network attached storage
(NAS) device can be easily accessed via the CIFS or NFS protocols for general storage or for backup purposes.
A print server can be used to share any directly connected printers with other computers on the network.
IP phones or smartphones (when connected via Wi-Fi) utilizing VoIP technologies
Smart speakers Television:
Some new TVs and DVRs include integrated WiFi connectivity which allows the user to access services such as Netflix and YouTube Home audio:
Digital audio players, and stereo systems with network connectivity can allow a user to easily access their music library, often using Bonjour to discover and interface with an instance of iTunes running on a remote PC.
Gaming: video game consoles rely on connectivity to the home network to enable a significant portion of their overall features, such as the multiplayer in games, social network integration, ability to purchase or demo new games, and receive software updates.
Recent consoles have begun more aggressively pursuing the role of the sole entertainment and media hub of the home.
DLNA is a common protocol used for interoperability between networked media-centric devices in the homeSome older entertainment devices may not feature the appropriate network interfaces required for home network connectivity.
In some situations, USB dongles and PCI Network Interface Cards are available as accessories that enable this functionality.
Connected" light bulbs such as Lifx, Philips Hue, Samsung Smart Bulb, GE Link
ZigBee Light Link is the open standards protocol used by current major "Connected" light bulb vendors
Security alarms: iSmartAlarm Garage door and gate openers: Liftmaster MyQ, GoGogate
HVAC:
Nest Learning
Thermostat
Smoke/CO detectors: Nest Protect
The convenience, availability, and reliability of externally managed cloud computing resources continues to become an appealing choice for many home-dwellers without interest or experience in IT.
For these individuals, the subscription fees and/or privacy risks associated with such services are often perceived as lower cost than having to configure and maintain similar facilities within a home network.
In such situations, local services along with the devices maintaining them are replaced by those in an external data center and made accessible to the home-dweller's computing devices via a WAN connection.
Small standalone embedded home network devices typically require remote configuration from a PC on the same network.
For example, broadband modems are often configured through a web browser running on a PC in the same network.
These devices usually use a minimal Linux distribution with a lightweight HTTP server running in the background to allow the user to conveniently modify system variables from a GUI rendered in their browser.
These pages use HTML forms extensively and make attempts to offer styled, visually appealing views that are also descriptive and easy to use.
Apple devices aim to make networking as hidden and automatic as possible, utilizing a zero-configuration networking protocol called Bonjour embedded within their otherwise proprietary line of software and hardware products.
Microsoft offers simple access control features built into their Windows operating system.
Homegroup is a feature that allows shared disk access, shared printer access and shared scanner access among all computers and users (typically family members) in a home, in a similar fashion as in a small office workgroup, e.g., by means of distributed peer-to-peer networking (without a central server).
Additionally, a home server may be added for increased functionality.
The Windows HomeGroup feature was introduced with Microsoft Windows 7 in order to simplify file sharing in residences.
All users (typically all family members), except guest accounts, may access any shared library on any computer that is connected to the home group.
Passwords are not required from the family members during logon.
Instead, secure file sharing is possible by means of a temporary password that is used when adding a computer to the HomeGroup.
The wireless signal strength of the standard residential wireless router may not be powerful enough to cover the entire house or may not be able to get through to all floors of multiple floor residences.
In such situations, the installation of one or more wireless repeaters may be necessary.
WiFi often extends beyond the boundaries of a home and can create coverage where it is least wanted, offering a channel through which non-residents could compromise a system and retrieve personal data.
To prevent this it is usually sufficient to enforce the use of authentication, encryption, or VPN that requires a password for network connectivity.
However new Wi-Fi standards working at 60 GHz, such as 802.11ad, enable confidence that the LAN will not trespass physical barriers, as at such frequencies a simple wall would attenuate the signal considerably.
For home networks relying on powerline communication technology, how to deal with electrical noise injected into the system from standard household appliances remains the largest challenge.
Whenever any appliance is turned on or turned off
it creates noise that could possibly disrupt data transfer through the wiring.
IEEE products that are certified to be HomePlug 1.0 compliant have been engineered to no longer interfere with, or receive interference from other devices plugged into the same home's electrical grid.
The administration of proliferating devices and software in home networks, and the growing amount of private data, is fast becoming an issue by itself.
Keeping overview, applying without delay SW updates and security patches, keeping juniors internet use within safe boundaries, structuring of storage and access levels for private files and other data, Data backups, detection and cleaning of any infections, operating virtual private networks for easy access to resources in the home network when away, etc..
Such things are all issues that require attention and planned careful work in order to provide a secure, resilient, and stable home network easy to use for all members of the household and their guests.
Soon after the dawn of modern computers in the late 1940s
– early 1950s, researchers started realizing the immense potential these machines had for modern society.
One of the first challenges was to make such machine capable of “thinking” like humans.
In particular, making these machines capable of making important decisions the way humans do.
The medical / healthcare field presented the tantalizing challenge to enable these machines to make medical diagnostic  decisions.
Thus, in the late 1950's, right after the information age had fully arrived, researchers started experimenting with the prospect of using computer technology to emulate human decision-making.
For example, biomedical researchers started creating computer-aided systems for diagnostic applications in medicine and biology.
These early diagnostic systems used patients' symptoms and laboratory test results as inputs to generate a diagnostic outcome.
These systems were often described as the early forms of expert systems.
However, researchers had realized that there were significant limitations when using traditional methods such as flow-charts  statistical pattern-matching,  or probability theory.
This previous situation gradually led to the development of expert systems, which used knowledge-based approaches.
These expert systems in medicine were the MYCIN expert system,  the INTERNIST-I expert system and
later, in the middle of the 1980s, the CADUCEUS.
Expert systems were formally introduced around 1965 by the Stanford Heuristic Programming Project led by Edward Feigenbaum, who is sometimes termed the "father of expert systems"; other key early contributors were Bruce Buchanan and Randall Davis.
The Stanford researchers tried to identify domains where expertise was highly valued and complex, such as diagnosing infectious diseases (Mycin) and identifying unknown organic molecules (Dendral).
The idea that "intelligent systems derive their power from the knowledge they possess rather than from the specific formalisms and inference schemes they use" – as Feigenbaum said – was at the time a significant step forward, since the past research had been focused on heuristic computational methods, culminating in attempts to develop very general-purpose problem solvers (foremostly the conjunct work of Allen Newell and Herbert Simon).
Expert systems became some of the first truly successful forms of artificial intelligence (AI) software.
Research on expert systems was also active in France.
While in the US the focus tended to be on rules-based systems, first on systems hard coded on top of LISP programming environments and then on expert system shells developed by vendors such as Intellicorp, in France research focused more on systems developed in Prolog.
The advantage of expert system shells was that they were somewhat easier for nonprogrammers to use.
The advantage of Prolog environments was that they were not focused only on if-then rules; Prolog environments provided a much better realization of a complete first order logic environment.
In the 1980s, expert systems proliferated.
Universities offered expert system courses and two thirds of the Fortune 500 companies applied the technology in daily business activities.
Interest was international with the Fifth Generation Computer Systems project in Japan and increased research funding in Europe.
In 1981, the first IBM PC, with the PC DOS operating system, was introduced.
The imbalance between the high affordability of the relatively powerful chips in the PC, compared to the much more expensive cost of processing power in the mainframes that dominated the corporate IT world at the time, created a new type of architecture for corporate computing, termed the client-server model.
Calculations and reasoning could be performed at a fraction of the price of a mainframe using a PC.
This model also enabled business units to bypass corporate IT departments and directly build their own applications.
As a result, client server had a tremendous impact on the expert systems market.
Expert systems were already outliers in much of the business world, requiring new skills that many IT departments did not have and were not eager to develop.
They were a natural fit for new PC-based shells that promised to put application development into the hands of end users and experts.
Until then, the main development environment for expert systems had been high end Lisp machines from Xerox, Symbolics, and Texas Instruments.
With the rise of the PC and client server computing, vendors such as Intellicorp and Inference Corporation shifted their priorities to developing PC based tools.
Also, new vendors, often financed by venture capital (such as Aion Corporation, Neuron Data, Exsys, and many others), started appearing regularly.
The first expert system to be used in a design capacity for a large-scale product was the SID (Synthesis of Integral Design) software program, developed in 1982.
Written in LISP, SID generated 93% of the VAX 9000 CPU logic gates.
Input to the software was a set of rules created by several expert logic designers.
SID expanded the rules and generated software logic synthesis routines many times the size of the rules themselves.
Surprisingly, the combination of these rules resulted in an overall design that exceeded the capabilities of the experts themselves, and in many cases out-performed the human counterparts.
While some rules contradicted others, top-level control parameters for speed and area provided the tie-breaker.
The program was highly controversial, but used nevertheless due to project budget constraints.
It was terminated by logic designers after the VAX 9000 project completion.
During the years before the middle of the 1970s, the expectations of what expert systems can accomplish in many fields tended to be extremely optimistic.
At the beginning of these early studies, researchers were hoping to develop entirely automatic (i.e., completely computerized) expert systems.
The expectations of people of what computers can do were frequently too idealistic.
This situation radically changed after Richard M. Karp published his breakthrough paper:
“Reducibility among Combinatorial Problems” in the early 1970s.
Thanks to Karp's work it became clear that there are certain limitations and possibilities when one designs computer algorithms.
His findings describe what computers can do and what they cannot do.
Many of the computational problems related to this type of expert systems have certain pragmatic limitations.
These findings laid down the groundwork that led to the next developments in the field.
In the 1990s and beyond, the term expert system and the idea of a standalone AI system mostly dropped from the IT lexicon.
There are two interpretations of this.
One is that "expert systems failed": the IT world moved on because expert systems did not deliver on their over hyped promise.
The other is the mirror opposite, that expert systems were simply victims of their success: as IT professionals grasped concepts such as rule engines, such tools migrated from being standalone tools for developing special purpose expert systems, to being one of many standard tools.
Many of the leading major business application suite vendors (such as SAP, Siebel, and Oracle) integrated expert system abilities into their suite of products as a way of specifying business logic – rule engines are no longer simply for defining the rules an expert would use but for any type of complex, volatile, and critical business logic; they often go hand in hand with business process automation and integration environments.
The limitations of the previous type of expert systems have urged researchers to develop new types of approaches.
They have developed more efficient, flexible and powerful approaches in order to emulate the human decision-making process.
Some of the approaches that researchers have developed are based on new methods of artificial intelligence (AI), and in particular in machine learning and data mining approaches with a feedback mechanism.
Related is the discussion on the disadvantages section.
Modern systems can incorporate new knowledge more easily and thus update themselves easily.
Such systems can generalize from existing knowledge better and deal with vast amounts of complex data.
Related is the subject of big data here.
Sometimes these type of expert systems are called “intelligent systems.
An expert system is an example of a knowledge-based system.
Expert systems were the first commercial systems to use a knowledge-based architecture.
A knowledge-based system is essentially composed of two sub-systems: the knowledge base and the inference engine.
The knowledge base represents facts about the world.
In early expert systems such as Mycin and Dendral, these facts were represented mainly as flat assertions about variables.
In later expert systems developed with commercial shells, the knowledge base took on more structure and used concepts from object-oriented programming.
The world was represented as classes, subclasses, and instances and assertions were replaced by values of object instances.
The rules worked by querying and asserting values of the objects.
The inference engine is an automated reasoning system that evaluates the current state of the knowledge-base, applies relevant rules, and then asserts new knowledge into the knowledge base.
The inference engine may also include abilities for explanation, so that it can explain to a user the chain of reasoning used to arrive at a particular conclusion by tracing back over the firing of rules that resulted in the assertion.
There are mainly two modes for an inference engine: forward chaining and backward chaining.
The different approaches are dictated by whether the inference engine is being driven by the antecedent (left hand side) or the consequent (right hand side) of the rule.
In forward chaining an antecedent fires and asserts the consequent.
For example, consider the following rule:  A simple example of forward chaining would be to assert Man(Socrates) to the system and then trigger the inference engine.
It would match R1 and assert Mortal(Socrates) into the knowledge base.
Backward chaining is a bit less straight forward.
In backward chaining the system looks at possible conclusions and works backward to see if they might be true.
So if the system was trying to determine if Mortal(Socrates) is true it would find R1 and query the knowledge base to see if Man(Socrates) is true.
One of the early innovations of expert systems shells was to integrate inference engines with a user interface.
This could be especially powerful with backward chaining.
If the system needs to know a particular fact but does not, then it can simply generate an input screen and ask the user if the information is known.
So in this example, it could use R1 to ask the user if Socrates was a Man and then use that new information accordingly.
The use of rules to explicitly represent knowledge also enabled explanation abilities.
In the simple example above if the system had used R1 to assert that Socrates was Mortal and a user wished to understand why Socrates was mortal they could query the system and the system would look back at the rules which fired to cause the assertion and present those rules to the user as an explanation.
In English if the user asked "Why is Socrates Mortal?
" the system would reply "Because all men are mortal and Socrates is a man".
A significant area for research was the generation of explanations from the knowledge base in natural English rather than simply by showing the more formal but less intuitive rules.
As expert systems evolved, many new techniques were incorporated into various types of inference engines.
Some of the most important of these were:  Truth maintenance.
These systems record the dependencies in a knowledge-base so that when facts are altered, dependent knowledge can be altered accordingly.
For example, if the system learns that Socrates is no longer known to be a man it will revoke the assertion that Socrates is mortal. Hypothetical reasoning.
In this, the knowledge base can be divided up into many possible views, a.k.a. worlds.
This allows the inference engine to explore multiple possibilities in parallel.
For example, the system may want to explore the consequences of both assertions, what will be true if Socrates is a Man and what will be true if he is not?
Uncertainty systems.
One of the first extensions of simply using rules to represent knowledge was also to associate a probability with each rule.
So, not to assert that Socrates is mortal, but to assert Socrates may be mortal with some probability value.
Simple probabilities were extended in some systems with sophisticated mechanisms for uncertain reasoning, such as Fuzzy logic, and combination of probabilities.
Ontology classification.
With the addition of object classes to the knowledge base, a new type of reasoning was possible.
Along with reasoning simply about object values, the system could also reason about object structures.
In this simple example, Man can represent an object class and R1 can be redefined as a rule that defines the class of all men.
These types of special purpose inference engines are termed classifiers.
Although they were not highly used in expert systems, classifiers are very powerful for unstructured volatile domains, and are a key technology for the Internet and the emerging Semantic Web.
The goal of knowledge-based systems is to make the critical information required for the system to work explicit rather than implicit.
In a traditional computer program the logic is embedded in code that can typically only be reviewed by an IT specialist.
With an expert system the goal was to specify the rules in a format that was intuitive and easily understood, reviewed, and even edited by domain experts rather than IT experts.
The benefits of this explicit knowledge representation were rapid development and ease of maintenance.
Ease of maintenance is the most obvious benefit.
This was achieved in two ways.
First, by removing the need to write conventional code, many of the normal problems that can be caused by even small changes to a system could be avoided with expert systems.
Essentially, the logical flow of the program (at least at the highest level) was simply a given for the system, simply invoke the inference engine.
This also was a reason for the second benefit: rapid prototyping.
With an expert system shell it was possible to enter a few rules and have a prototype developed in days rather than the months or year typically associated with complex IT projects.
A claim for expert system shells that was often made was that they removed the need for trained programmers and that experts could develop systems themselves.
In reality, this was seldom if ever true.
While the rules for an expert system were more comprehensible than typical computer code, they still had a formal syntax where a misplaced comma or other character could cause havoc as with any other computer language.
Also, as expert systems moved from prototypes in the lab to deployment in the business world, issues of integration and maintenance became far more critical.
Inevitably demands to integrate with, and take advantage of, large legacy databases and systems arose.
To accomplish this, integration required the same skills as any other type of system.
The most common disadvantage cited for expert systems in the academic literature is the knowledge acquisition problem.
Obtaining the time of domain experts for any software application is always difficult, but for expert systems it was especially difficult because the experts were by definition highly valued and in constant demand by the organization.
As a result of this problem, a great deal of research in the later years of expert systems was focused on tools for knowledge acquisition, to help automate the process of designing, debugging, and maintaining rules defined by experts.
However, when looking at the life-cycle of expert systems in actual use, other problems – essentially the same problems as those of any other large system – seem at least as critical as knowledge acquisition: integration, access to large databases, and performance.
Performance could be especially problematic because early expert systems were built using tools (such as earlier Lisp versions) that interpreted code expressions without first compiling them.
This provided a powerful development environment, but with the drawback that it was virtually impossible to match the efficiency of the fastest compiled languages (such as C).
System and database integration were difficult for early expert systems because the tools were mostly in languages and platforms that were neither familiar to nor welcome in most corporate IT environments – programming languages such as Lisp and Prolog, and hardware platforms such as Lisp machines and personal computers.
As a result, much effort in the later stages of expert system tool development was focused on integrating with legacy environments such as COBOL and large database systems, and on porting to more standard platforms.
These issues were resolved mainly by the client-server paradigm shift, as PCs were gradually accepted in the IT environment as a legitimate platform for serious business system development and as affordable minicomputer servers provided the processing power needed for AI applications.
Another major challenge of expert systems emerges when the size of the knowledge base increases.
This causes the processing complexity to increase.
For instance, when an expert system with 100 million rules was envisioned as the ultimate expert system, it became obvious that such system would be too complex and it would face too many computational problems.
An inference engine would have to be able to process huge numbers of rules to reach a decision.
How to verify that decision rules are consistent with each other is also a challenge when there are too many rules.
Usually such problem leads to a satisfiability (SAT) formulation.
This is a well-known NP-complete problem Boolean satisfiability problem.
If we assume only binary variables, say n of them, and then the corresponding search space is of size 2                                                           n                                     {\displaystyle ^{n}}   .
Thus, the search space can grow exponentially.
There are also questions on how to prioritize the use of the rules in order to operate more efficiently, or how to resolve ambiguities (for instance, if there are too many else-if sub-structures within a single rule) and so on.
Other problems are related to the overfitting and overgeneralization effects when using known facts and trying to generalize to other cases not described explicitly in the knowledge base.
Such problems exist with methods that employ machine learning approaches too.
Another problem related to the knowledge base is how to make updates of its knowledge quickly and effectively.
Also how to add a new knowledge (i.e., where to add among many rules) is challenging.
Modern approaches that rely on machine learning methods are easier in this regard.
Because of the above challenges, it became clear that a new approaches to AI were required instead of rule-based technologies.
These new approaches are based on the use of machine learning techniques, along with the use of feedback mechanisms.
The key challenges that expert systems in medicine (if one considers computer-aided diagnostic systems as modern expert systems), and perhaps in other application domains, include issues related to aspects such as: big data, existing regulations, healthcare practice, various algorithmic issues, and system assessment.
Hayes-Roth divides expert systems applications into 10 categories illustrated in the following table.
The example applications were not in the original Hayes-Roth table, and some of them arose well afterward.
Any application that is not footnoted is described in the Hayes-Roth book.
Also, while these categories provide an intuitive framework to describe the space of expert systems applications, they are not rigid categories, and in some cases an application may show traits of more than one category.
Hearsay was an early attempt at solving voice recognition through an expert systems approach.
For the most part this category or expert systems was not all that successful.
Hearsay and all interpretation systems are essentially pattern recognition systems—looking for patterns in noisy data.
In the case of Hearsay recognizing phonemes in an audio stream.
Other early examples were analyzing sonar data to detect Russian submarines.
These kinds of systems proved much more amenable to a neural network AI solution than a rule-based approach.
CADUCEUS and MYCIN were medical diagnosis systems.
The user describes their symptoms to the computer as they would to a doctor and the computer returns a medical diagnosis.
Dendral was a tool to study hypothesis formation in the identification of organic molecules.
The general problem it solved—designing a solution given a set of constraints—was one of the most successful areas for early expert systems applied to business domains such as salespeople configuring Digital Equipment Corporation (DEC) VAX computers and mortgage loan application development.
SMH.PAL is an expert system for the assessment of students with multiple disabilities.
Mistral  is an expert system to monitor dam safety, developed in the 90's by Ismes (Italy).
It gets data from an automatic monitoring system and performs a diagnosis of the state of the dam.
Its first copy, installed in 1992 on the Ridracoli Dam (Italy), is still operational 24/7/365.
It has been installed on several dams in Italy and abroad (e.g., Itaipu Dam in Brazil), and on landslide sites under the name of Eydenet, and on monuments under the name of Kaleidos.
Mistral is a registered trade mark of CESI.
Human facial expression has been the subject of scientific investigation for more than one hundred years.
Study of facial movements and expressions started from a biological point of view.
After some older investigations, for example by John Bulwer in the late 1640s, Charles Darwin's book The Expression of the Emotions in Men and Animals can be considered a major departure for modern research in behavioural biology.
Computer based facial expression modelling and animation is not a new endeavour.
The earliest work with computer based facial representation was done in the early-1970s.
The first three-dimensional facial animation was created by Parke in 1972.
In 1973, Gillenson developed an interactive system to assemble and edit line drawn facial images.
in 1974, Parke developed a parameterized three-dimensional facial model.
One of the most important attempts to describe facial movements was Facial Action Coding System (FACS).
Originally developed by Carl-Herman Hjortsjö  in the 1960s and updated by Ekman and Friesen in 1978, FACS defines 46 basic facial Action Units (AUs).
A major group of these Action Units represent primitive movements of facial muscles in actions such as raising brows, winking, and talking.
Eight AU's are for rigid three-dimensional head movements, (i.e. turning and tilting left and right and going up, down, forward and backward).
FACS has been successfully used for describing desired movements of synthetic faces and also in tracking facial activities.
The early-1980s saw the development of the first physically based muscle-controlled face model by Platt and the development of techniques for facial caricatures by Brennan.
In 1985, the animated short film Tony de Peltrie was a landmark for facial animation.
This marked the first time computer facial expression and speech animation were a fundamental part of telling the story.
The late-1980s saw the development of a new muscle-based model by Waters, the development of an abstract muscle action model by Magnenat-Thalmann and colleagues, and approaches to automatic speech synchronization by Lewis and Hill.
The 1990s have seen increasing activity in the development of facial animation techniques and the use of computer facial animation as a key storytelling component as illustrated in animated films such as Toy Story (1995), Antz (1998), Shrek, and Monsters,
Inc. (both 2001), and computer games such as Sims.
Casper (1995), a milestone in this decade, was the first movie in which a lead actor was produced exclusively using digital facial animation.
The sophistication of the films increased after 2000.
In The Matrix Reloaded and The Matrix Revolutions, dense optical flow from several high-definition cameras was used to capture realistic facial movement at every point on the face.
Polar Express (film) used a large Vicon system to capture upward of 150 points.
Although these systems are automated, a large amount of manual clean-up effort is still needed to make the data usable.
Another milestone in facial animation was reached by The Lord of the Rings, where a character specific shape base system was developed.
Mark Sagar pioneered the use of FACS in entertainment facial animation, and FACS based systems developed by Sagar were used on Monster House, King Kong, and other films.
The generation of facial animation data can be approached in different ways:
1.) marker-based motion capture on points or marks on the face of a performer, 2.) markerless motion capture techniques using different type of cameras, 3.)
audio-driven techniques, and 4.)
keyframe animation.
Motion capture uses cameras placed around a subject.
The subject is generally fitted either with reflectors (passive motion capture) or sources (active motion capture) that precisely determine the subject's position in space.
The data recorded by the cameras is then digitized and converted into a three-dimensional computer model of the subject.
Until recently, the size of the detectors/sources used by motion capture systems made the technology inappropriate for facial capture.
However, miniaturization and other advancements have made motion capture a viable tool for computer facial animation.
Facial motion capture was used extensively in Polar Express by Imageworks where hundreds of motion points were captured.
This film was very accomplished and while it attempted to recreate realism, it was criticized for having fallen in the 'uncanny valley', the realm where animation realism is sufficient for human recognition and to convey the emotional message but where the characters fail to be perceived as realistic.
The main difficulties of motion capture are the quality of the data which may include vibration as well as the retargeting of the geometry of the points.
Markerless motion capture aims at simplifying the motion capture process by avoiding encumbering the performer with markers.
Several techniques came out recently leveraging different sensors, among which standard video cameras, Kinect and depth sensors or other structured-light based devices.
Systems based on structured light may achieve real-time performance without the use of any markers using a high speed structured light scanner.
The system is based on a robust offline face tracking stage which trains the system with different facial expressions.
The matched sequences are used to build a person-specific linear face model that is subsequently used for online face tracking and expression transfer.
Audio-driven techniques are particularly well fitted for speech animation.
Speech is usually treated in a different way to the animation of facial expressions, this is because simple keyframe-based approaches to animation typically provide a poor approximation to real speech dynamics.
Often visemes are used to represent the key poses in observed speech (i.e. the position of the lips, jaw and tongue when producing a particular phoneme), however there is a great deal of variation in the realisation of visemes during the production of natural speech.
The source of this variation is termed coarticulation which is the influence of surrounding visemes upon the current viseme (i.e. the effect of context).
To account for coarticulation current systems either explicitly take into account context when blending viseme keyframes or use longer units such as diphone, triphone, syllable or even word and sentence-length units.
One of the most common approaches to speech animation is the use of dominance functions introduced by Cohen and Massaro.
Each dominance function represents the influence over time that a viseme has on a speech utterance.
Typically the influence will be greatest at the center of the viseme and will degrade with distance from the viseme center.
Dominance functions are blended together to generate a speech trajectory in much the same way that spline basis functions are blended together to generate a curve.
The shape of each dominance function will be different according to both which viseme it represents and what aspect of the face is being controlled (e.g. lip width, jaw rotation etc.).
This approach to computer-generated speech animation can be seen in the Baldi talking head.
Other models of speech use basis units which include context (e.g. diphones, triphones etc.) instead of visemes.
As the basis units already incorporate the variation of each viseme according to context and to some degree the dynamics of each viseme, no model of coarticulation is required.
Speech is simply generated by selecting appropriate units from a database and blending the units together.
This is similar to concatenative techniques in audio speech synthesis.
The disadvantage to these models is that a large amount of captured data is required to produce natural results, and whilst longer units produce more natural results the size of database required expands with the average length of each unit.
Finally, some models directly generate speech animations from audio.
These systems typically use hidden markov models or neural nets to transform audio parameters into a stream of control parameters for a facial model.
The advantage of this method is the capability of voice context handling, the natural rhythm, tempo, emotional and dynamics handling without complex approximation algorithms.
The training database is not needed to be labeled since there are no phonemes or visemes needed; the only needed data is the voice and the animation parameters.
Keyframe animation is the least automated of the processes to create animation data although it delivers the maximum amount of control over the animation.
It is often used in combination with other techniques to deliver the final polish to the animation.
The keyframe data can be made of scalar values defining the morph targets coefficients or rotation and translation values of the bones in models with a bone based rig.
Often to speed up the keyframe animation process a control rig is used by the animation.
The control rig represents a higher level of abstraction that can act on multiple morph targets coefficients or bones at the same time.
For example, a "smile" control can act simultaneously on the mouth shape curving up and the eyes squinting.
The main techniques used to apply facial animation to a character are: 1.)
morph targets animation, 2.)
bone driven animation, 3.) texture-based animation (
2D or 3D), and 4.) physiological models.
Morph targets (also called "blendshapes") based systems offer a fast playback as well as a high degree of fidelity of expressions.
The technique involves modeling portions of the face mesh to approximate expressions and visemes and then blending the different sub meshes, known as morph targets or blendshapes.
Perhaps the most accomplished character using this technique was Gollum, from The Lord of the Rings.
Drawbacks of this technique are that they involve intensive manual labor and are specific to each character.
Recently, new concepts in 3D modeling have started to emerge.
Recently, a new technology departing from the traditional techniques starts to emerge, such as Curve Controlled Modeling  that emphasizes the modeling of the movement of a 3D object instead of the traditional modeling of the static shape.
Bone driven animation is very broadly used in games.
The bones setup can vary between few bones to close to a hundred to allow all subtle facial expressions.
The main advantages of bone driven animation is that the same animation can be used for different characters as long as the morphology of their faces is similar, and secondly they do not require loading in memory all the Morph targetsdata.
Bone driven animation is most widely supported by 3D game engines.
Bone driven animation can be used both 2D and 3D animation.
For example, it is possible to rig and animated using bones a 2D character using Adobe Flash.
Texture-based animation uses pixel color to create the animation on the character face.
2D facial animation is commonly based upon the transformation of images, including both images from still photography and sequences of video.
Image morphing is a technique which allows in-between transitional images to be generated between a pair of target still images or between frames from sequences of video.
These morphing techniques usually consist of a combination of a geometric deformation technique, which aligns the target images, and a cross-fade which creates the smooth transition in the image texture.
An early example of image morphing can be seen in Michael Jackson's video for "Black Or White".
In 3D animation texture based animation can be achieved by animating the texture itself or the UV mapping.
In the latter case a texture map of all the facial expression is created and the UV map animation is used to transition from one expression to the next.
Physiological models, such as skeletal muscle systems and physically based head models, form another approach in modeling the head and face.
Here, the physical and anatomical characteristics of bones, tissues, and skin are simulated to provide a realistic appearance (e.g. spring-like elasticity).
Such methods can be very powerful for creating realism but the complexity of facial structures make them computationally expensive, and difficult to create.
Considering the effectiveness of parameterized models for communicative purposes (as explained in the next section), it may be argued that physically based models are not a very efficient choice in many applications.
This does not deny the advantages of physically based models and the fact that they can even be used within the context of parameterized models to provide local details when needed.
Many face animation languages are used to describe the content of facial animation.
They can be input to a compatible "player" software which then creates the requested actions.
Face animation languages are closely related to other multimedia presentation languages such as SMIL and VRML.
Due to the popularity and effectiveness of XML as a data representation mechanism, most face animation languages are XML-based.
For instance, this is a sample from Virtual Human Markup Language (VHML):
More advanced languages allow decision-making, event handling, and parallel and sequential actions.
The Face Modeling Language (FML) is an XML-based language for describing face animation.
FML supports
MPEG-4 Face Animation Parameters (FAPS), decision-making and dynamic event handling, and typical programming constructs such as loops.
It is part of the iFACE system.
The following is an example from FML:
Computer Facial Animation by Frederic I. Parke, Keith Waters 2008
ISBN 1-56881-448-8 Data-driven 3D facial animation by Zhigang Deng,
Ulrich Neumann 2007
ISBN 1-84628-906-8 Handbook of Virtual Humans by Nadia Magnenat-Thalmann and Daniel Thalmann, 2004
ISBN 0
-470-02316-3
Osipa, Jason (2005).
Stop Staring: Facial Modeling and Animation
Done
Right
(2nd ed.).
John Wiley & Sons.
ISBN 978-0-471-78920-8.
AI for Good is a movement in which institutions are employing AI to tackle some of the world's greatest economic and social challenges.
For example, the University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address socially relevant problems such as homelessness.
At Stanford, researchers are using AI to analyze satellite images to identify which areas have the highest poverty levels.
In agriculture new AI advancements show improvements in gaining yield and to increase the research and development of growing crops.
New artificial intelligence now predicts the time it takes for a crop like a tomato to be ripe and ready for picking thus increasing efficiency of farming.
These advances go on including Crop and Soil Monitoring, Agricultural Robots, and Predictive Analytics.
Crop and soil monitoring uses new algorithms and data collected on the field to manage and track the health of crops making it easier and more sustainable for the farmers.
More specializations of Ai in agriculture is one such as greenhouse automation, simulation, modeling, and optimization techniques.
Due to the increase in population and the growth of demand for food in the future there will need to be at least a 70% increase in yield from agriculture to sustain this new demand.
More and more of the public perceives that the adaption of these new techniques and the use of Artificial intelligence will help reach that goal.
The Air Operations Division (AOD) uses AI for the rule based expert systems.
The AOD has use for artificial intelligence for surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.
The use of artificial intelligence in simulators is proving to be very useful for the AOD.
Airplane simulators are using artificial intelligence in order to process the data taken from simulated flights.
Other than simulated flying, there is also simulated aircraft warfare.
The computers are able to come up with the best success scenarios in these situations.
The computers can also create strategies based on the placement, size, speed and strength of the forces and counter forces.
Pilots may be given assistance in the air during combat by computers.
The artificial intelligent programs can sort the information and provide the pilot with the best possible maneuvers, not to mention getting rid of certain maneuvers that would be impossible for a human being to perform.
Multiple aircraft are needed to get good approximations for some calculations so computer simulated pilots are used to gather data.
These computer simulated pilots are also used to train future air traffic controllers.
The system used by the AOD in order to measure performance was the Interactive Fault Diagnosis and Isolation System, or IFDIS.
It is a rule based expert system put together by collecting information from TF-30 documents and the expert advice from mechanics that work on the TF-30.
This system was designed to be used for the development of the TF-30 for the RAAF F-111C.
The performance system was also used to replace specialized workers.
The system allowed the regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.
The AOD also uses artificial intelligence in speech recognition software.
The air traffic controllers are giving directions to the artificial pilots and the AOD wants to the pilots to respond to the ATC's with simple responses.
The programs that incorporate the speech software must be trained, which means they use neural networks.
The program used, the Verbex 7000, is still a very early program that has plenty of room for improvement.
The improvements are imperative because ATCs use very specific dialog and the software needs to be able to communicate correctly and promptly every time.
The Artificial Intelligence supported Design of Aircraft, or AIDA, is used to help designers in the process of creating conceptual designs of aircraft.
This program allows the designers to focus more on the design itself and less on the design process.
The software also allows the user to focus less on the software tools.
The AIDA uses rule based systems to compute its data.
This is a diagram of the arrangement of the AIDA modules.
Although simple, the program is proving effective.
In 2003, NASA's Dryden Flight Research Center, and many other companies, created software that could enable a damaged aircraft to continue flight until a safe landing zone can be reached.
The software compensates for all the damaged components by relying on the undamaged components.
The neural network used in the software proved to be effective and marked a triumph for artificial intelligence.
The Integrated Vehicle Health Management system, also used by NASA, on board an aircraft must process and interpret data taken from the various sensors on the aircraft.
The system needs to be able to determine the structural integrity of the aircraft.
The system also needs to implement protocols in case of any damage taken the vehicle.
Haitham Baomar and Peter Bentley are leading a team from the University College of London to develop an artificial intelligence based Intelligent Autopilot System (IAS) designed to teach an autopilot system to behave like a highly experienced pilot who is faced with an emergency situation such as severe weather, turbulence, or system failure.
Educating the autopilot relies on the concept of supervised machine learning “which treats the young autopilot as a human apprentice going to a flying school”.
The autopilot records the actions of the human pilot generating learning models using artificial neural networks.
The autopilot is then given full control and observed by the pilot as it executes the training exercise.
The Intelligent Autopilot System combines the principles of Apprenticeship Learning and Behavioural Cloning whereby the autopilot observes the low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.
IAS implementation employs three phases; pilot data collection, training, and autonomous control.
Baomar and Bentley's goal is to create a more autonomous autopilot to assist pilots in responding to emergency situations.
AI researchers have created many tools to solve the most difficult problems in computer science.
Many of their inventions have been adopted by mainstream computer science and are no longer considered a part of AI.
(See AI effect.)
According to Russell & Norvig (2003, p. 15), all of the following were originally developed in AI laboratories: time sharing, interactive interpreters, graphical user interfaces and the computer mouse, Rapid application development environments, the linked list data structure, automatic storage management, symbolic programming, functional programming, dynamic programming and object-oriented programming.
AI can be used to potentially determine the developer of anonymous binaries.
AI can be used to create other AI.
For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and COCO.
According to Google, NASNet's performance exceeded all previously published ImageNet performance.
In June 2016, a research team from the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face, a program which animates the face of a target person, transposing the facial expressions of an exterior source.
The technology has been demonstrated animating the lips of people including Barack Obama and Vladimir Putin.
Since then, other methods have been demonstrated based on deep neural network, from which the name "deepfake" was taken.
Hollywood film studios had already used the technique in animated films, but it took time and efforts from professionals.
The main difference is that today anyone can use a deep fake software and rig videos.
In September 2018, the U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deepfake documents on their platform.
Vincent Nozick, a researcher from the Institut Gaspard Monge, found a way to detect rigged documents by analyzing the movements of the eyelid.
The DARPA (a research group associated with the U.S. Department of Defense) has given 68 million dollars to work on deepfake detection.
In Europe, the Horizon 2020 program financed InVid, software designed to help journalists to detect fake documents.
The Future of AI in the classroom One of the more promising innovations is the idea of a personal AI tutor or assistant for each individual student.
Because a single teacher can't work with every student at once, AI tutors would allow for students to get extra, one-on-one help in areas of needed growth.
AI tutors also eliminate the intimidating idea of tutor labs or human tutors which can cause anxiety and stress for some students.
In future classrooms, ambient informatics can play a beneficial role.
Ambient informatics is the idea that information is everywhere in the environment and that technologies automatically adjust to your personal preferences.
When students sit at their desk, their devices will be able to create lessons, problems, and games to tailor to the specific student's needs, particularly where a student may be struggling, and give immediate feedback.
This eliminates the idea of a “one-size-fits-all classroom” as we will no longer have to force students to learn exactly the same material at exactly the same pace.
While there are many benefits to the use of AI in the classroom, there are also several dangers that need to be taken into account before implementing them.
As far as the future of AI in education, there are many new possibilities due to what has been coined by The New York Times as “The Great AI Awakening.
” One of these possibilities mentioned by Forbes included the providing of adaptive learning programs, which assess and react to a student's emotions and learning preferences.
Another advancement includes the presentation of performance data and enrichment methods on an individual basis.
Within curriculum, AI could help determine if there are underlying biases in texts and instructions.
For teachers, AI could soon have the power to relay data regarding efficacy of varying learning interventions from a, potentially, global database.
As a whole AI has the power to influence education by taking district, state, national, and global data into consideration as it seeks to better individualize learning for all.
Although AI can provide many assets to a classroom, many experts still agree that they will not be able to replace teachers altogether.
Many teachers fear the idea of AI replacing them in the classroom, especially with the idea of personal AI assistants for each student.
The reality is, AI can create a more dystopian environment with revenge effects.
This means that technology is inhibiting society from moving forward and causing negative, unintended effects on society.
An example of a revenge effect is that the extended use of technology may hinder students' ability to focus and stay on task instead of helping them learn and grow.
Also, AI has been known to lead to the loss of both human agency and simultaneity.
If students are relying solely on AI tutors composed of algorithms and wires, it reduces their ability to control their own education and learning.
Also, the need for AI technologies to work simultaneously may lead to system failures which could ruin an entire school day if we are relying on AI assistants to create lessons for students every day.
It is inevitable that AI technologies will be taking over the classroom in the years to come, thus it is essential that the kinks of these new innovations are worked out before teachers decide whether or not to implement them into their daily schedules.
Algorithmic trading involves the use of complex AI systems to make trading decisions at speeds several orders of magnitudes greater than any human is capable of, often making millions of trades in a day without any human intervention.
Such trading is called High-frequency Trading, and it represents one of the fastest growing sectors in financial trading.
Many banks, funds, and proprietary trading firms now have entire portfolios which are managed purely by AI systems.
Automated trading systems are typically used by large institutional investors, but recent years have also seen an influx of smaller, proprietary firms trading with their own AI systems.
Several large financial institutions have invested in AI engines to assist with their investment practices.
BlackRock's AI engine, Aladdin, is used both within the company and to clients to help with investment decisions.
Its wide range of functionalities includes the use of natural language processing to read text such as news, broker reports, and social media feeds.
It then gauges the sentiment on the companies mentioned and assigns a score.
Banks such as UBS and Deutsche Bank use an AI engine called Sqreem (Sequential Quantum Reduction and Extraction Model) which can mine data to develop consumer profiles and match them with the wealth management products they'd most likely want.
Goldman Sachs uses Kensho, a market analytics platform that combines statistical computing with big data and natural language processing.
Its machine learning systems mine through hoards of data on the web and assess correlations between world events and their impact on asset prices.
Information Extraction, part of artificial intelligence, is used to extract information from live news feed and to assist with investment decisions.
Several products are emerging that utilize AI to assist people with their personal finances.
For example, Digit is an app powered by artificial intelligence that automatically helps consumers optimize their spending and savings based on their own personal habits and goals.
The app can analyze factors such as monthly income, current balance, and spending habits, then make its own decisions and transfer money to the savings account.
Wallet.
AI, an upcoming startup in San Francisco, builds agents that analyze data that a consumer would leave behind, from Smartphone check-ins to tweets, to inform the consumer about their spending behavior.
Robo-advisors are becoming more widely used in the investment management industry.
Robo-advisors provide financial advice and portfolio management with minimal human intervention.
This class of financial advisers work based on algorithms built to automatically develop a financial portfolio according to the investment goals and risk tolerance of the clients.
It can adjust to real-time changes in the market and accordingly calibrate the portfolio.
An online lender, Upstart, analyze vast amounts of consumer data and utilizes machine learning algorithms to develop credit risk models that predict a consumer's likelihood of default.
Their technology will be licensed to banks for them to leverage for their underwriting processes as well.
ZestFinance developed their Zest Automated Machine Learning (ZAML) Platform specifically for credit underwriting as well.
This platform utilizes machine learning to analyze tens of thousands traditional and nontraditional variables (from purchase transactions to how a customer fills out a form) used in the credit industry to score borrowers.
The platform is particularly useful to assign credit scores to those with limited credit histories, such as millennials.
The 1980s is really when AI started to become prominent in the finance world.
This is when expert systems became more of a commercial product in the financial field.
For example, Dupont had built 100 expert systems which helped them save close to $10 million a year.
” One of the first systems was the Protrader expert system designed by K.C. Chen and Ting-peng Lian that was able to predict the 87-point drop in DOW Jones Industrial Average in 1986.
The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.
One of the first expert systems that helped with financial plans was created by Applied Expert Systems (APEX) called the PlanPower.
It was first commercially shipped in 1986.
Its function was to help give financial plans for people with incomes over $75,000 a year.
That then led to the Client Profiling System that was used for incomes between $25,000 and $200,000 a year.
The 1990s was a lot more about fraud detection.
One of the systems that was started in 1993 was the FinCEN Artificial Intelligence system (FAIS).
It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering which would have been equal to $1 billion.
Although expert systems did not last in the finance world, it did help jump-start the use of AI and help make it what it is today.
Robots have become common in many industries and are often given jobs that are considered dangerous to humans.
Robots have proven effective in jobs that are very repetitive which may lead to mistakes or accidents due to a lapse in concentration and other jobs which humans may find degrading.
In 2014, China, Japan, the United States, the Republic of Korea and Germany together amounted to 70% of the total sales volume of robots.
In the automotive industry, a sector with particularly high degree of automation, Japan had the highest density of industrial robots in the world: 1,414 per 10,000 employees.
Artificial neural networks are used as clinical decision support systems for medical diagnosis, such as in Concept Processing technology in EMR software.
Other tasks in medicine that can potentially be performed by artificial intelligence and are beginning to be developed include:  Computer-aided interpretation of medical images.
Such systems help scan digital images, e.g. from computed tomography, for typical appearances and to highlight conspicuous sections, such as possible diseases.
A typical application is the detection of a tumor.
Heart sound analysis Companion robots for the care of the elderly Mining medical records to provide more useful information.
Design treatment plans.
Assist in repetitive jobs including medication management.
Provide consultations.
Drug creation Using avatars in place of patients for clinical training Predict the likelihood of death from surgical procedures Predict HIV progressionThere are over 90 AI startups in the health industry working in these fields.
IDx's first solution, IDx-DR, is the first autonomous AI-based diagnostic system authorized for commercialization by the FDA.
Another application of AI is in the human resources and recruiting space.
There are three ways AI is being used by human resources and recruiting professionals: to screen resumes and rank candidates according to their level of qualification, to predict candidate success in given roles through job matching platforms, and rolling out recruiting chat bots that can automate repetitive communication tasks.
Typically, resume screening involves a recruiter or other HR professional scanning through a database of resumes.
The job market has seen a notable change due to artificial intelligence implementation.
It has simplified the process for both recruiters and job seekers (i.e., Google for Jobs and applying online).
According to Raj Mukherjee from Indeed.com, 65% of people launch a job search again within 91 days of being hired.
AI-powered engine streamlines the complexity of job hunting by operating information on job skills, salaries, and user tendencies, matching people to the most relevant positions.
Machine intelligence calculates what wages would be appropriate for a particular job, pulls and highlights resume information for recruiters using natural language processing, which extracts relevant words and phrases from text using specialized software.
Another application is an AI resume builder which requires 5 minutes to compile a CV as opposed to spending hours doing the same job.
In the AI age chatbots assist website visitors and solve daily workflows.
Revolutionary AI tools complement people's skills and allow HR managers to focus on tasks of higher priority.
However, Artificial Intelligence impact on jobs research suggests that by 2030 intelligent agents and robots can eliminate 30% of the world's human labor.
Moreover, the research proves automation will displace between 400 and 800 million employees.
Glassdoor's research report states that recruiting and HR are expected to see much broader adoption of AI in job market 2018 and beyond.
Some AI applications are geared towards the analysis of audiovisual media content such as movies, TV programs, advertisement videos or user-generated content.
The solutions often involve computer vision, which is a major application area of AI.
Typical use case scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for recognizing relevant scenes, objects or faces.
The motivation for using AI-based media analysis can be — among other things
— the facilitation of media search, the creation of a set of descriptive keywords for a media item, media content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for the placement of relevant advertisements.
Media analysis AI companies often provide their services over a REST API that enables machine-based automatic access to the technology and allows machine-reading of the results.
For example, IBM, Microsoft, Amazon and the video AI company Valossa allow access to their media recognition technology by using RESTful APIs.
AI is also widely used in E-commerce applications like visual search, chatbots, and automated product tagging.
Another generic application is to increase search discoverability and making social media content shoppable.
The main military applications of Artificial Intelligence and Machine Learning are to enhance Command and Control, Communications, Sensors, Integration and Interoperability.
Artificial Intelligence technologies enables coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Join Fires between networked combat vehicles and tanks also
inside  Manned  and
Unmanned Teams (MUM-T).
While the evolution of music has always been affected by technology, artificial intelligence has enabled, through scientific advances, to emulate, at some extent, human-like composition.
Among notable early efforts, David Cope created an AI called Emily Howell that managed to become well known in the field of Algorithmic Computer Music.
The algorithm behind Emily Howell is registered as a US patent.
The AI Iamus created 2012 the first complete classical album fully composed by a computer.
Other endeavours, like AIVA (Artificial Intelligence Virtual Artist), focus on composing symphonic music, mainly classical music for film scores.
It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.
Artificial intelligences can even produce music usable in a medical setting, with Melomics's effort to use computer-generated music for stress and pain relief.
Moreover, initiatives such as Google Magenta, conducted by the Google Brain team, want to find out if an artificial intelligence can be capable of creating compelling art.
At Sony CSL Research Laboratory, their Flow Machines software has created pop songs by learning music styles from a huge database of songs.
By analyzing unique combinations of styles and optimizing techniques, it can compose in any style.
Another artificial intelligence musical composition project,
The Watson Beat, written by IBM Research, doesn't need a huge database of music like the Google Magenta and Flow Machines projects, since it uses Reinforcement Learning and Deep Belief Networks to compose music on a simple seed input melody and a select style.
Since the software has been open sourced musicians, such as Taryn Southern have been collaborating with the project to create music.
The company Narrative Science makes computer-generated news and reports commercially available, including summarizing team sporting events based on statistical data from the game in English.
It also creates financial reports and real estate analyses.
Similarly, the company Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.
The company is projected to generate one billion stories in 2014, up from 350 million in 2013.
The organisation OpenAI has also created an AI capable of writing text.
Echobox is a software company that helps publishers increase traffic by 'intelligently' posting articles on social media platforms such as Facebook and Twitter.
By analysing large amounts of data, it learns how specific audiences respond to different articles at different times of the day.
It then chooses the best stories to post and the best times to post them.
It uses both historical and real-time data to understand to what has worked well in the past as well as what is currently trending on the web.
Another company, called Yseop, uses artificial intelligence to turn structured data into intelligent comments and recommendations in natural language.
Yseop is able to write financial reports, executive summaries, personalized sales or marketing documents and more at a speed of thousands of pages per second and in multiple languages including English, Spanish, French & German.
Boomtrain's is another example of AI that is designed to learn how to best engage each individual reader with the exact articles—sent through the right channel at the right time—that will be most relevant to the reader.
It's like hiring a personal editor for each individual reader to curate the perfect reading experience.
IRIS.TV is helping media companies with its AI-powered video personalization and programming platform.
It allows publishers and content owners to surface contextually relevant content to audiences based on consumer viewing patterns.
Beyond automation of writing tasks given data input, AI has shown significant potential for computers to engage in higher-level creative work.
AI Storytelling has been an active field of research since James Meehan's development of TALESPIN, which made up stories similar to the fables of Aesop.
The program would start with a set of characters who wanted to achieve certain goals, with the story as a narration of the characters' attempts at executing plans to satisfy these goals.
Since Meehan, other researchers have worked on AI Storytelling using similar or different approaches.
Mark Riedl and Vadim Bulitko argued that the essence of storytelling was an experience management problem, or "how to balance the need for a coherent story progression with user agency, which are often at odds."While most research on AI storytelling has focused on story generation (e.g. character and plot), there has also been significant investigation in story communication.
In 2002, researchers at North Carolina State University developed an architectural framework for narrative prose generation.
Their particular implementation was able faithfully reproduced text variety and complexity of a number of stories, such as red riding hood, with human-like adroitness.
This particular field continues to gain interest.
In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.
Artificial intelligence is implemented in automated online assistants that can be seen as avatars on web pages.
It can avail for enterprises to reduce their operation and training cost.
A major underlying technology to such systems is natural language processing.
Pypestream uses automated customer service for its mobile application designed to streamline communication with customers.
Major companies are investing in AI to handle difficult customer in the future.
Google's most recent development analyzes language and converts speech into text.
The platform can identify angry customers through their language and respond appropriately.
Power electronics converters are an enabling technology for renewable energy, energy storage, electric vehicles and high-voltage direct current transmission systems within the electrical grid.
These converters are prone to failures and such failures can cause downtimes that may require costly maintenance or even have catastrophic consequences in mission critical applications.
Researchers are using AI to do the automated design process for reliable power electronics converters, by calculating exact design parameters that ensure desired lifetime of the converter under specified mission profile.
Artificial Intelligence has been combined with many sensor technologies, such as Digital Spectrometry by IdeaCuria Inc. which enables many applications such as at home water quality monitoring.
Many telecommunications companies make use of heuristic search in the management of their workforces, for example
BT Group has deployed heuristic search in a scheduling application that provides the work schedules of 20,000 engineers.
The 1990s saw some of the first attempts to mass-produce domestically aimed types of basic Artificial Intelligence for education, or leisure.
This prospered greatly with the Digital Revolution, and helped introduce people, especially children, to a life of dealing with various types of Artificial Intelligence, specifically in the form of Tamagotchis and Giga Pets, iPod Touch, the Internet, and the first widely released robot, Furby.
A mere year later an improved type of domestic robot was released in the form of Aibo, a robotic dog with intelligent features and autonomy.
Companies like Mattel have been creating an assortment of AI-enabled toys for kids as young as age three.
Using proprietary AI engines and speech recognition tools, they are able to understand conversations, give intelligent responses and learn quickly.
AI has also been applied to video games, for example video game bots, which are designed to stand in as opponents where humans aren't available or desired.
Fuzzy logic controllers have been developed for automatic gearboxes in automobiles.
For example, the 2006 Audi TT, VW Touareg and VW Caravell feature the DSP transmission which utilizes Fuzzy Logic.
A number of Škoda variants (Škoda Fabia) also currently include a Fuzzy Logic-based controller.
Today's cars now have AI-based driver assist features such as self-parking and advanced cruise controls.
AI has been used to optimize traffic management applications, which in turn reduces wait times, energy use, and emissions by as much as 25 percent.
In the future, fully autonomous cars will be developed.
AI in transportation is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities.
The major challenge to developing this AI is the fact that transportation systems are inherently complex systems involving a very large number of components and different parties, each having different and often conflicting objectives.
Due to this high degree of complexity of the transportation, and in particular the automotive, application, it is in most cases not possible to train an AI algorithm in a real-world driving environment.
To overcome the challenge of training neural networks for automated driving, methodologies based on virtual development resp.
testing toolchains have been proposed.
Studies related to Wikipedia has been using artificial intelligence to support various operations.
One of the most important areas - automatic detection of vandalism  and data quality assessment in Wikipedia.
The team at the Wikimedia Foundation released a model that is designed to detect vandalism, spam, and personal attack.
This model can also help students write better Wikipedia articles.
Typical problems to which AI methods are applied Other fields in which AI methods are implemented
Eugene Goostman is portrayed as being a 13-year-old boy from Odessa, Ukraine, who has a pet guinea pig and a father who is a gynaecologist.
Veselov stated that Goostman was designed to be a "character with a believable personality".
The choice of age was intentional, as, in Veselov's opinion, a thirteen-year-old is "not too old to know everything and not too young to know nothing".
Goostman's young age also induces people who "converse" with him to forgive minor grammatical errors in his responses.
In 2014, work was made on improving the bot's "dialog controller", allowing Goostman to output more human-like dialogue.
A conversation between Scott Aaronson and Eugene Goostman ran as follows:
Eugene Goostman has competed in a number of Turing test competitions, including the Loebner Prize contest; it finished joint second in the Loebner test in 2001, and came second to Jabberwacky in 2005 and to Elbot in 2008.
On 23 June 2012, Goostman won a Turing test competition at Bletchley Park in Milton Keynes, held to mark the centenary of its namesake, Alan Turing.
The competition, which featured five bots, twenty-five hidden humans, and thirty judges, was considered to be the largest-ever Turing test contest by its organizers.
After a series of five-minute-long text conversations, 29% of the judges were convinced that the bot was an actual human.
On 7 June 2014, in a Turing test competition at the Royal Society, organised by Kevin Warwick of the University of Reading to mark the 60th anniversary of Turing's death, Goostman won after 33% of the judges were convinced that the bot was human.
30 judges took part in the event, which included Lord Sharkey, a sponsor of Turing's posthumous pardon, artificial intelligence Professor Aaron Sloman, Fellow of the Royal Society Mark Pagel and Red Dwarf actor Robert Llewellyn.
Each judge partook in a textual conversation with each of the five bots; at the same time, they also conversed with a human.
In all, a total of 300 conversations were conducted.
In Warwick's view, this made Goostman the first machine to pass a Turing test.
In a press release, he added that:  Some will claim that the Test has already been passed.
The words Turing Test have been applied to similar competitions around the world.
However this event involved more simultaneous comparison tests than ever before, was independently verified and, crucially, the conversations were unrestricted.
A true Turing Test does not set the questions or topics prior to the conversations.
In his 1950 paper "Computing Machinery and Intelligence", Turing predicted that by the year 2000, computer programs would be sufficiently advanced that the average interrogator would, after five minutes of questioning, "not have more than 70 per cent chance" of correctly guessing whether they were speaking to a human or a machine.
Although Turing phrased this as a prediction rather than a "threshold for intelligence", commentators believe that Warwick had chosen to interpret it as meaning that if 30% of interrogators were fooled, the software had "passed the Turing test".
AI for Good is a movement in which institutions are employing AI to tackle some of the world's greatest economic and social challenges.
For example, the University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address socially relevant problems such as homelessness.
At Stanford, researchers are using AI to analyze satellite images to identify which areas have the highest poverty levels.
In agriculture new AI advancements show improvements in gaining yield and to increase the research and development of growing crops.
New artificial intelligence now predicts the time it takes for a crop like a tomato to be ripe and ready for picking thus increasing efficiency of farming.
These advances go on including Crop and Soil Monitoring, Agricultural Robots, and Predictive Analytics.
Crop and soil monitoring uses new algorithms and data collected on the field to manage and track the health of crops making it easier and more sustainable for the farmers.
More specializations of Ai in agriculture is one such as greenhouse automation, simulation, modeling, and optimization techniques.
Due to the increase in population and the growth of demand for food in the future there will need to be at least a 70% increase in yield from agriculture to sustain this new demand.
More and more of the public perceives that the adaption of these new techniques and the use of Artificial intelligence will help reach that goal.
The Air Operations Division (AOD) uses AI for the rule based expert systems.
The AOD has use for artificial intelligence for surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.
The use of artificial intelligence in simulators is proving to be very useful for the AOD.
Airplane simulators are using artificial intelligence in order to process the data taken from simulated flights.
Other than simulated flying, there is also simulated aircraft warfare.
The computers are able to come up with the best success scenarios in these situations.
The computers can also create strategies based on the placement, size, speed and strength of the forces and counter forces.
Pilots may be given assistance in the air during combat by computers.
The artificial intelligent programs can sort the information and provide the pilot with the best possible maneuvers, not to mention getting rid of certain maneuvers that would be impossible for a human being to perform.
Multiple aircraft are needed to get good approximations for some calculations so computer simulated pilots are used to gather data.
These computer simulated pilots are also used to train future air traffic controllers.
The system used by the AOD in order to measure performance was the Interactive Fault Diagnosis and Isolation System, or IFDIS.
It is a rule based expert system put together by collecting information from TF-30 documents and the expert advice from mechanics that work on the TF-30.
This system was designed to be used for the development of the TF-30 for the RAAF F-111C.
The performance system was also used to replace specialized workers.
The system allowed the regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.
The AOD also uses artificial intelligence in speech recognition software.
The air traffic controllers are giving directions to the artificial pilots and the AOD wants to the pilots to respond to the ATC's with simple responses.
The programs that incorporate the speech software must be trained, which means they use neural networks.
The program used, the Verbex 7000, is still a very early program that has plenty of room for improvement.
The improvements are imperative because ATCs use very specific dialog and the software needs to be able to communicate correctly and promptly every time.
The Artificial Intelligence supported Design of Aircraft, or AIDA, is used to help designers in the process of creating conceptual designs of aircraft.
This program allows the designers to focus more on the design itself and less on the design process.
The software also allows the user to focus less on the software tools.
The AIDA uses rule based systems to compute its data.
This is a diagram of the arrangement of the AIDA modules.
Although simple, the program is proving effective.
In 2003, NASA's Dryden Flight Research Center, and many other companies, created software that could enable a damaged aircraft to continue flight until a safe landing zone can be reached.
The software compensates for all the damaged components by relying on the undamaged components.
The neural network used in the software proved to be effective and marked a triumph for artificial intelligence.
The Integrated Vehicle Health Management system, also used by NASA, on board an aircraft must process and interpret data taken from the various sensors on the aircraft.
The system needs to be able to determine the structural integrity of the aircraft.
The system also needs to implement protocols in case of any damage taken the vehicle.
Haitham Baomar and Peter Bentley are leading a team from the University College of London to develop an artificial intelligence based Intelligent Autopilot System (IAS) designed to teach an autopilot system to behave like a highly experienced pilot who is faced with an emergency situation such as severe weather, turbulence, or system failure.
Educating the autopilot relies on the concept of supervised machine learning “which treats the young autopilot as a human apprentice going to a flying school”.
The autopilot records the actions of the human pilot generating learning models using artificial neural networks.
The autopilot is then given full control and observed by the pilot as it executes the training exercise.
The Intelligent Autopilot System combines the principles of Apprenticeship Learning and Behavioural Cloning whereby the autopilot observes the low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.
IAS implementation employs three phases; pilot data collection, training, and autonomous control.
Baomar and Bentley's goal is to create a more autonomous autopilot to assist pilots in responding to emergency situations.
AI researchers have created many tools to solve the most difficult problems in computer science.
Many of their inventions have been adopted by mainstream computer science and are no longer considered a part of AI.
(See AI effect.)
According to Russell & Norvig (2003, p. 15), all of the following were originally developed in AI laboratories: time sharing, interactive interpreters, graphical user interfaces and the computer mouse, Rapid application development environments, the linked list data structure, automatic storage management, symbolic programming, functional programming, dynamic programming and object-oriented programming.
AI can be used to potentially determine the developer of anonymous binaries.
AI can be used to create other AI.
For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and COCO.
According to Google, NASNet's performance exceeded all previously published ImageNet performance.
In June 2016, a research team from the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face, a program which animates the face of a target person, transposing the facial expressions of an exterior source.
The technology has been demonstrated animating the lips of people including Barack Obama and Vladimir Putin.
Since then, other methods have been demonstrated based on deep neural network, from which the name "deepfake" was taken.
Hollywood film studios had already used the technique in animated films, but it took time and efforts from professionals.
The main difference is that today anyone can use a deep fake software and rig videos.
In September 2018, the U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deepfake documents on their platform.
Vincent Nozick, a researcher from the Institut Gaspard Monge, found a way to detect rigged documents by analyzing the movements of the eyelid.
The DARPA (a research group associated with the U.S. Department of Defense) has given 68 million dollars to work on deepfake detection.
In Europe, the Horizon 2020 program financed InVid, software designed to help journalists to detect fake documents.
The Future of AI in the classroom One of the more promising innovations is the idea of a personal AI tutor or assistant for each individual student.
Because a single teacher can't work with every student at once, AI tutors would allow for students to get extra, one-on-one help in areas of needed growth.
AI tutors also eliminate the intimidating idea of tutor labs or human tutors which can cause anxiety and stress for some students.
In future classrooms, ambient informatics can play a beneficial role.
Ambient informatics is the idea that information is everywhere in the environment and that technologies automatically adjust to your personal preferences.
When students sit at their desk, their devices will be able to create lessons, problems, and games to tailor to the specific student's needs, particularly where a student may be struggling, and give immediate feedback.
This eliminates the idea of a “one-size-fits-all classroom” as we will no longer have to force students to learn exactly the same material at exactly the same pace.
While there are many benefits to the use of AI in the classroom, there are also several dangers that need to be taken into account before implementing them.
As far as the future of AI in education, there are many new possibilities due to what has been coined by The New York Times as “The Great AI Awakening.
” One of these possibilities mentioned by Forbes included the providing of adaptive learning programs, which assess and react to a student's emotions and learning preferences.
Another advancement includes the presentation of performance data and enrichment methods on an individual basis.
Within curriculum, AI could help determine if there are underlying biases in texts and instructions.
For teachers, AI could soon have the power to relay data regarding efficacy of varying learning interventions from a, potentially, global database.
As a whole AI has the power to influence education by taking district, state, national, and global data into consideration as it seeks to better individualize learning for all.
Although AI can provide many assets to a classroom, many experts still agree that they will not be able to replace teachers altogether.
Many teachers fear the idea of AI replacing them in the classroom, especially with the idea of personal AI assistants for each student.
The reality is, AI can create a more dystopian environment with revenge effects.
This means that technology is inhibiting society from moving forward and causing negative, unintended effects on society.
An example of a revenge effect is that the extended use of technology may hinder students' ability to focus and stay on task instead of helping them learn and grow.
Also, AI has been known to lead to the loss of both human agency and simultaneity.
If students are relying solely on AI tutors composed of algorithms and wires, it reduces their ability to control their own education and learning.
Also, the need for AI technologies to work simultaneously may lead to system failures which could ruin an entire school day if we are relying on AI assistants to create lessons for students every day.
It is inevitable that AI technologies will be taking over the classroom in the years to come, thus it is essential that the kinks of these new innovations are worked out before teachers decide whether or not to implement them into their daily schedules.
Algorithmic trading involves the use of complex AI systems to make trading decisions at speeds several orders of magnitudes greater than any human is capable of, often making millions of trades in a day without any human intervention.
Such trading is called High-frequency Trading, and it represents one of the fastest growing sectors in financial trading.
Many banks, funds, and proprietary trading firms now have entire portfolios which are managed purely by AI systems.
Automated trading systems are typically used by large institutional investors, but recent years have also seen an influx of smaller, proprietary firms trading with their own AI systems.
Several large financial institutions have invested in AI engines to assist with their investment practices.
BlackRock's AI engine, Aladdin, is used both within the company and to clients to help with investment decisions.
Its wide range of functionalities includes the use of natural language processing to read text such as news, broker reports, and social media feeds.
It then gauges the sentiment on the companies mentioned and assigns a score.
Banks such as UBS and Deutsche Bank use an AI engine called Sqreem (Sequential Quantum Reduction and Extraction Model) which can mine data to develop consumer profiles and match them with the wealth management products they'd most likely want.
Goldman Sachs uses Kensho, a market analytics platform that combines statistical computing with big data and natural language processing.
Its machine learning systems mine through hoards of data on the web and assess correlations between world events and their impact on asset prices.
Information Extraction, part of artificial intelligence, is used to extract information from live news feed and to assist with investment decisions.
Several products are emerging that utilize AI to assist people with their personal finances.
For example, Digit is an app powered by artificial intelligence that automatically helps consumers optimize their spending and savings based on their own personal habits and goals.
The app can analyze factors such as monthly income, current balance, and spending habits, then make its own decisions and transfer money to the savings account.
Wallet.
AI, an upcoming startup in San Francisco, builds agents that analyze data that a consumer would leave behind, from Smartphone check-ins to tweets, to inform the consumer about their spending behavior.
Robo-advisors are becoming more widely used in the investment management industry.
Robo-advisors provide financial advice and portfolio management with minimal human intervention.
This class of financial advisers work based on algorithms built to automatically develop a financial portfolio according to the investment goals and risk tolerance of the clients.
It can adjust to real-time changes in the market and accordingly calibrate the portfolio.
An online lender, Upstart, analyze vast amounts of consumer data and utilizes machine learning algorithms to develop credit risk models that predict a consumer's likelihood of default.
Their technology will be licensed to banks for them to leverage for their underwriting processes as well.
ZestFinance developed their Zest Automated Machine Learning (ZAML) Platform specifically for credit underwriting as well.
This platform utilizes machine learning to analyze tens of thousands traditional and nontraditional variables (from purchase transactions to how a customer fills out a form) used in the credit industry to score borrowers.
The platform is particularly useful to assign credit scores to those with limited credit histories, such as millennials.
The 1980s is really when AI started to become prominent in the finance world.
This is when expert systems became more of a commercial product in the financial field.
For example, Dupont had built 100 expert systems which helped them save close to $10 million a year.
” One of the first systems was the Protrader expert system designed by K.C. Chen and Ting-peng Lian that was able to predict the 87-point drop in DOW Jones Industrial Average in 1986.
The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.
One of the first expert systems that helped with financial plans was created by Applied Expert Systems (APEX) called the PlanPower.
It was first commercially shipped in 1986.
Its function was to help give financial plans for people with incomes over $75,000 a year.
That then led to the Client Profiling System that was used for incomes between $25,000 and $200,000 a year.
The 1990s was a lot more about fraud detection.
One of the systems that was started in 1993 was the FinCEN Artificial Intelligence system (FAIS).
It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering which would have been equal to $1 billion.
Although expert systems did not last in the finance world, it did help jump-start the use of AI and help make it what it is today.
Robots have become common in many industries and are often given jobs that are considered dangerous to humans.
Robots have proven effective in jobs that are very repetitive which may lead to mistakes or accidents due to a lapse in concentration and other jobs which humans may find degrading.
In 2014, China, Japan, the United States, the Republic of Korea and Germany together amounted to 70% of the total sales volume of robots.
In the automotive industry, a sector with particularly high degree of automation, Japan had the highest density of industrial robots in the world: 1,414 per 10,000 employees.
Artificial neural networks are used as clinical decision support systems for medical diagnosis, such as in Concept Processing technology in EMR software.
Other tasks in medicine that can potentially be performed by artificial intelligence and are beginning to be developed include:  Computer-aided interpretation of medical images.
Such systems help scan digital images, e.g. from computed tomography, for typical appearances and to highlight conspicuous sections, such as possible diseases.
A typical application is the detection of a tumor.
Heart sound analysis Companion robots for the care of the elderly Mining medical records to provide more useful information.
Design treatment plans.
Assist in repetitive jobs including medication management.
Provide consultations.
Drug creation Using avatars in place of patients for clinical training Predict the likelihood of death from surgical procedures Predict HIV progressionThere are over 90 AI startups in the health industry working in these fields.
IDx's first solution, IDx-DR, is the first autonomous AI-based diagnostic system authorized for commercialization by the FDA.
Another application of AI is in the human resources and recruiting space.
There are three ways AI is being used by human resources and recruiting professionals: to screen resumes and rank candidates according to their level of qualification, to predict candidate success in given roles through job matching platforms, and rolling out recruiting chat bots that can automate repetitive communication tasks.
Typically, resume screening involves a recruiter or other HR professional scanning through a database of resumes.
The job market has seen a notable change due to artificial intelligence implementation.
It has simplified the process for both recruiters and job seekers (i.e., Google for Jobs and applying online).
According to Raj Mukherjee from Indeed.com, 65% of people launch a job search again within 91 days of being hired.
AI-powered engine streamlines the complexity of job hunting by operating information on job skills, salaries, and user tendencies, matching people to the most relevant positions.
Machine intelligence calculates what wages would be appropriate for a particular job, pulls and highlights resume information for recruiters using natural language processing, which extracts relevant words and phrases from text using specialized software.
Another application is an AI resume builder which requires 5 minutes to compile a CV as opposed to spending hours doing the same job.
In the AI age chatbots assist website visitors and solve daily workflows.
Revolutionary AI tools complement people's skills and allow HR managers to focus on tasks of higher priority.
However, Artificial Intelligence impact on jobs research suggests that by 2030 intelligent agents and robots can eliminate 30% of the world's human labor.
Moreover, the research proves automation will displace between 400 and 800 million employees.
Glassdoor's research report states that recruiting and HR are expected to see much broader adoption of AI in job market 2018 and beyond.
Some AI applications are geared towards the analysis of audiovisual media content such as movies, TV programs, advertisement videos or user-generated content.
The solutions often involve computer vision, which is a major application area of AI.
Typical use case scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for recognizing relevant scenes, objects or faces.
The motivation for using AI-based media analysis can be — among other things
— the facilitation of media search, the creation of a set of descriptive keywords for a media item, media content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for the placement of relevant advertisements.
Media analysis AI companies often provide their services over a REST API that enables machine-based automatic access to the technology and allows machine-reading of the results.
For example, IBM, Microsoft, Amazon and the video AI company Valossa allow access to their media recognition technology by using RESTful APIs.
AI is also widely used in E-commerce applications like visual search, chatbots, and automated product tagging.
Another generic application is to increase search discoverability and making social media content shoppable.
The main military applications of Artificial Intelligence and Machine Learning are to enhance Command and Control, Communications, Sensors, Integration and Interoperability.
Artificial Intelligence technologies enables coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Join Fires between networked combat vehicles and tanks also
inside  Manned  and
Unmanned Teams (MUM-T).
While the evolution of music has always been affected by technology, artificial intelligence has enabled, through scientific advances, to emulate, at some extent, human-like composition.
Among notable early efforts, David Cope created an AI called Emily Howell that managed to become well known in the field of Algorithmic Computer Music.
The algorithm behind Emily Howell is registered as a US patent.
The AI Iamus created 2012 the first complete classical album fully composed by a computer.
Other endeavours, like AIVA (Artificial Intelligence Virtual Artist), focus on composing symphonic music, mainly classical music for film scores.
It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.
Artificial intelligences can even produce music usable in a medical setting, with Melomics's effort to use computer-generated music for stress and pain relief.
Moreover, initiatives such as Google Magenta, conducted by the Google Brain team, want to find out if an artificial intelligence can be capable of creating compelling art.
At Sony CSL Research Laboratory, their Flow Machines software has created pop songs by learning music styles from a huge database of songs.
By analyzing unique combinations of styles and optimizing techniques, it can compose in any style.
Another artificial intelligence musical composition project,
The Watson Beat, written by IBM Research, doesn't need a huge database of music like the Google Magenta and Flow Machines projects, since it uses Reinforcement Learning and Deep Belief Networks to compose music on a simple seed input melody and a select style.
Since the software has been open sourced musicians, such as Taryn Southern have been collaborating with the project to create music.
The company Narrative Science makes computer-generated news and reports commercially available, including summarizing team sporting events based on statistical data from the game in English.
It also creates financial reports and real estate analyses.
Similarly, the company Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.
The company is projected to generate one billion stories in 2014, up from 350 million in 2013.
The organisation OpenAI has also created an AI capable of writing text.
Echobox is a software company that helps publishers increase traffic by 'intelligently' posting articles on social media platforms such as Facebook and Twitter.
By analysing large amounts of data, it learns how specific audiences respond to different articles at different times of the day.
It then chooses the best stories to post and the best times to post them.
It uses both historical and real-time data to understand to what has worked well in the past as well as what is currently trending on the web.
Another company, called Yseop, uses artificial intelligence to turn structured data into intelligent comments and recommendations in natural language.
Yseop is able to write financial reports, executive summaries, personalized sales or marketing documents and more at a speed of thousands of pages per second and in multiple languages including English, Spanish, French & German.
Boomtrain's is another example of AI that is designed to learn how to best engage each individual reader with the exact articles—sent through the right channel at the right time—that will be most relevant to the reader.
It's like hiring a personal editor for each individual reader to curate the perfect reading experience.
IRIS.TV is helping media companies with its AI-powered video personalization and programming platform.
It allows publishers and content owners to surface contextually relevant content to audiences based on consumer viewing patterns.
Beyond automation of writing tasks given data input, AI has shown significant potential for computers to engage in higher-level creative work.
AI Storytelling has been an active field of research since James Meehan's development of TALESPIN, which made up stories similar to the fables of Aesop.
The program would start with a set of characters who wanted to achieve certain goals, with the story as a narration of the characters' attempts at executing plans to satisfy these goals.
Since Meehan, other researchers have worked on AI Storytelling using similar or different approaches.
Mark Riedl and Vadim Bulitko argued that the essence of storytelling was an experience management problem, or "how to balance the need for a coherent story progression with user agency, which are often at odds."While most research on AI storytelling has focused on story generation (e.g. character and plot), there has also been significant investigation in story communication.
In 2002, researchers at North Carolina State University developed an architectural framework for narrative prose generation.
Their particular implementation was able faithfully reproduced text variety and complexity of a number of stories, such as red riding hood, with human-like adroitness.
This particular field continues to gain interest.
In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.
Artificial intelligence is implemented in automated online assistants that can be seen as avatars on web pages.
It can avail for enterprises to reduce their operation and training cost.
A major underlying technology to such systems is natural language processing.
Pypestream uses automated customer service for its mobile application designed to streamline communication with customers.
Major companies are investing in AI to handle difficult customer in the future.
Google's most recent development analyzes language and converts speech into text.
The platform can identify angry customers through their language and respond appropriately.
Power electronics converters are an enabling technology for renewable energy, energy storage, electric vehicles and high-voltage direct current transmission systems within the electrical grid.
These converters are prone to failures and such failures can cause downtimes that may require costly maintenance or even have catastrophic consequences in mission critical applications.
Researchers are using AI to do the automated design process for reliable power electronics converters, by calculating exact design parameters that ensure desired lifetime of the converter under specified mission profile.
Artificial Intelligence has been combined with many sensor technologies, such as Digital Spectrometry by IdeaCuria Inc. which enables many applications such as at home water quality monitoring.
Many telecommunications companies make use of heuristic search in the management of their workforces, for example
BT Group has deployed heuristic search in a scheduling application that provides the work schedules of 20,000 engineers.
The 1990s saw some of the first attempts to mass-produce domestically aimed types of basic Artificial Intelligence for education, or leisure.
This prospered greatly with the Digital Revolution, and helped introduce people, especially children, to a life of dealing with various types of Artificial Intelligence, specifically in the form of Tamagotchis and Giga Pets, iPod Touch, the Internet, and the first widely released robot, Furby.
A mere year later an improved type of domestic robot was released in the form of Aibo, a robotic dog with intelligent features and autonomy.
Companies like Mattel have been creating an assortment of AI-enabled toys for kids as young as age three.
Using proprietary AI engines and speech recognition tools, they are able to understand conversations, give intelligent responses and learn quickly.
AI has also been applied to video games, for example video game bots, which are designed to stand in as opponents where humans aren't available or desired.
Fuzzy logic controllers have been developed for automatic gearboxes in automobiles.
For example, the 2006 Audi TT, VW Touareg and VW Caravell feature the DSP transmission which utilizes Fuzzy Logic.
A number of Škoda variants (Škoda Fabia) also currently include a Fuzzy Logic-based controller.
Today's cars now have AI-based driver assist features such as self-parking and advanced cruise controls.
AI has been used to optimize traffic management applications, which in turn reduces wait times, energy use, and emissions by as much as 25 percent.
In the future, fully autonomous cars will be developed.
AI in transportation is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities.
The major challenge to developing this AI is the fact that transportation systems are inherently complex systems involving a very large number of components and different parties, each having different and often conflicting objectives.
Due to this high degree of complexity of the transportation, and in particular the automotive, application, it is in most cases not possible to train an AI algorithm in a real-world driving environment.
To overcome the challenge of training neural networks for automated driving, methodologies based on virtual development resp.
testing toolchains have been proposed.
Studies related to Wikipedia has been using artificial intelligence to support various operations.
One of the most important areas - automatic detection of vandalism  and data quality assessment in Wikipedia.
The team at the Wikimedia Foundation released a model that is designed to detect vandalism, spam, and personal attack.
This model can also help students write better Wikipedia articles.
Typical problems to which AI methods are applied Other fields in which AI methods are implemented
In his famous Turing test, Alan Turing picked language, the defining feature of human beings, for its basis.
Yet, there are many other useful abilities that can be described as showing some form of intelligence.
This gives better insight into the comparative success of artificial intelligence in different areas.
In what has been called the Feigenbaum test, the inventor of expert systems argued for subject specific expert tests.
A paper by Jim Gray of Microsoft in 2003 suggested extending the Turing test to speech understanding, speaking and recognizing objects and behavior.
AI, like electricity or the steam engine, is a general purpose technology.
There is no consensus on how to characterize which tasks AI tends to excel at.
Some versions of Moravec's paradox observe that humans are more likely to outperform machines in areas such as physical dexterity that have been the direct target of natural selection.
While projects such as AlphaZero have succeeded in generating their own knowledge from scratch, many other machine learning projects require large training datasets.
Researcher Andrew Ng has suggested, as a "highly imperfect rule of thumb", that "almost anything a typical human can do with less than one second of mental thought, we can probably now or in the near future automate using AI."Games provide a high-profile benchmark for assessing rates of progress; many games have a large professional player base and a well-established competitive rating system.
AlphaGo brought the era of classical board-game benchmarks to a close.
Games of imperfect knowledge provide new challenges to AI in the area of game theory; the most prominent milestone in this area was brought to a close by Libratus' poker victory in 2017.
E-sports continue to provide additional benchmarks; Facebook AI, Deepmind, and others have engaged with the popular StarCraft franchise of videogames.
Broad classes of outcome for an AI test may be given as:  optimal: it is not possible to perform better (note: some of these entries were solved by humans) super-human: performs better than all humans high-human: performs better than most humans par-human: performs similarly to most humans sub-human: performs worse than most humans
Tic-Tac-Toe Connect Four:
1988 Checkers (aka 8x8 draughts):
Weakly solved (2007)
Rubik's Cube: Mostly solved (2010)
Heads-up limit hold'em poker: Statistically optimal in the sense that "a human lifetime of play is not sufficient to establish with statistical significance that the strategy is not an exact solution" (2015)
Othello (aka reversi):
c. 1997
Scrabble: 2006
Backgammon:
c. 1995-2002
Chess: Supercomputer (c. 1997)
; Personal computer (c. 2006); Mobile phone (c. 2009); Computer defeats human + computer (c. 2017)  Jeopardy!:
Question answering, although the machine did not use speech recognition (2011)
Shogi:
c. 2017
Arimaa: 2015
Go:
2017 Heads-up
no-limit hold'em poker: 2017
Crosswords:
c. 2012 Dota 2: 2018 Bridge card-playing:
According to a 2009 review, "the best programs are attaining expert status as (bridge) card players", excluding bidding.
StarCraft II: 2019
Optical character recognition for ISO 1073-1:1976 and similar special characters.
Classification of images Handwriting recognition
Optical character recognition for printed text (nearing par-human for Latin-script typewritten text)
Object recognition Facial recognition:
Low to mid human accuracy (as of 2014) Visual question answering, such as the VQA 1.0 Various robotics tasks that may require advances in robot hardware as well as AI, including: Stable bipedal locomotion: Bipedal robots can walk, but are less stable than human walkers (as of 2017) Humanoid soccer Speech recognition: "nearly equal to human performance" (2017) Explainability.
Current medical systems can diagnose certain medical conditions well, but cannot explain to users why they made the diagnosis.
Stock market prediction: Financial data collection and processing using Machine Learning algorithms Various tasks that are difficult to solve without contextual knowledge, including: Translation Word-sense disambiguation Natural language processing
An expert poll around 2016, conducted by Katja Grace of the Future of Humanity Institute and associates, gave median estimates of 3 years for championship Angry Birds, 4 years for the World Series of Poker, and 6 years for StarCraft.
On more subjective tasks, the poll gave 6 years for folding laundry as well as an average human worker, 7–10 years for expertly answering 'easily Googleable' questions, 8 years for average speech transcription, 9 years for average telephone banking, and 11 years for expert songwriting, but over 30 years for writing a New York Times bestseller or winning the Putnam math competition.
An AI defeated a grandmaster in a regulation tournament game for the first time in 1988; rebranded as Deep Blue, it beat the reigning human world chess champion in 1997 (see Deep Blue versus Garry Kasparov).
AlphaGo defeated a European Go champion in October 2015, and Lee Sedol in March 2016, one of the world's top players (see AlphaGo versus Lee Sedol).
According to Scientific American and other sources, most observers had expected superhuman Computer Go performance to be at least a decade away.
AI pioneer and economist Herbert A. Simon inaccurately predicted in 1965:
"Machines will be capable, within twenty years, of doing any work a man can do".
Similarly, in 1970 Marvin Minsky wrote that "Within a generation... the problem of creating artificial intelligence will substantially be
solved."Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when AGI would arrive was 2040 to 2050, depending on the poll.
The Grace poll around 2016 found results varied depending on how the question was framed.
Respondents asked to estimate "when unaided machines can accomplish every task better and more cheaply than human workers" gave an aggregated median answer of 45 years and a 10% chance of it occurring within 9 years.
Other respondents asked to estimate "when all occupations are fully automatable.
That is, when for any occupation, machines could be built to carry out the task better and more cheaply than human workers" estimated a median of 122 years and a 10% probability of 20 years.
The median response for when "AI researcher" could be fully automated was around 90 years.
No link was found between seniority and optimism, but Asian researchers were much more optimistic than North American researchers on average; Asians predicted 30 years on average for "accomplish every task", compared with the 74 years predicted by North Americans.
Blue Brain Project, an attempt to create a synthetic brain by reverse-engineering the mammalian brain down to the molecular level.
Google Brain
A deep learning project part of Google X attempting to have intelligence similar or equal to human-level.
Human Brain Project NuPIC, an open source implementation by Numenta of its cortical learning algorithm.
4CAPS, developed at Carnegie Mellon University under Marcel A.
Just ACT-R, developed at Carnegie Mellon University under John R. Anderson.
AIXI,
Universal Artificial Intelligence developed by Marcus Hutter at IDSIA and ANU.
CALO, a DARPA-funded, 25-institution effort to integrate many artificial intelligence approaches (natural language processing, speech recognition, machine vision, probabilistic logic, planning, reasoning, many forms of machine learning) into an AI assistant that learns to help manage your office environment. CHREST, developed under Fernand Gobet at Brunel University and Peter C. Lane at the University of Hertfordshire.
CLARION, developed under Ron Sun at Rensselaer Polytechnic Institute and University of Missouri.
CoJACK, an ACT-R inspired extension to the JACK multi-agent system that adds a cognitive architecture to the agents for eliciting more realistic (human-like) behaviors in virtual environments. Copycat, by Douglas Hofstadter and Melanie Mitchell at the Indiana University. DUAL, developed at the New Bulgarian University under Boicho Kokinov.
FORR developed by Susan L. Epstein at The City University of New York. IDA and LIDA, implementing Global Workspace Theory, developed under Stan Franklin at the University of Memphis.
OpenCog Prime, developed using the OpenCog Framework.
Procedural Reasoning System (PRS), developed by Michael Georgeff and Amy L. Lansky at SRI International.
Psi-Theory developed under Dietrich Dörner at the Otto-Friedrich University in Bamberg, Germany.
R-CAST, developed at the Pennsylvania State University.
Soar, developed under Allen Newell and John Laird at Carnegie Mellon University and the University of Michigan.
Society of mind and its successor the Emotion machine proposed by Marvin Minsky.
Subsumption architectures, developed e.g. by Rodney Brooks (though it could be argued whether they are cognitive).
AlphaGo, software developed by Google that plays the Chinese board game Go.
Chinook, a computer program that plays English draughts; the first to win the world champion title in the competition against humans.
Deep Blue, a chess-playing computer developed by IBM which beat Garry Kasparov in 1997.
FreeHAL, a self-learning conversation simulator (chatterbot) which uses semantic nets to organize its knowledge to imitate a very close human behavior within conversations.
Halite, an artificial intelligence programming competition created by Two Sigma.
Libratus, a poker AI that beat world-class poker players in 2017, intended to be generalisable to other applications.
Quick, Draw!, an online game developed by Google that challenges players to draw a picture of an object or idea and then uses a neural network to guess what the drawing is.
Stockfish AI, an open source chess engine currently ranked the highest in many computer chess rankings.
TD-Gammon, a program that learned to play world-class backgammon partly by playing against itself (temporal difference learning with neural networks).
Serenata de Amor, project for the analysis of public expenditures and detect discrepancies.
Braina, an intelligent personal assistant application with a voice interface for Windows OS.
Cyc, an attempt to assemble an ontology and database of everyday knowledge, enabling human-like reasoning.
Eurisko, a language by Douglas Lenat for solving problems which consists of heuristics, including some for how to use and change its heuristics.
Google Now, an intelligent personal assistant with a voice interface in Google's Android and Apple Inc.'s iOS, as well as Google Chrome web browser on personal computers.
Holmes a new AI created by Wipro.
Microsoft Cortana, an intelligent personal assistant with a voice interface in Microsoft's various Windows 10 editions.
Mycin, an early medical expert system.
Open Mind Common Sense, a project based at the MIT Media Lab to build a large common sense knowledge base from online contributions.
P.A.N., a publicly available text analyzer.
Siri, an intelligent personal assistant and knowledge navigator with a voice-interface in Apple Inc.'s iOS and macOS.
SNePS, simultaneously a logic-based, frame-based, and network-based knowledge representation, reasoning, and acting system.
Viv (software), a new AI by the creators of Siri.
Wolfram Alpha, an online service that answers queries by computing the answer from structured data.
AIBO, the robot pet for the home, grew out of Sony's Computer Science Laboratory (CSL).
Cog, a robot developed by MIT to study theories of cognitive science and artificial intelligence, now discontinued.
Melomics, a bioinspired technology for music composition and synthesization of music, where computers develop their own style, rather than mimic musicians.
AIML, an XML dialect for creating natural language software agents.
Apache Lucene, a high-performance, full-featured text search engine library written entirely in Java.
Apache OpenNLP, a machine learning based toolkit for the processing of natural language text.
It supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking and parsing.
Artificial Linguistic Internet Computer Entity (A.L.I.C.E.), an award-winning natural language processing chatterbot.
Cleverbot, successor to Jabberwacky, now with 170m lines of conversation, Deep Context, fuzziness and parallel processing.
Cleverbot learns from around 2 million user interactions per month.
ELIZA, a famous 1966 computer program by Joseph Weizenbaum, which parodied person-centered therapy.
Jabberwacky, a chatterbot by Rollo Carpenter, aiming to simulate natural human chat.
Mycroft, a free and open-source intelligent personal assistant that uses a natural language user interface.
PARRY, another early chatterbot, written in 1972 by Kenneth Colby, attempting to simulate a paranoid schizophrenic.
SHRDLU, an early natural language processing computer program developed by Terry Winograd at MIT from 1968 to 1970. SYSTRAN, a machine translation technology by the company of the same name, used by Yahoo!, AltaVista and Google, among others.
1 the Road, the first novel marketed by an AI.
Synthetic Environment for Analysis and Simulations (SEAS), a model of the real world used by Homeland security and the United States Department of Defense that uses simulation and AI to predict and evaluate future events and courses of action.
Apache Mahout, a library of scalable machine learning algorithms.
Deeplearning4j, an open-source, distributed deep learning framework written for the JVM.
Keras, a high level open-source software library for machine learning (works on top of other libraries).
Microsoft Cognitive Toolkit (previously known as CNTK), an open source toolkit for building artificial neural networks. OpenNN, a comprehensive C++ library implementing neural networks.
PyTorch, an open-source Tensor and Dynamic neural network in Python.
TensorFlow, an open-source software library for machine learning.
Theano, a Python library and optimizing compiler for manipulating and evaluating mathematical expressions, especially matrix-valued ones.
Neural Designer, a commercial deep learning tool for predictive analytics. Neuroph, a Java neural network framework. OpenCog, a GPL-licensed framework for artificial intelligence written in C++, Python and Scheme.
RapidMiner, an environment for machine learning and data mining, now developed commercially.
Weka, a free implementation of many machine learning algorithms in Java.
Data Applied, a web based data mining environment.
Grok, a service that ingests data streams and creates actionable predictions in real time.
Watson, a pilot service by IBM to uncover and share data-driven insights, and to spur cognitive applications.
The name machine learning was coined in 1959 by Arthur Samuel.
Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure
P if its performance at tasks in T, as measured by P,  improves with experience E."
This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms.
This follows Alan Turing's proposal in his paper "Computing Machinery and Intelligence", in which the question "Can machines think?
" is replaced with the question
"Can machines do what we (as thinking entities) can do?".
In Turing's proposal the various characteristics that could be possessed by a thinking machine and the various implications in constructing one are exposed.
Machine learning tasks are classified into several broad categories.
In supervised learning, the algorithm builds a mathematical model from a set of data that contains both the inputs and the desired outputs.
For example, if the task were determining whether an image contained a certain object, the training data for a supervised learning algorithm would include images with and without that object (the input), and each image would have a label (the output) designating whether it contained the object.
In special cases, the input may be only partially available, or restricted to special feedback.
Semi-supervised learning algorithms develop mathematical models from incomplete training data, where a portion of the sample input doesn't have labels.
Classification algorithms and regression algorithms are types of supervised learning.
Classification algorithms are used when the outputs are restricted to a limited set of values.
For a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email.
For an algorithm that identifies spam emails, the output would be the prediction of either "spam" or "not spam", represented by the Boolean values true and false.
Regression algorithms are named for their continuous outputs, meaning they may have any value within a range.
Examples of a continuous value are the temperature, length, or price of an object.
In unsupervised learning, the algorithm builds a mathematical model from a set of data which contains only inputs and no desired output labels.
Unsupervised learning algorithms are used to find structure in the data, like grouping or clustering of data points.
Unsupervised learning can discover patterns in the data, and can group the inputs into categories, as in feature learning.
Dimensionality reduction is the process of reducing the number of "features", or inputs, in a set of data.
Active learning algorithms access the desired outputs (training labels) for a limited set of inputs based on a budget, and optimize the choice of inputs for which it will acquire training labels.
When used interactively, these can be presented to a human user for labeling.
Reinforcement learning algorithms are given feedback in the form of positive or negative reinforcement in a dynamic environment, and are used in autonomous vehicles or in learning to play a game against a human opponent.
Other specialized algorithms in machine learning include topic modeling, where the computer program is given a set of natural language documents and finds other documents that cover similar topics.
Machine learning algorithms can be used to find the unobservable probability density function in density estimation problems.
Meta learning algorithms learn their own inductive bias based on previous experience.
In developmental robotics, robot learning algorithms generate their own sequences of learning experiences, also known as a curriculum, to cumulatively acquire new skills through self-guided exploration and social interaction with humans.
These robots use guidance mechanisms such as active learning, maturation, motor synergies, and imitation.
Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term "Machine Learning" in 1959 while at IBM.
A representative book of the machine learning research during 1960s was the Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.
The interest of machine learning related to pattern recognition continued during 1970s, as described in the book of Duda and Hart in 1973.
As a scientific endeavor, machine learning grew out of the quest for artificial intelligence.
Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data.
They attempted to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.
Probabilistic reasoning was also employed, especially in automated medical diagnosis.
However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning.
Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.
By 1980, expert systems had come to dominate AI, and statistics was out of favor.
Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.
Neural networks research had been abandoned by AI and computer science around the same time.
This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart and Hinton.
Their main success came in the mid-1980s with the reinvention of backpropagation.
Machine learning, reorganized as a separate field, started to flourish in the 1990s.
The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature.
It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.
It also benefited from the increasing availability of digitized information, and the ability to distribute it via the Internet.
Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases).
Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy.
Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge.
Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.
Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples.
Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples).
The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.
Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns.
According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.
He also suggested the term data science as a placeholder to call the overall field.
Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,
wherein "algorithmic model" means more or less the machine learning algorithms like Random forest.
Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.
A core objective of a learner is to generalize from its experience.
Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set.
The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.
The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory.
Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms.
Instead, probabilistic bounds on the performance are quite common.
The bias–variance decomposition is one way to quantify generalization error.
For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data.
If the hypothesis is less complex than the function, then the model has underfit the data.
If the complexity of the model is increased in response, then the training error decreases.
But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.
In addition to performance bounds, learning theorists study the time complexity and feasibility of learning.
In computational learning theory, a computation is considered feasible if it can be done in polynomial time.
There are two kinds of time complexity results.
Positive results show that a certain class of functions can be learned in polynomial time.
Negative results show that certain classes cannot be learned in polynomial time.
The types of machine learning algorithms differ in their approach, the type of data they input and output, and the type of task or problem that they are intended to solve.
Performing machine learning involves creating a model, which is trained on some training data and then can process additional data to make predictions.
Various types of models have been used and researched for machine learning systems.
Usually, machine learning models require a lot of data in order for them to perform well.
Usually, when training a machine learning model, one needs to collect a large, representative sample of data from a training set.
Data from the training set can be as varied as a corpus of text, a collection of images, and data collected from individual users of a service.
Overfitting is something to watch out for when training a machine learning model.
There are many applications for machine learning, including:  In 2006, the online movie company Netflix held the first "Netflix Prize" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%.
A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.
Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns ("everything is a recommendation") and they changed their recommendation engine accordingly.
In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis.
In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors' jobs would be lost in the next two decades to automated machine learning medical diagnostic software.
In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings, and that it may have revealed previously unrecognized influences among artists.
In 2019 Springer Nature published the first research book created using machine learning.
Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.
Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.
In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.
Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of investment.
Machine learning approaches in particular can suffer from different data biases.
A machine learning system trained on current customers only may not be able to predict the needs of new customer groups that are not represented in the training data.
When trained on man-made data, machine learning is likely to pick up the same constitutional and unconscious biases already present in society.
Language models learned from data have been shown to contain human-like biases.
Machine learning systems used for criminal risk assessment have been found to be biased against black people.
In 2015, Google photos would often tag black people as gorillas, and in 2018 this still was not well resolved, but Google reportedly was still using the workaround to remove all gorilla from the training data, and thus was not able to recognize real gorillas at all.
Similar issues with recognizing non-white people have been found in many other systems.
In 2016, Microsoft tested a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.
Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.
Concern for reducing bias in machine learning and propelling its use for human good is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that "There's nothing artificial about AI...
It's inspired by people, it's created by people, and—
most importantly—it impacts people.
It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.
Classification machine learning models can be validated by accuracy estimation techniques like the Holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set.
In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model.
In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.
In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively.
Similarly, investigators sometimes report the False Positive Rate (FPR) as well as the False Negative Rate (FNR).
However, these rates are ratios that fail to reveal their numerators and denominators.
The Total Operating Characteristic (TOC) is an effective method to express a model's diagnostic ability.
TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used Receiver Operating Characteristic (ROC) and ROC's associated Area Under the Curve (AUC).
Machine learning poses a host of ethical questions.
Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.
For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants.
Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.
Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.
Other forms of ethical challenges, not related to personal biases, are more seen in health care.
There are concerns among health care professionals that these systems might not be designed in the public's interest, but as income generating machines.
This is especially true in the United States where there is a perpetual ethical dilemma of improving health care, but also increasing profits.
For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes in.
There is huge potential for machine learning in health care to provide professionals a great tool to diagnose, medicate, and even plan recovery paths for patients, but this will not happen until the personal biases mentioned previously, and these "greed" biases are addressed.
Software suites containing a variety of machine learning algorithms include the following:
Journal of Machine Learning Research Machine Learning Nature Machine Intelligence Neural Computation
Conference on Neural Information Processing Systems International Conference on Machine Learning
Deep learning is a class of machine learning algorithms that uses multiple layers to progressively extract higher level features from the raw input.
For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.
Most modern deep learning models are based on artificial neural networks, specifically, Convolutional Neural Networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.
In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation.
In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face.
Importantly, a deep learning process can learn which features to optimally place in which level on its own.
(Of course, this does not completely eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.)The word "deep" in "deep learning" refers to the number of layers through which the data is transformed.
More precisely, deep learning systems have a substantial credit assignment path (CAP) depth.
The CAP is the chain of transformations from input to output.
CAPs describe potentially causal connections between input and output.
For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized).
For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.
No universally agreed upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2.
CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function.
Beyond that, more layers do not add to the function approximator ability of the network.
Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.
Deep learning architectures can be constructed with a greedy layer-by-layer method.
Deep learning helps to disentangle these abstractions and pick out which features improve performance.
For supervised learning tasks, deep learning methods eliminate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.
Deep learning algorithms can be applied to unsupervised learning tasks.
This is an important benefit because unlabeled data are more abundant than the labeled data.
Examples of deep structures that can be trained in an unsupervised manner are neural history compressors and deep belief networks.
Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.
The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.
In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.
Recent work also showed that universal approximation also holds for non-bounded activation functions such as the rectified linear unit.
The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow.
Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; If the width is smaller or equal to the input dimension, then deep neural network is not a universal approximator.
The probabilistic interpretation derives from the field of machine learning.
It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively.
More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.
The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks.
The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.
The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.
The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1965.
A 1971 paper described a deep network with 8 layers trained by the group method of data handling algorithm.
Other deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in 1980.
In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970, to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail.
While the algorithm worked, training required 3 days.
By 1991 such systems were used for recognizing isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model.
Weng et al. suggested that a human brain does not use a monolithic 3-D object model and in 1992 they published Cresceptron, a method for performing 3-D object recognition in cluttered scenes.
Because it directly used natural images, Cresceptron started the beginning of general-purpose visual learning for natural 3D worlds.
Cresceptron is a cascade of layers similar to Neocognitron.
But while Neocognitron required a human programmer to hand-merge features, Cresceptron learned an open number of features in each layer without supervision, where each feature is represented by a convolution kernel.
Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network.
Max pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization.
In 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained.
Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.
In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.
Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.
Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural network's (ANN) computational cost and a lack of understanding of how the brain wires its biological networks.
Both shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years.
These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.
Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models.
Additional difficulties were the lack of training data and limited computing power.
Most speech recognition researchers moved away from neural nets to pursue generative modeling.
An exception was at SRI International in the late 1990s.
Funded by the US government's NSA and DARPA, SRI studied deep neural networks in speech and speaker recognition.
The speaker recognition team led by Larry Heck achieved the first significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation.
While SRI experienced success with deep neural networks in speaker recognition, they were unsuccessful in demonstrating similar success in speech recognition.
The principle of elevating "raw" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms.
The raw features of speech, waveforms, later produced excellent larger-scale results.
Many aspects of speech recognition were taken over by a deep learning method called long short-term memory (LSTM), a recurrent neural network published by Hochreiter and Schmidhuber in 1997.
LSTM RNNs avoid the vanishing gradient problem and can learn "Very Deep Learning" tasks that require memories of events that happened thousands of discrete time steps before, which is important for speech.
In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks.
Later it was combined with connectionist temporal classification (CTC) in stacks of LSTM RNNs.
In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.
In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation.
The papers referred to learning for deep belief nets.
Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR).
Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.
Convolutional neural networks (CNNs) were superseded for ASR by CTC for LSTM.
but are more successful in computer vision.
The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.
Industrial applications of deep learning to large-scale speech recognition started around 2010.
The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical.
It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets.
However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.
The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.
Analysis around 2009-2010, contrasted the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition, eventually leading to pervasive and dominant use in that industry.
That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.
In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.
Advances in hardware have enabled renewed interest in deep learning.
In 2009, Nvidia was involved in what was called the “big bang” of deep learning, “as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs).
That year, Google Brain used Nvidia GPUs to create capable DNNs.
While there, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times.
In particular, GPUs are well-suited for the matrix/vector computations involved in machine learning.
GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days.
Further, specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models.
In 2012, a team led by George E. Dahl won the "Merck Molecular Activity Challenge" using multi-task deep neural networks to predict the biomolecular target of one drug.
In 2014, Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the "Tox21 Data Challenge" of NIH, FDA and NCATS.Significant additional impacts in image or object recognition were felt from 2011 to 2012.
Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, fast implementations of CNNs with max-pooling on GPUs in the style of Ciresan and colleagues were needed to progress on computer vision.
In 2011, this approach achieved for the first time superhuman performance in a visual pattern recognition contest.
Also in 2011, it won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest.
Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records.
In October 2012, a similar system by Krizhevsky et al. won the large-scale ImageNet competition by a significant margin over shallow machine learning methods.
In November 2012, Ciresan et al.'s
system also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic.
In 2013 and 2014, the error rate on the ImageNet task using deep learning was further reduced, following a similar trend in large-scale speech recognition.
The Wolfram Image Identification project publicized these improvements.
Image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.
Some researchers assess that the October 2012 ImageNet victory anchored the start of a "deep learning revolution" that has transformed the AI industry.
In March 2019, Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.
Artificial neural networks (ANNs) or connectionist systems
are computing systems inspired by the biological neural networks that constitute animal brains.
Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming.
For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the analytic results to identify cats in other images.
They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.
An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain).
Each connection (synapse) between neurons can transmit a signal to another neuron.
The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it.
Neurons may have state, generally represented by real numbers, typically between 0 and 1.
Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.
Typically, neurons are organized in layers.
Different layers may perform different kinds of transformations on their inputs.
Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.
The original goal of the neural network approach was to solve problems in the same way that a human brain would.
Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.
Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
As of 2017, neural networks typically have a few thousand to a few million units and millions of connections.
Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, playing "Go" ).
A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers.
The DNN finds the correct mathematical manipulation to turn the input into the output, whether it be a linear relationship or a non-linear relationship.
The network moves through the layers calculating the probability of each output.
For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed.
The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label.
Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name "deep" networks.
DNNs can model complex non-linear relationships.
DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.
The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.
Deep architectures include many variants of a few basic approaches.
Each architecture has found success in specific domains.
It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.
DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back.
At first, the DNN creates a map of virtual neurons and assigns random numerical values, or "weights", to connections between them.
The weights and inputs are multiplied and return an output between 0 and 1.
If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.
That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.
Recurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling.
Long short-term memory is particularly effective for this use.
Convolutional deep neural networks (CNNs) are used in computer vision.
CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).
Large-scale automatic speech recognition is the first and most convincing successful case of deep learning.
LSTM RNNs can learn "Very Deep Learning" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms.
LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.
The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT.
The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.
Its small size lets many configurations be tried.
More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models.
This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed.
The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.
The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003-2007, accelerated progress in eight major areas: Scale-up/out and accelerated DNN training and decoding Sequence discriminative training Feature processing by deep models with solid understanding of the underlying mechanisms Adaptation of DNNs and related deep models Multi-task and transfer learning by DNNs and related deep models CNNs and how to design them to best exploit domain knowledge of speech RNN and its rich LSTM variants Other types of deep models including tensor-based models and integrated deep generative/discriminative models.
All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.
A common evaluation set for image classification is the MNIST database data set.
MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples.
As with TIMIT, its small size lets users test multiple configurations.
A comprehensive list of results on this set is available.
Deep learning-based image recognition has become "superhuman", producing more accurate results than human contestants.
This first occurred in 2011.Deep learning-trained vehicles now interpret 360° camera views.
Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.
Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks.
DNNs have proven themselves capable, for example, of a) identifying the style period of a given painting, b) Neural Style Transfer - capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video, and c) generating striking imagery based on random visual input fields.
Neural networks have been used for implementing language models since the early 2000s.
LSTM helped to improve machine translation and language modeling.
Other key techniques in this field are negative sampling and word embedding.
Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space.
Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar.
A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN.
Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.
Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, Text classification and others.
Recent developments generalize word embedding to sentence embedding.
Google Translate (GT) uses a large end-to-end long short-term memory network.
Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system "learns from millions of examples.
It translates "whole sentences at a time, rather than pieces.
Google Translate supports over one hundred languages.
The network encodes the "semantics of the sentence rather than simply memorizing phrase-to-phrase translations".
GT uses English as an intermediate between most language pairs.
A large percentage of candidate drugs fail to win regulatory approval.
These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects.
Research has explored use of deep learning to predict the biomolecular targets, off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.
AtomNet is a deep learning system for structure-based rational drug design.
AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.
In 2019 generative neural networks were used to produce molecules that were validated experimentally all the way into mice , .
Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables.
The estimated value function was shown to have a natural interpretation as customer lifetime value.
Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music recommendations.
Multiview deep learning has been applied for learning user preferences from multiple domains.
The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.
An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.
In medical informatics, deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data.
Deep learning has also showed efficacy in healthcare.
Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement
Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and assimilated before a target segment can be created and used in ad serving by any ad server.
Deep learning has been used to interpret large, many-dimensioned advertising datasets.
Many data points are collected during the request/serve/click internet advertising cycle.
This information can form the basis of machine learning to improve ad selection.
Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization.
These applications include learning methods such as "Shrinkage Fields for Effective Image Restoration" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.
Deep learning is being successfully applied to financial fraud detection and anti-money laundering.
Deep anti-money laundering detection system can spot and recognize relationships and similarities between data and, further down the road, learn to detect anomalies or classify and predict specific events".
The solution leverages both supervised learning techniques, such as the classification of suspicious transactions, and unsupervised learning, e.g. anomaly detection.
The United States Department of Defense applied deep learning to train robots in new tasks through observation.
Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s.
These developmental theories were instantiated in computational models, making them predecessors of deep learning systems.
These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models.
Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers.
This process yields a self-organizing stack of transducers, well-tuned to their operating environment.
A 1995 description stated, "...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature."A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective.
On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.
Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.
In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.
Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported.
For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations.
Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels.
Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.
Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input.
In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.
Google Translate uses an LSTM to translate between more than 100 languages.
In 2015, Blippar demonstrated a mobile augmented reality application that uses deep learning to recognize objects in real time.
In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.
As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.
First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers.
Deep TAMER used deep learning to provide a robot the ability to learn new tasks through observation.
Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person.
The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as “good job” and “bad job.
Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science.
A main criticism concerns the lack of theory surrounding some methods.
Learning in the most common deep architectures is implemented using well-understood gradient descent.
However, the theory surrounding other algorithms, such as contrastive divergence is less clear.
(e.g., Does it converge?
If so, how fast?
What is it approximating?)
Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.
Others point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution.
Despite the power of deep learning methods, they still lack much of the functionality needed for realizing this goal entirely.
Research psychologist Gary Marcus noted:"Realistically, deep learning is only part of the larger challenge of building intelligent machines.
Such techniques lack ways of representing causal relationships (...
) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used.
The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning."As an alternative to this emphasis on the limits of deep learning, one author speculated that it might be possible to train a machine vision stack to perform the sophisticated task of discriminating between "old master" and amateur figure drawings, and hypothesized that such a sensitivity might represent the rudiments of a non-trivial machine empathy.
This same author proposed that this would be in line with anthropology, which identifies a concern with aesthetics as a key element of behavioral modernity.
In further reference to the idea that artistic sensitivity might inhere within relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's web site.
Some deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images and misclassifying minuscule perturbations of correctly classified images.
Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures.
These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events.
Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI).
As deep learning moves from the lab into the world, research and experience shows that artificial neural networks are vulnerable to hacks and deception.
By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize.
For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target.
Such a manipulation is termed an “adversarial attack.
In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points and thereby generate images that deceived it.
The modified images looked no different to human eyes.
Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.
One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it.
A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.
Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another.
In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.
ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry.
ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.
Another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address that would download malware.
In “data poisoning,” false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.
Policy issues in legal informatics arise from the use of informational technologies in the implementation of law, such as the use of subpoenas for information found in email, search queries, and social networks.
Policy approaches to legal informatics issues vary throughout the world.
For example, European countries tend to require destruction or anonymization of data so that it cannot be used for discovery.
The widespread introduction of cloud computing provides several benefits in delivering legal services.
Legal service providers can use the Software as a Service model to earn a profit by charging customers a per-use or subscription fee.
This model has several benefits over traditional bespoke services.
Software as a service is much more scalable.
Traditional bespoke models require an attorney to spend more of a limited resource (their time) on each additional client.
Using Software as a Service, a legal service provider can put in effort once to develop the product and then use a much less limited resource (cloud computing power) to provide service to each additional customer.
Software as a service can be used to complement traditional bespoke services by handling routine tasks, leaving an attorney free to concentrate on bespoke work.
Software as a service can be delivered more conveniently because it does not require the legal service provider to be available at the same time as the customer.
Software as a service also complicates the attorney-client relationship in a way that may have implications for attorney-client privilege.
The traditional delivery model makes it easy to create delineations of when attorney-client privilege attaches and when it does not.
But in more complex models of legal service delivery other actors or automated processes may moderate the relationship between a client and their attorney making it difficult to tell which communications should be legally privileged.
Artificial intelligence is employed in online dispute resolution platforms that use optimization algorithms and blind-bidding.
Artificial intelligence is also frequently employed in modeling the legal ontology, "an explicit, formal, and general specification of a conceptualization of properties of and relations between objects in a given domain".Artificial intelligence and law (AI and law) is a subfield of artificial intelligence (AI) mainly concerned with applications of AI to legal informatics problems and original research on those problems.
It is also concerned to contribute in the other direction: to export tools and techniques developed in the context of legal problems to AI in general.
For example, theories of legal decision making, especially models of argumentation, have contributed to knowledge representation and reasoning; models of social organization based on norms have contributed to multi-agent systems; reasoning with legal cases has contributed to case-based reasoning; and the need to store and retrieve large amounts of textual data has resulted in contributions to conceptual information retrieval and intelligent databases.
Within the practice issues conceptual area, progress continues to be made on both litigation and transaction focused technologies.
In particular, technology including predictive coding has the potential to effect substantial efficiency gains in law practice.
Though predictive coding has largely been applied in the litigation space, it is beginning to make inroads in transaction practice, where it is being used to improve document review in mergers and acquisitions.
Other advances, including XML coding in transaction contracts, and increasingly advanced document preparation systems demonstrate the importance of legal informatics in the transactional law space.
Current applications of AI in the legal field utilize machines to review documents, particularly when a high level of completeness and confidence in the quality of document analysis is depended upon, such as in instances of litigation and where due diligence play a role. .
Predictive coding leverages small samples to cross-reference similar items, weed out less relevant documents so attorneys can focus on the truly important key documents, produces statistically validated results, equal to or surpassing the accuracy and, prominently, the rate of human review. .
Advances in technology and legal informatics have led to new models for the delivery of legal services.
Legal services have traditionally been a "bespoke" product created by a professional attorney on an individual basis for each client.
However, to work more efficiently, parts of these services will move sequentially from (1) bespoke to (2) standardized, (3) systematized, (4) packaged, and (5) commoditized.
Moving from one stage to the next will require embracing different technologies and knowledge systems.
The spread of the Internet and development of legal technology and informatics are extending legal services to individuals and small-medium companies.
Corporate legal departments may use legal informatics for such purposes as to manage patent portfolios, and for preparation, customization and management of documents.
